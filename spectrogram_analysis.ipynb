{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spectrogram_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/spectrogram_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjJxgwQL1toO"
      },
      "source": [
        "# Music classification and generation with spectrograms\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Beatrix Benko, Lina Teichmann"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W376J475Po7i"
      },
      "source": [
        "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
        "\n",
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sySkmalZ44xu"
      },
      "source": [
        "## This notebook\n",
        "This notebook loads the GTZAN dataset which includes audiofiles and spectrograms. You can use this dataset or find your own. The first part of the notebook is all about data visualization and show how to make spectrograms from audiofiles. The second part of the notebook includes a CNN that is trained on the spectrograms to predict music genre. Below we also provide links to tutorials and other resources if you want to try to do some of the harder project ideas. \n",
        "\n",
        "Have fun :) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYJE5NDb4zwD"
      },
      "source": [
        "## Acknowledgements\n",
        "This notebook was written by Beatrix BenkÅ‘ and Lina Teichmann.\n",
        "\n",
        "**Useful code examples:** \n",
        "\n",
        "https://towardsdatascience.com/music-genre-classification-with-python-c714d032f0d8\n",
        "\n",
        "[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)\n",
        "\n",
        "[https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py)\n",
        "\n",
        "https://github.com/kamalesh0406/Audio-Classification \n",
        "\n",
        "https://github.com/zcaceres/spec_augment\n",
        "\n",
        "https://musicinformationretrieval.com/ipython_audio.html "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbqPV_UHk0T7",
        "cellView": "form"
      },
      "source": [
        "# @title Install dependencies\n",
        "!sudo apt-get install -y ffmpeg --quiet\n",
        "!pip install seaborn --quiet\n",
        "!pip install librosa --quiet\n",
        "!pip install imageio --quiet\n",
        "!pip install imageio-ffmpeg --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPDlpr54BODT"
      },
      "source": [
        "# Import necessary libraries.\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import imageio\n",
        "import random, shutil \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n",
        "import librosa\n",
        "import librosa.display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "My6Is2kXd4_B"
      },
      "source": [
        "import requests\n",
        "\n",
        "fname = \"music.zip\"\n",
        "url = \"https://osf.io/drjhb/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvtCXVUW6Ag9"
      },
      "source": [
        "## Loading GTZAN dataset (includes spectrograms)\n",
        "\n",
        "The GTZAN dataset for music genre classification can be dowloaded from Kaggle: https://www.kaggle.com/andradaolteanu/gtzan-dataset-music-genre-classification. \n",
        "\n",
        "To download from Kaggle using this code you need to download and copy over your api token. In Kaggle go to the upper right side -> account -> API -> create API token. This downloads a json file. Copy the content into api_token. It should look like this: \n",
        "\n",
        "api_token = {\"username\":\"johnsmith\",\"key\":\"123a123a123\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmLI_Pld139B"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(fname, 'r') as zipObj:\n",
        "  # Extract all the contents of zip file in different directory\n",
        "  zipObj.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIkRHHMVIehd"
      },
      "source": [
        "## Have a look at the data\n",
        "\n",
        "In this section we are looking at an example of an audio waveform. Then we'll transform the sound wave to a spectrogram and compare it with the spectrogram that was included with the downloaded dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjhNGJi3GApt"
      },
      "source": [
        "# Inspect an audio file from the dataset.\n",
        "\n",
        "sample_path = 'Data/genres_original/jazz/jazz.00000.wav'\n",
        "\n",
        "# if you want to listen to the audio, uncomment below.\n",
        "# display.Audio(sample_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNUgOxweI1T1"
      },
      "source": [
        "y, sample_rate = librosa.load(sample_path)\n",
        "\n",
        "print('y:', y, '\\n')\n",
        "print('y shape:', np.shape(y), '\\n')\n",
        "print('Sample rate (KHz):', sample_rate, '\\n')\n",
        "print(f'Length of audio: {np.shape(y)[0]/sample_rate}')\n",
        "\n",
        "# Plot th sound wave.\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "librosa.display.waveplot(y=y, sr=sample_rate);\n",
        "plt.title(\"Sound wave of jazz.00000.wav\", fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn_aVkw8RauO"
      },
      "source": [
        "# Convert sound wave to spectrogram. \n",
        "\n",
        "# Short-time Fourier transform (STFT).\n",
        "\n",
        "D = np.abs(librosa.stft(y, n_fft=2048, hop_length=512))\n",
        "print('Shape of D object:', np.shape(D))\n",
        "\n",
        "# Convert amplitude spectrogram to Decibels-scaled spectrogram.\n",
        "\n",
        "DB = librosa.amplitude_to_db(D, ref = np.max)\n",
        "\n",
        "# Creating the spectogram.\n",
        "\n",
        "plt.figure(figsize = (16, 6))\n",
        "librosa.display.specshow(DB, sr=sample_rate, hop_length=512,\n",
        "                         x_axis='time', y_axis='log')\n",
        "plt.colorbar()\n",
        "plt.title('Decibels-scaled spectrogram', fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2kQo1cO-nXh"
      },
      "source": [
        "The mel spectrogram uses mel sclae intead of a linear one: [mel scale](https://en.wikipedia.org/wiki/Mel_scale) is a perceptual scale of pitches judged by listeners to be equal in distance from one another. The reference point between this scale and normal frequency measurement is defined by assigning a perceptual pitch of 1000 mels to a 1000 Hz tone, 40 dB above the listener's threshold. Above about 500 Hz, increasingly large intervals are judged by listeners to produce equal pitch increments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynrvvul1S2Rt"
      },
      "source": [
        "# Convert sound wave to mel spectrogram.\n",
        " \n",
        "y, sr = librosa.load(sample_path)\n",
        "\n",
        "S = librosa.feature.melspectrogram(y, sr=sr)\n",
        "S_DB = librosa.amplitude_to_db(S, ref=np.max)\n",
        "plt.figure(figsize=(15, 5))\n",
        "librosa.display.specshow(S_DB, sr=sr, hop_length=512,\n",
        "                         x_axis='time', y_axis='log')\n",
        "plt.colorbar()\n",
        "plt.title(\"Mel spectrogram\", fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8barREusHLK4"
      },
      "source": [
        "# Visualize the mel spectrogram of the same sample from the dataset. \n",
        "\n",
        "img_path = 'Data/images_original/jazz/jazz00000.png'\n",
        "img = imageio.imread(img_path)\n",
        "print(img.shape)\n",
        "\n",
        "plt.imshow(img, interpolation='nearest')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DM5Yzr5Vk1uH"
      },
      "source": [
        "## Train a simple CNN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc8HbVEM9qK0",
        "cellView": "form"
      },
      "source": [
        "# @title Helper functions (run me)\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "      print(\"WARNING: For this notebook to perform best, \"\n",
        "          \"if possible, in the menu under `Runtime` -> \"\n",
        "          \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "      print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device\n",
        "\n",
        "\n",
        "#  Plotting function.\n",
        "\n",
        "def plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc):\n",
        "  epochs = len(train_loss)\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  ax1.plot(list(range(epochs)), train_loss, label='Training Loss')\n",
        "  ax1.plot(list(range(epochs)), validation_loss, label='Validation Loss')\n",
        "  ax1.set_xlabel('Epochs')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.set_title('Epoch vs Loss')\n",
        "  ax1.legend()\n",
        "\n",
        "  ax2.plot(list(range(epochs)), train_acc, label='Training Accuracy')\n",
        "  ax2.plot(list(range(epochs)), validation_acc, label='Validation Accuracy')\n",
        "  ax2.set_xlabel('Epochs')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_title('Epoch vs Accuracy')\n",
        "  ax2.legend()\n",
        "  fig.set_size_inches(15.5, 5.5)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Db-MG1diXaIT"
      },
      "source": [
        "device = set_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7W8dt-YKNvo"
      },
      "source": [
        "# Create folder with training, testing and validation data.\n",
        "\n",
        "spectrograms_dir = \"Data/images_original/\"\n",
        "folder_names = ['Data/train/', 'Data/test/', 'Data/val/']\n",
        "train_dir = folder_names[0]\n",
        "test_dir = folder_names[1]\n",
        "val_dir = folder_names[2]\n",
        "\n",
        "for f in folder_names:\n",
        "  if os.path.exists(f):\n",
        "    shutil.rmtree(f)\n",
        "    os.mkdir(f)\n",
        "  else:\n",
        "    os.mkdir(f)\n",
        "\n",
        "# Loop over all genres.\n",
        "\n",
        "genres = list(os.listdir(spectrograms_dir))\n",
        "for g in genres:\n",
        "  # find all images & split in train, test, and validation\n",
        "  src_file_paths= []\n",
        "  for im in glob.glob(os.path.join(spectrograms_dir, f'{g}',\"*.png\"), recursive=True):\n",
        "    src_file_paths.append(im)\n",
        "  random.shuffle(src_file_paths)\n",
        "  test_files = src_file_paths[0:10]\n",
        "  val_files = src_file_paths[10:20]\n",
        "  train_files = src_file_paths[20:]\n",
        "\n",
        "  #  make destination folders for train and test images\n",
        "  for f in folder_names:\n",
        "    if not os.path.exists(os.path.join(f + f\"{g}\")):\n",
        "      os.mkdir(os.path.join(f + f\"{g}\"))\n",
        "\n",
        "  # copy training and testing images over\n",
        "  for f in train_files:\n",
        "    shutil.copy(f, os.path.join(os.path.join(train_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
        "  for f in test_files:\n",
        "    shutil.copy(f, os.path.join(os.path.join(test_dir + f\"{g}\") + '/',os.path.split(f)[1]))\n",
        "  for f in val_files:\n",
        "    shutil.copy(f, os.path.join(os.path.join(val_dir + f\"{g}\") + '/',os.path.split(f)[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fCQLhS6I_5x"
      },
      "source": [
        "# Data loading.\n",
        "\n",
        "train_dataset = datasets.ImageFolder(\n",
        "    train_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ]))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=25, shuffle=True, num_workers=0)\n",
        "\n",
        "val_dataset = datasets.ImageFolder(\n",
        "    val_dir,\n",
        "    transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ]))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=25, shuffle=True, num_workers=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aJGLqEiFaE_"
      },
      "source": [
        "# Make a CNN & train it to predict genres.\n",
        "\n",
        "class music_net(nn.Module):\n",
        "  def __init__(self):\n",
        "    \"\"\"Intitalize neural net layers\"\"\"\n",
        "    super(music_net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
        "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
        "    self.fc1 = nn.Linear(in_features=9856, out_features=10)\n",
        "\n",
        "    self.batchnorm1 = nn.BatchNorm2d(num_features=8)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(num_features=16)\n",
        "    self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
        "    self.batchnorm4 = nn.BatchNorm2d(num_features=64)\n",
        "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
        "\n",
        "\n",
        "  def forward(self, x):   \n",
        "    # Conv layer 1.\n",
        "    x = self.conv1(x)\n",
        "    x = self.batchnorm1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 2.\n",
        "    x = self.conv2(x)\n",
        "    x = self.batchnorm2(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 3.\n",
        "    x = self.conv3(x)\n",
        "    x = self.batchnorm3(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 4.\n",
        "    x = self.conv4(x)\n",
        "    x = self.batchnorm4(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Conv layer 5.\n",
        "    x = self.conv5(x)\n",
        "    x = self.batchnorm5(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "\n",
        "    # Fully connected layer 1.\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc1(x)\n",
        "    x = F.softmax(x)\n",
        "\n",
        "    return x\n",
        "        \n",
        "\n",
        "def train(model, device, train_loader, validation_loader, epochs):\n",
        "  criterion =  nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "  train_loss, validation_loss = [], []\n",
        "  train_acc, validation_acc = [], []\n",
        "  with tqdm(range(epochs), unit='epoch') as tepochs:\n",
        "    tepochs.set_description('Training')\n",
        "    for epoch in tepochs:\n",
        "      model.train()\n",
        "      # keep track of the running loss\n",
        "      running_loss = 0.\n",
        "      correct, total = 0, 0\n",
        "\n",
        "      for data, target in train_loader:\n",
        "        # getting the training set\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # Get the model output (call the model with the data from this batch)\n",
        "        output = model(data)\n",
        "        # Zero the gradients out)\n",
        "        optimizer.zero_grad()\n",
        "        # Get the Loss\n",
        "        loss  = criterion(output, target)\n",
        "        # Calculate the gradients\n",
        "        loss.backward() \n",
        "        # Update the weights (using the training step of the optimizer)\n",
        "        optimizer.step() \n",
        "\n",
        "        tepochs.set_postfix(loss=loss.item())\n",
        "        running_loss += loss  # add the loss for this batch\n",
        "\n",
        "        # get accuracy\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "      # append the loss for this epoch\n",
        "      train_loss.append(running_loss/len(train_loader))  \n",
        "      train_acc.append(correct/total)\n",
        "\n",
        "      # evaluate on validation data\n",
        "      model.eval()\n",
        "      running_loss = 0.\n",
        "      correct, total = 0, 0\n",
        "      \n",
        "      for data, target in validation_loader:\n",
        "        # getting the validation set\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        tepochs.set_postfix(loss=loss.item())\n",
        "        running_loss += loss.item()\n",
        "        # get accuracy\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "      validation_loss.append(running_loss/len(validation_loader))\n",
        "      validation_acc.append(correct/total)\n",
        "\n",
        "  return train_loss, train_acc, validation_loss, validation_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6PHbEwn3dI9"
      },
      "source": [
        "# Run training. \n",
        "\n",
        "net = music_net().to(device)\n",
        "train_loss, train_acc, validation_loss, validation_acc = train(net, device, train_loader, val_loader, 50)\n",
        "plot_loss_accuracy(train_loss, train_acc, validation_loss, validation_acc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}