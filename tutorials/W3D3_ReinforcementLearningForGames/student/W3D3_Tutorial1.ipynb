{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D3_ReinforcementLearningForGames/student/W3D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neuromatch Academy: Week 3, Day 3, Tutorial 1\n",
    "# RL for games \n",
    "\n",
    "__Content creators:__ Tim Lilicrap, Blake Richards.\n",
    "\n",
    "__Content reviewers:__ Arush Tagade, Lily Cheng, Melvin Selim Atay\n",
    "\n",
    "__Content editors:__ Melvin Selim Atay, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Namrata Bafna, Spiros Chavlis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Tutorial Objectives\n",
    "\n",
    "In this tutotial, you will learn how to implement a game loop and improve the performance of a random player. \n",
    "\n",
    "The specific objectives for this tutorial:\n",
    "*   Understand the format of two-players games\n",
    "*   Learn about value network and policy network \n",
    "* Learn about Monte Carlo Tree Search (MCTS) and compare its performance to policy-based and value-based players\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "outputId": "8df51440-35cf-49cf-99d8-5ba9f86c544c"
   },
   "outputs": [],
   "source": [
    "#@markdown Tutorial slides\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('<iframe src=\"https://docs.google.com/presentation/d/1AkCj22vFM-i7u0H5EZd1_WNonCI7CilpD1Y9OxG1FYA/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we have:\n",
    "\n",
    "\n",
    "1.  **Import cell**: imports all libraries you use in the tutorial\n",
    "2.  **Hidden Figure settings cell**: sets up the plotting style (copy exactly)\n",
    "3. **Hidden Plotting functions cell**: contains all functions used to create plots throughout the tutorial (so students don't waste time looking at boilerplate matplotlib but can here if they wish to). Please use only matplotlib for plotting for consistency.\n",
    "4. **Hidden Helper functions cell**: This should contain functions that students have previously used or that are very simple. Any helper functions that are being used for the first time and are important should be placed directly above the relevant text or exercise (see Section 1.1 for an example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b173974e-e47d-4de4-a5bf-5c4e9c3445d4"
   },
   "outputs": [],
   "source": [
    "#@title Clone a repo from github\n",
    "#@markdown Run this cell!\n",
    "!git clone https://github.com/raymondchua/nma_rl_games.git\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/nma_rl_games/alpha-zero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ee7bc987-a0e9-453b-88a4-c1c59a3ced15"
   },
   "outputs": [],
   "source": [
    "# Install modules\n",
    "!pip install tqdm --quiet\n",
    "!pip install coloredlogs --quiet\n",
    "\n",
    "# Imports\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import Arena\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import coloredlogs\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils import *\n",
    "from Game import Game\n",
    "from MCTS import MCTS\n",
    "\n",
    "from othello.OthelloPlayers import *\n",
    "from othello.OthelloLogic import Board\n",
    "from othello.OthelloGame import OthelloGame\n",
    "from othello.pytorch.NNet import NNetWrapper as NNet\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from utils import *\n",
    "from pickle import Pickler, Unpickler\n",
    "from tqdm.notebook import tqdm\n",
    "from NeuralNet import NeuralNet\n",
    "from __future__ import print_function\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from random import shuffle\n",
    "from pickle import Pickler, Unpickler\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed value\n",
    "seed_value= 1234\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'numIters': 1,            # in training setting this was 1000 and num of episodes=100\n",
    "    'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,      # To control exploration and exploitation\n",
    "    'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n",
    "    'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n",
    "    'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "    'maxDepth':5,             # Maximum number of rollouts\n",
    "    'numMCsims': 5,           # Number of monte carlo simulations\n",
    "    'mc_topk': 3,             # top k actions for monte carlo rollout\n",
    "\n",
    "    'checkpoint': './temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "    # define neural network arguments\n",
    "    'lr': 0.001,               # lr: learning rate\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Section 1: Create a game/agent loop for RL\n",
    "***Goal***: How to setup a game environment with multiple players for reinforcement learning experiments.\n",
    "\n",
    "***Exercise***: \n",
    "\n",
    "\n",
    "*   Build an agent that plays random moves\n",
    "*   Connect with connect 4 game\n",
    "*   Generate games including wins and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "e00faf82-b96c-4ba0-8af0-128004235d1a"
   },
   "outputs": [],
   "source": [
    "#@title Video 1: Video 1 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloGame(Game):\n",
    "  square_content = {\n",
    "      -1: \"X\",\n",
    "      +0: \"-\",\n",
    "      +1: \"O\"\n",
    "      }\n",
    "\n",
    "  @staticmethod\n",
    "  def getSquarePiece(piece):\n",
    "    return OthelloGame.square_content[piece]\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "  def getInitBoard(self):\n",
    "    # return initial board (numpy board)\n",
    "    b = Board(self.n)\n",
    "    return np.array(b.pieces)\n",
    "\n",
    "  def getBoardSize(self):\n",
    "    # (a,b) tuple\n",
    "    return (self.n, self.n)\n",
    "\n",
    "  def getActionSize(self):\n",
    "    # return number of actions\n",
    "    return self.n*self.n + 1\n",
    "\n",
    "  def getNextState(self, board, player, action):\n",
    "    # if player takes action on board, return next (board,player)\n",
    "    # action must be a valid move\n",
    "    if action == self.n*self.n:\n",
    "      return (board, -player)\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    move = (int(action/self.n), action%self.n)\n",
    "    b.execute_move(move, player)\n",
    "    return (b.pieces, -player)\n",
    "\n",
    "  def getValidMoves(self, board, player):\n",
    "    # return a fixed size binary vector\n",
    "    valids = [0]*self.getActionSize()\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    legalMoves =  b.get_legal_moves(player)\n",
    "    if len(legalMoves)==0:\n",
    "      valids[-1]=1\n",
    "      return np.array(valids)\n",
    "    for x, y in legalMoves:\n",
    "      valids[self.n*x+y]=1\n",
    "    return np.array(valids)\n",
    "\n",
    "  def getGameEnded(self, board, player):\n",
    "    # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n",
    "    # player = 1\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    if b.has_legal_moves(player):\n",
    "      return 0\n",
    "    if b.has_legal_moves(-player):\n",
    "      return 0\n",
    "    if b.countDiff(player) > 0:\n",
    "      return 1\n",
    "    return -1\n",
    "\n",
    "  def getCanonicalForm(self, board, player):\n",
    "    # return state if player==1, else return -state if player==-1\n",
    "    return player*board\n",
    "\n",
    "  def getSymmetries(self, board, pi):\n",
    "    # mirror, rotational\n",
    "    assert(len(pi) == self.n**2+1)  # 1 for pass\n",
    "    pi_board = np.reshape(pi[:-1], (self.n, self.n))\n",
    "    l = []\n",
    "\n",
    "    for i in range(1, 5):\n",
    "      for j in [True, False]:\n",
    "        newB = np.rot90(board, i)\n",
    "        newPi = np.rot90(pi_board, i)\n",
    "        if j:\n",
    "          newB = np.fliplr(newB)\n",
    "          newPi = np.fliplr(newPi)\n",
    "        l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n",
    "    return l\n",
    "\n",
    "  def stringRepresentation(self, board):\n",
    "    return board.tostring()\n",
    "\n",
    "  def stringRepresentationReadable(self, board):\n",
    "    board_s = \"\".join(self.square_content[square] for row in board for square in row)\n",
    "    return board_s\n",
    "\n",
    "  def getScore(self, board, player):\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    return b.countDiff(player)\n",
    "\n",
    "  @staticmethod\n",
    "  def display(board):\n",
    "    n = board.shape[0]\n",
    "    print(\"   \", end=\"\")\n",
    "    for y in range(n):\n",
    "      print(y, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for y in range(n):\n",
    "      print(y, \"|\", end=\"\")    # print the row #\n",
    "      for x in range(n):\n",
    "        piece = board[y][x]    # get the piece to print\n",
    "        print(OthelloGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Create a random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer():\n",
    "  def __init__(self, game):\n",
    "    self.game = game\n",
    "\n",
    "  def play(self, board):\n",
    "\n",
    "    valids = self.game.getValidMoves(board, 1)               # STUDENTS\n",
    "    prob = valids/valids.sum()                               # STUDENTS\n",
    "    a = np.random.choice(self.game.getActionSize(), p=prob)  # STUDENTS\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2. Initiate the game board\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d817627e-f4b5-4d0b-d727-a4352df5ba2d"
   },
   "outputs": [],
   "source": [
    "# Display the board\n",
    "game = OthelloGame(6)\n",
    "board = game.getInitBoard()\n",
    "game.display(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6bbdb5b5-d668-4f41-984b-749eaff9a5da"
   },
   "outputs": [],
   "source": [
    "# observe the game board size\n",
    "print('Board size = {}' .format(game.getBoardSize()))\n",
    "\n",
    "# observe the action size\n",
    "print('Action size = {}'.format(game.getActionSize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3. Create two random agents to play against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c057fe29-bd17-491a-ed1c-6fccc1ec87ef"
   },
   "outputs": [],
   "source": [
    "# define the random player\n",
    "player1 = RandomPlayer(game).play  # player 1 is a random player\n",
    "player2 = RandomPlayer(game).play  # player 2 is a random player\n",
    "\n",
    "# define number of games\n",
    "num_games = 20\n",
    "\n",
    "# start the competition\n",
    "arena = Arena.Arena(player1, player2 , game, display=None)  # to see the steps of the competition set \"display=OthelloGame.display\"\n",
    "\n",
    "result = arena.playGames(num_games, verbose=False)  # return  ( number of games won by player1, num of games won by player2, num of games won by nobody)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "478eb029-8024-4cba-8655-a7a80ce0f602"
   },
   "outputs": [],
   "source": [
    "print(\"\\nNumber of games won by player1 = {},\\nNumber of games won by player2 = {},\\nNumber of games won by nobody = {} out of {} games\" .format(result[0], result[1], result[2], num_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4. Compute win rate for the random player (player 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c5c7e946-0377-46af-f482-5e2ceac86b84"
   },
   "outputs": [],
   "source": [
    "win_rate_player1 = result[0]/num_games\n",
    "print('\\n Win rate for player 1 over 20 games: {}%'.format(win_rate_player1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Train a value function from expert game data\n",
    "**Goal:** Learn how to train a value function from a dataset of games played by an expert.\n",
    "\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "* Load a dataset of expert generated games.\n",
    "* Train a network to minimize MSE for win/loss predictions given board states sampled throughout the game. This will be done on a very small number of games. We will provide a network trained on a larger dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "067f3712-afe5-43f6-8f2a-f3108315b29b"
   },
   "outputs": [],
   "source": [
    "#@title Video 2: Video 2 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Some self-play info/code goes here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1. Load expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainExamples(folder, filename):\n",
    "  trainExamplesHistory = []\n",
    "  modelFile = os.path.join(folder, filename)\n",
    "  examplesFile = modelFile + \".examples\"\n",
    "  if not os.path.isfile(examplesFile):\n",
    "    print(f'File \"{examplesFile}\" with trainExamples not found!')\n",
    "    r = input(\"Continue? [y|n]\")\n",
    "    if r != \"y\":\n",
    "      sys.exit()\n",
    "  else:\n",
    "    print(\"File with train examples found. Loading it...\")\n",
    "    with open(examplesFile, \"rb\") as f:\n",
    "      trainExamplesHistory = Unpickler(f).load()\n",
    "    print('Loading done!')\n",
    "    # examples based on the model were already collected (loaded)\n",
    "    return trainExamplesHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c760426f-d30d-4ab6-f75e-391bd680ae2b"
   },
   "outputs": [],
   "source": [
    "path = F\"/content/nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2. Define the Neural Network Architecture for Othello\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 2.2: Implement the NN `OthelloNNet` for Othello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloNNet(nn.Module):\n",
    "  def __init__(self, game, args):\n",
    "    # game params\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.args = args\n",
    "\n",
    "    super(OthelloNNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1,\n",
    "                           padding=1)\n",
    "    self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "    self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn3 = nn.BatchNorm2d(args.num_channels)\n",
    "    self.bn4 = nn.BatchNorm2d(args.num_channels)\n",
    "\n",
    "    self.fc1 = nn.Linear(args.num_channels * (self.board_x - 4) * (self.board_y - 4), 1024)\n",
    "    self.fc_bn1 = nn.BatchNorm1d(1024)\n",
    "\n",
    "    self.fc2 = nn.Linear(1024, 512)\n",
    "    self.fc_bn2 = nn.BatchNorm1d(512)\n",
    "\n",
    "    self.fc3 = nn.Linear(512, self.action_size)\n",
    "\n",
    "    self.fc4 = nn.Linear(512, 1)\n",
    "\n",
    "  def forward(self, s):\n",
    "    # s: batch_size x board_x x board_y\n",
    "    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n",
    "\n",
    "    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
    "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "    pi = self.fc3(s)  # batch_size x action_size\n",
    "    v = self.fc4(s)   # batch_size x 1\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Calculate the probability distribution and the value\")\n",
    "    #################################################\n",
    "    # return a probability distribution over actions at the current state and the value of the current state.\n",
    "    return ..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_a1507cf8.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3. Define the Value network\n",
    " During the training the ground truth will be uploaded from the **MCTS simulations** available at 'checkpoint_x.path.tar.examples'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 2.3: Implement the `ValueNetwork`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(NeuralNet):\n",
    "  def __init__(self, game):\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "\n",
    "    if args.cuda:\n",
    "      self.nnet.cuda()\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    examples: list of examples, each example is of form (board, pi, v)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        v_losses = []   # to store the losses per epoch\n",
    "        batch_count = int(len(examples) / args.batch_size)  # len(examples)=200, batch-size=64, batch_count=3\n",
    "        t = tqdm(range(batch_count), desc='Training Value Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # read the ground truth information from MCTS simulation using the loaded examples\n",
    "          boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # length of boards, pis, vis = 64\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "          # predict\n",
    "          if args.cuda: # to run on GPU if available\n",
    "            boards, target_vs = boards.contiguous().cuda(), target_vs.contiguous().cuda()\n",
    "\n",
    "          #################################################\n",
    "          ## TODO for students: details of what they should do ##\n",
    "          # Fill out function and remove\n",
    "          raise NotImplementedError(\"Compute the output\")\n",
    "          #################################################\n",
    "          # compute output\n",
    "          _, out_v = ...\n",
    "          l_v = ...\n",
    "          total_loss = l_v\n",
    "\n",
    "          # record loss\n",
    "          v_losses.append(l_v.item())\n",
    "          t.set_postfix(Loss_v=l_v.item())\n",
    "\n",
    "          # compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          total_loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    board: np array with board\n",
    "    \"\"\"\n",
    "    # timing\n",
    "    start = time.time()\n",
    "\n",
    "    # preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    if args.cuda:\n",
    "      board = board.contiguous().cuda()\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "        _, v = self.nnet(board)\n",
    "    return v.data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_v(self, targets, outputs):\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Calculate the loss\")\n",
    "    #################################################\n",
    "    # Mean squared error (MSE)\n",
    "    return ...\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(folder):\n",
    "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "      os.mkdir(folder)\n",
    "    else:\n",
    "      print(\"Checkpoint Directory exists! \")\n",
    "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
    "    print(\"Model saved! \")\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "      raise (\"No model in path {}\".format(filepath))\n",
    "    map_location = None if args.cuda else 'cpu'\n",
    "    checkpoint = torch.load(filepath, map_location=map_location)\n",
    "    self.nnet.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_a74840dc.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4. Train the value network and observe the MSE loss progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "14d25ca2-b586-4b29-c40a-92a7c2eb4b48"
   },
   "outputs": [],
   "source": [
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Section 3: Use a trained value network to play games\n",
    "**Goal**: Learn how to use a value function in order to make a player that works better than a random player.\n",
    "\n",
    "**Exercise:**\n",
    "* Sample random valid moves and use the value function to rank them\n",
    "* Choose the best move as the action and play it\n",
    "Show that doing so beats the random player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "a4632f58-4cdb-4168-c35a-948659407f27"
   },
   "outputs": [],
   "source": [
    "#@title Video 3: Video 3 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 3.1: Value-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need to change the checkpoint address based on the file name that's in your colab\n",
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = F\"/content/nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueBasedPlayer():\n",
    "  def __init__(self, game, vnet):\n",
    "    self.game = game\n",
    "    self.vnet = vnet\n",
    "\n",
    "  def play(self, board):\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    candidates = []\n",
    "    max_num_actions = 3\n",
    "    va = np.where(valids)[0]\n",
    "    va_list = va.tolist()\n",
    "    shuffle(va_list)\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Implement the value-based player\")\n",
    "    #################################################\n",
    "    for a in va_list:\n",
    "      # return next board state using getNextState() function\n",
    "      nextBoard, _ = ...\n",
    "      # predict the value of next state using value network\n",
    "      value = ...\n",
    "      # add the value and the action as a tuple to the candidate lists, note that you might need to change the sign of the value based on the player\n",
    "      candidates += ...\n",
    "\n",
    "      if len(candidates) == max_num_actions:\n",
    "        break\n",
    "\n",
    "    candidates.sort()\n",
    "\n",
    "    return candidates[0][1]\n",
    "\n",
    "\n",
    "# playing games between a value-based player and a random player\n",
    "num_games = 20\n",
    "player1 = ValueBasedPlayer(game, vnet).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "## Uncomment the code below to check your code!\n",
    "# result = arena.playGames(num_games, verbose=False)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "e64de058-6881-46c5-a8ba-c75ac420a1bb"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_0fb03f3b.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Arena.playGames (1): 100%|██████████| 10/10 [00:01<00:00,  9.30it/s]\n",
    "Arena.playGames (2): 100%|██████████| 10/10 [00:00<00:00, 10.60it/s](13, 7, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result of pitting a value-based player against a random player**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e8e819da-7f48-4a31-8bee-7fec3f672e9a"
   },
   "outputs": [],
   "source": [
    "print(\"\\nNumber of games won by player1 = {}, \\nNumber of games won by player2 = {}, \\nNumber of games won by nobody = {} out of {} games\" .format(result[0], result[1], result[2], num_games))\n",
    "\n",
    "win_rate_player1 = result[0]/num_games # result[0] is the number of times that player 1 wins\n",
    "print('\\nWin rate for player 1 over {} games: {}%'.format(num_games, win_rate_player1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Train a policy network from expert game data\n",
    "**Goal**: How to train a policy network via supervised learning / behavioural cloning.\n",
    "\n",
    "**Exercise**:\n",
    "* Train a network to predict the next move in an expert dataset by maximizing the log likelihood of the next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "b44d62b2-fda6-4fd9-9b09-7a4dd06e2f26"
   },
   "outputs": [],
   "source": [
    "#@title Video 4: Video 4 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 4.1: Implement `PolicyNetwork`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(NeuralNet):\n",
    "  def __init__(self, game):\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "\n",
    "    if args.cuda:\n",
    "      self.nnet.cuda()\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    examples: list of examples, each example is of form (board, pi, v)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        pi_losses = []\n",
    "\n",
    "        batch_count = int(len(examples) / args.batch_size)\n",
    "\n",
    "        t = tqdm(range(batch_count), desc='Training Policy Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "          boards, pis, _ = list(zip(*[examples[i] for i in sample_ids]))\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_pis = torch.FloatTensor(np.array(pis))\n",
    "\n",
    "          # predict\n",
    "          if args.cuda:\n",
    "            boards, target_pis = boards.contiguous().cuda(), target_pis.contiguous().cuda()\n",
    "\n",
    "          #################################################\n",
    "          ## TODO for students: details of what they should do ##\n",
    "          # Fill out function and remove\n",
    "          raise NotImplementedError(\"Compute the output\")\n",
    "          #################################################\n",
    "          # compute output\n",
    "          out_pi, _ = ...\n",
    "          l_pi = ...\n",
    "          total_loss = l_pi\n",
    "\n",
    "          # record loss\n",
    "          pi_losses.append(l_pi.item())\n",
    "          t.set_postfix(Loss_pi=l_pi.item())\n",
    "\n",
    "          # compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          l_pi.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    board: np array with board\n",
    "    \"\"\"\n",
    "    # timing\n",
    "    start = time.time()\n",
    "\n",
    "    # preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    if args.cuda: board = board.contiguous().cuda()\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "      pi,_ = self.nnet(board)\n",
    "    return torch.exp(pi).data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_pi(self, targets, outputs):\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Compute the loss\")\n",
    "    #################################################\n",
    "    # loss function. Be careful with the sign!\n",
    "    return ...\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(folder):\n",
    "      print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "      os.mkdir(folder)\n",
    "    else:\n",
    "      print(\"Checkpoint Directory exists! \")\n",
    "    torch.save({'state_dict': self.nnet.state_dict(),}, filepath)\n",
    "    print(\"Model saved! \")\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "      raise (\"No model in path {}\".format(filepath))\n",
    "    map_location = None if args.cuda else 'cpu'\n",
    "    checkpoint = torch.load(filepath, map_location=map_location)\n",
    "    self.nnet.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "\n",
    "# we use the same actor-critic network to output a policy\n",
    "# game = OthelloGame(6)\n",
    "# pnet = PolicyNetwork(game)\n",
    "# pnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "e2514ec8-815c-4c85-f75e-d7152bfc9085"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_3c744ce7.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#Section 5: Use a trained policy network to play games\n",
    "**Goal**: How to use a policy network to play games.\n",
    "\n",
    "**Exercise:** \n",
    "* Use the policy network to give probabilities for the next move.\n",
    "* Build a player that takes the move given the maximum probability by the network.\n",
    "* Compare this to another player that samples moves according to the probability distribution output by the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "331a982a-4c43-424f-a53f-0f9d6bd2f205"
   },
   "outputs": [],
   "source": [
    "#@title Video 5: Video 5 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 5.1: `Implement the PolicyBasedPlayer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need to change the checkpoint address based on the file name that's in your colab\n",
    "\n",
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = F\"/content/nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyBasedPlayer():\n",
    "  def __init__(self, game, pnet, greedy=True):\n",
    "    self.game = game\n",
    "    self.pnet = pnet\n",
    "    self.greedy = greedy\n",
    "\n",
    "  def play(self, board):\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Define the play\")\n",
    "    #################################################\n",
    "    action_probs = ...\n",
    "    vap = ...  # masking invalid moves\n",
    "    sum_vap = ...\n",
    "\n",
    "    if sum_vap > 0:\n",
    "      vap /= sum_vap  # renormalize\n",
    "    else:\n",
    "      # if all valid moves were masked we make all valid moves equally probable\n",
    "      print(\"All valid moves were masked, doing a workaround.\")\n",
    "      vap = vap + valids\n",
    "      vap /= np.sum(vap)\n",
    "\n",
    "    if self.greedy:\n",
    "      # greedy policy player\n",
    "      a = np.where(vap == np.max(vap))[0][0]\n",
    "    else:\n",
    "      # sample-based policy player\n",
    "      a = np.random.choice(self.game.getActionSize(), p=vap)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "# playing games\n",
    "num_games = 20\n",
    "player1 = PolicyBasedPlayer(game, pnet, greedy=True).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "## Uncomment below to test!\n",
    "# result = arena.playGames(num_games, verbose=False)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "20f198ba-fa8c-47b8-bc4e-5319b7f6047d"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_eef83994.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "46f3d4e1-c487-4283-d743-fb7c9fa874e5"
   },
   "outputs": [],
   "source": [
    "win_rate_player1 = result[0] / num_games\n",
    "print('\\n Win rate for player 1 over {} games: {}%'.format(num_games, win_rate_player1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5.1. Comparing a player that samples from the action probablities versus the policy player which takes the maximum probability\n",
    "We can see that the greedy policy player achieves better performance than the sample based policy player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6ba90e57-a2df-40dc-e8cf-c40519fbb295"
   },
   "outputs": [],
   "source": [
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet, greedy=False).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "77d86ff8-ad57-49ee-8849-8f48eb1d1bc8"
   },
   "outputs": [],
   "source": [
    "win_rate_player1 = result[0]/num_games\n",
    "print('\\n Win rate for player 1 over {} games: {}%'.format(num_games, win_rate_player1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5.2. Compare greedy policy based player versus value based player "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6a08b5a3-a0ae-40db-f302-131243c36948"
   },
   "outputs": [],
   "source": [
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet).play\n",
    "player2 = ValueBasedPlayer(game, vnet).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2b314f31-8da2-40bf-b083-369f0759d285"
   },
   "outputs": [],
   "source": [
    "win_rate_player1 = result[0]/num_games\n",
    "print('\\n Win rate for player 1 over {} games: {}%'.format(num_games, win_rate_player1*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Plan using Monte Carlo rollouts\n",
    "\n",
    "\n",
    "**Goal**: \n",
    "Teach the students the core idea behind using simulated rollouts to understand the future and value actions.\n",
    "\n",
    "\n",
    "**Exercise**: \n",
    "* Build a loop to run Monte Carlo simulations using the policy network.\n",
    "* Use this to obtain better estimates of the value of moves.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "1627d47b-d7b0-43f5-c2b7-3214359e0c15"
   },
   "outputs": [],
   "source": [
    "#@title Video 6: Video 6 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 6.1: `MonteCarlo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarlo():\n",
    "  def __init__(self, game, nnet, args):\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "\n",
    "    self.Ps = {}  # stores initial policy (returned by neural net)\n",
    "    self.Es = {}  # stores game.getGameEnded ended for board s\n",
    "\n",
    "  # call this rollout\n",
    "  def simulate(self, canonicalBoard):\n",
    "    \"\"\"\n",
    "    This function performs one monte carlo rollout\n",
    "    \"\"\"\n",
    "\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    init_start_state = s\n",
    "    temp_v = 0\n",
    "    isfirstAction = None\n",
    "\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Build the loop\")\n",
    "    #################################################\n",
    "    for i in range(self.args.maxDepth): # maxDepth\n",
    "\n",
    "      if s not in self.Es:\n",
    "        self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "      if self.Es[s] != 0:\n",
    "        # terminal state\n",
    "        temp_v= -self.Es[s]\n",
    "        break\n",
    "\n",
    "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "      valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "      self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
    "      sum_Ps_s = np.sum(self.Ps[s])\n",
    "\n",
    "      if sum_Ps_s > 0:\n",
    "        self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "      else:\n",
    "        # if all valid moves were masked make all valid moves equally probable\n",
    "        # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "        # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
    "        log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "        self.Ps[s] = self.Ps[s] + valids\n",
    "        self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "      #################################################\n",
    "      ## TODO for students: details of what they should do ##\n",
    "      # Fill out function and remove\n",
    "      raise NotImplementedError(\"Take the action, find the next state\")\n",
    "      #################################################\n",
    "      # Take a random action\n",
    "      a = ...\n",
    "      # Find the next state and the next player\n",
    "      next_s, next_player = self.game.getNextState(..., ..., ...)\n",
    "      next_s = self.game.getCanonicalForm(..., ...)\n",
    "\n",
    "      s = self.game.stringRepresentation(next_s)\n",
    "      temp_v = v\n",
    "\n",
    "    return temp_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_649acaf2.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 7: Use Monte Carlo simulations to play games\n",
    "\n",
    "**Goal:** \n",
    "Teach students how to use simple Monte Carlo planning to play games.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "de0edf56-ed91-494c-8095-b32d07579ce5"
   },
   "outputs": [],
   "source": [
    "#@title Video 7: Video 7 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Exercise 7.1: Monte-Carlo simulations\n",
    "\n",
    "* Incorporate Monte Carlo simulations into an agent.\n",
    "* Run the resulting player versus the random, value-based, and policy-based players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloBasedPlayer():\n",
    "  def __init__(self, game, nnet, args):\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Use Monte Carlo!\")\n",
    "    #################################################\n",
    "    self.mc = ...\n",
    "    self.K = self.args.mc_topk\n",
    "\n",
    "  def play(self, canonicalBoard):\n",
    "    self.qsa = []\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    Ps, v = self.nnet.predict(canonicalBoard)\n",
    "    valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "    Ps = Ps * valids  # masking invalid moves\n",
    "    sum_Ps_s = np.sum(Ps)\n",
    "\n",
    "    if sum_Ps_s > 0:\n",
    "      Ps /= sum_Ps_s  # renormalize\n",
    "    else:\n",
    "      # if all valid moves were masked make all valid moves equally probable\n",
    "      # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "      # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
    "      log = logging.getLogger(__name__)\n",
    "      log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "      Ps = Ps + valids\n",
    "      Ps /= np.sum(Ps)\n",
    "\n",
    "    num_valid_actions = np.shape(np.nonzero(Ps))[1]\n",
    "\n",
    "    if num_valid_actions < self.K:\n",
    "      top_k_actions = np.argpartition(Ps,-num_valid_actions)[-num_valid_actions:]\n",
    "    else:\n",
    "      top_k_actions = np.argpartition(Ps,-self.K)[-self.K:]  # to get actions that belongs to top k prob\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Loop for the top actions\")\n",
    "    #################################################\n",
    "    for action in ...:\n",
    "      next_s, next_player = self.game.getNextState(..., ..., ...)\n",
    "      next_s = self.game.getCanonicalForm(..., ...)\n",
    "\n",
    "      values = []\n",
    "\n",
    "      # do some rollouts\n",
    "      for rollout in range(self.args.numMCsims):\n",
    "        value = self.mc.simulate(canonicalBoard)\n",
    "        values.append(value)\n",
    "\n",
    "      # average out values\n",
    "      avg_value = np.mean(values)\n",
    "      self.qsa.append((avg_value, action))\n",
    "\n",
    "    self.qsa.sort(key=lambda a: a[0])\n",
    "    self.qsa.reverse()\n",
    "    best_action = self.qsa[0][1]\n",
    "    return best_action\n",
    "\n",
    "  def getActionProb(self, canonicalBoard, temp=1):\n",
    "    if self.game.getGameEnded(canonicalBoard, 1) != 0:\n",
    "      return np.zeros((self.game.getActionSize()))\n",
    "\n",
    "    else:\n",
    "      action_probs = np.zeros((self.game.getActionSize()))\n",
    "      best_action = self.play(canonicalBoard)\n",
    "      action_probs[best_action] = 1\n",
    "\n",
    "    return action_probs\n",
    "\n",
    "\n",
    "game = OthelloGame(6)\n",
    "rp = RandomPlayer(game).play  # all players\n",
    "num_games = 20  # Feel free to change this number\n",
    "\n",
    "n1 = NNet(game)  # nNet players\n",
    "args1 = dotdict({'numMCsims': 10, 'maxRollouts':5, 'maxDepth':5, 'mc_topk': 3})\n",
    "\n",
    "## Uncomment below to check Monte Carlo agent!\n",
    "# mc1 = MonteCarloBasedPlayer(game, n1, args1)\n",
    "# n1p = lambda x: np.argmax(mc1.getActionProb(x))\n",
    "# arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
    "# MC_result = arena.playGames(num_games, verbose=False)\n",
    "# print(\"\\n Number of games won by player1 = {}, num of games won by player2 = {}, num of games won by nobody = {} out of {} games\" .format(MC_result[0], MC_result[1], MC_result[2], num_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "f36ad0d2-d346-4fd5-c741-6e12c29f4457"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_bd1c8ee0.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 8: Plan using Monte Carlo Tree Search\n",
    "\n",
    "**Goal:** \n",
    "Teach students to understand the core ideas behind Monte Carlo Tree Search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "f0d50d8f-9dbd-4be3-f02e-6c7426aa6b09"
   },
   "outputs": [],
   "source": [
    "#@title Video 8: Video 8 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 8.1: MCTS planner\n",
    "\n",
    "* Plug together pre-built Selection, Expansion & Backpropagation code to complete an MCTS planner.\n",
    "* Deploy the MCTS planner to understand an interesting position, producing value estimates and action counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "  \"\"\"\n",
    "  This class handles the MCTS tree.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    self.Qsa = {}    # stores Q values for s,a (as defined in the paper)\n",
    "    self.Nsa = {}    # stores #times edge s,a was visited\n",
    "    self.Ns = {}     # stores #times board s was visited\n",
    "    self.Ps = {}     # stores initial policy (returned by neural net)\n",
    "    self.Es = {}     # stores game.getGameEnded ended for board s\n",
    "    self.Vs = {}     # stores game.getValidMoves for board s\n",
    "\n",
    "  def search(self, canonicalBoard):\n",
    "    \"\"\"\n",
    "    This function performs one iteration of MCTS. It is recursively called\n",
    "    till a leaf node is found. The action chosen at each node is one that\n",
    "    has the maximum upper confidence bound as in the paper.\n",
    "\n",
    "    Once a leaf node is found, the neural network is called to return an\n",
    "    initial policy P and a value v for the state. This value is propagated\n",
    "    up the search path. In case the leaf node is a terminal state, the\n",
    "    outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n",
    "    updated.\n",
    "\n",
    "    NOTE: the return values are the negative of the value of the current\n",
    "    state. This is done since v is in [-1,1] and if v is the value of a\n",
    "    state for the current player, then its value is -v for the other player.\n",
    "\n",
    "    Returns:\n",
    "        v: the negative of the value of the current canonicalBoard\n",
    "    \"\"\"\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "\n",
    "    if s not in self.Es:\n",
    "      self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "    if self.Es[s] != 0:\n",
    "      # terminal node\n",
    "      return -self.Es[s]\n",
    "\n",
    "    if s not in self.Ps:\n",
    "      # leaf node\n",
    "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "      valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "      self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n",
    "      sum_Ps_s = np.sum(self.Ps[s])\n",
    "      if sum_Ps_s > 0:\n",
    "        self.Ps[s] /= sum_Ps_s  # renormalize\n",
    "      else:\n",
    "        # if all valid moves were masked make all valid moves equally probable\n",
    "        # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "        # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
    "        log = logging.getLogger(__name__)\n",
    "        log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "        self.Ps[s] = self.Ps[s] + valids\n",
    "        self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "      self.Vs[s] = valids\n",
    "      self.Ns[s] = 0\n",
    "\n",
    "      return -v\n",
    "\n",
    "    valids = self.Vs[s]\n",
    "    cur_best = -float('inf')\n",
    "    best_act = -1\n",
    "\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Complete the for loop\")\n",
    "    #################################################\n",
    "    # pick the action with the highest upper confidence bound\n",
    "    for a in range(self.game.getActionSize()):\n",
    "      if valids[a]:\n",
    "        if (s, a) in self.Qsa:\n",
    "          u = ... + ... * ... * math.sqrt(...) / (1 + ...)\n",
    "        else:\n",
    "          u = ... * ... * math.sqrt(... + 1e-8)\n",
    "\n",
    "        if u > cur_best:\n",
    "          cur_best = u\n",
    "          best_act = a\n",
    "\n",
    "    a = best_act\n",
    "    next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "    next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "    v = self.search(next_s)\n",
    "\n",
    "    if (s, a) in self.Qsa:\n",
    "      self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n",
    "      self.Nsa[(s, a)] += 1\n",
    "\n",
    "    else:\n",
    "      self.Qsa[(s, a)] = v\n",
    "      self.Nsa[(s, a)] = 1\n",
    "\n",
    "    self.Ns[s] += 1\n",
    "    return -v\n",
    "\n",
    "  def getNsa(self):\n",
    "    return self.Nsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_b71e46db.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 9: Use MCTS to play games \n",
    "\n",
    "**Goal:** \n",
    "Teach the students how to use the results of an MCTS to play games.\n",
    "\n",
    "**Exercise:** \n",
    "* Plug the MCTS planner into an agent.\n",
    "* Play games against other agents.\n",
    "* Explore the contributions of prior network, value function, number of simulations / time to play, and explore/exploit parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "outputId": "02cf6207-1f37-4b5e-860f-3c006b3e671e"
   },
   "outputs": [],
   "source": [
    "#@title Video 9: Video 9 Name\n",
    "# Insert the ID of the corresponding youtube video\n",
    "from IPython.display import YouTubeVideo\n",
    "video = YouTubeVideo(id=\"\", width=854, height=480, fs=1)\n",
    "print(\"Video available at https://youtu.be/\" + video.id)\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Exercise 9.1: Agent that uses an MCTS planner\n",
    "\n",
    "* Plug the MCTS planner into an agent.\n",
    "* Play games against other agents.\n",
    "* Explore the contributions of prior network, value function, number of simulations / time to play, and explore/exploit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearchBasedPlayer():\n",
    "  def __init__(self, game, nnet, args):\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    #################################################\n",
    "    ## TODO for students: details of what they should do ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Plug the planner\")\n",
    "    #################################################\n",
    "    self.mcts = MCTS(game, nnet, args)\n",
    "\n",
    "  def play(self, canonicalBoard, temp=1):\n",
    "    for i in range(self.args.numMCTSSims):\n",
    "      self.mcts.search(canonicalBoard)\n",
    "\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    self.Nsa = self.mcts.getNsa()\n",
    "    self.counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n",
    "\n",
    "    if temp == 0:\n",
    "      bestAs = np.array(np.argwhere(self.counts == np.max(self.counts))).flatten()\n",
    "      bestA = np.random.choice(bestAs)\n",
    "      probs = [0] * len(self.counts)\n",
    "      probs[bestA] = 1\n",
    "      return probs\n",
    "\n",
    "    self.counts = [x ** (1. / temp) for x in self.counts]\n",
    "    self.counts_sum = float(sum(self.counts))\n",
    "    probs = [x / self.counts_sum for x in self.counts]\n",
    "    return np.argmax(probs)\n",
    "\n",
    "  def getActionProb(self, canonicalBoard, temp=1):\n",
    "    action_probs = np.zeros((self.game.getActionSize()))\n",
    "    best_action = self.play(canonicalBoard)\n",
    "    action_probs[best_action] = 1\n",
    "\n",
    "    return action_probs\n",
    "\n",
    "\n",
    "game = OthelloGame(6)\n",
    "rp = RandomPlayer(game).play  # all players\n",
    "num_games = 20  # games\n",
    "n1 = NNet(game)  # nnet players\n",
    "args1 = dotdict({'numMCTSSims': 50, 'cpuct':1.0})\n",
    "## Uncomment below to check your agent!\n",
    "# mcts1 = MonteCarloTreeSearchBasedPlayer(game, n1, args1)\n",
    "# n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n",
    "# arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
    "# MCTS_result = arena.playGames(num_games, verbose=False)\n",
    "# print(\"\\n Number of games won by player1 = {}, num of games won by player2 = {}, num of games won by nobody = {} out of {} games\" .format(MCTS_result[0], MCTS_result[1], MCTS_result[2], num_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "text",
    "outputId": "2ef036a0-4760-40ec-acf0-ac2375238096"
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W3D3_ReinforcementLearningForGames/solutions/W3D3_Tutorial1_Solution_80345b99.py)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qQR1gJBX-7GC"
   ],
   "include_colab_link": true,
   "name": "W3D3_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
