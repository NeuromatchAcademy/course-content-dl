{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D4_GenerativeModels/student/W2D4_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Diffusion models\n",
    "\n",
    "**Week 2, Day 4: Generative Models**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Binxu Wang\n",
    "\n",
    "__Content reviewers:__ Shaonan Wang, Dongrui Deng, Dora Zhiyu Yang, Adrita Das\n",
    "\n",
    "__Content editors:__ Shaonan Wang\n",
    "\n",
    "__Production editors:__ Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "* Understand the idea behind Diffusion generative models: using score to enable reversal of diffusion process.\n",
    "* Learn the score function by learning to denoise data.\n",
    "* Hands-on experience in learning the score to a generate certain distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "from IPython.display import IFrame\n",
    "link_id = \"j89qg\"\n",
    "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install pillow --quiet\n",
    "!pip install seaborn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip3 install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_dl\",\n",
    "            \"user_key\": \"f379rz8y\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W2D4_T2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import ipywidgets as widgets  # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.lines as mlines\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "# You may have functions that plot results that aren't\n",
    "# particularly interesting. You can add these here to hide them.\n",
    "\n",
    "def plotting_z(z):\n",
    "  \"\"\"This function multiplies every element in an array by a provided value\n",
    "\n",
    "  Args:\n",
    "    z (ndarray): neural activity over time, shape (T, ) where T is number of timesteps\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "\n",
    "  ax.plot(z)\n",
    "  ax.set(\n",
    "      xlabel='Time (s)',\n",
    "      ylabel='Z',\n",
    "      title='Neural activity over time'\n",
    "      )\n",
    "\n",
    "\n",
    "def kdeplot(pnts, label=\"\", ax=None, titlestr=None, handles=[], color=\"\", **kwargs):\n",
    "  if ax is None:\n",
    "    ax = plt.gca()#figh, axs = plt.subplots(1,1,figsize=[6.5, 6])\n",
    "  sns.kdeplot(x=pnts[:,0], y=pnts[:,1], ax=ax, label=label, color=color, **kwargs)\n",
    "  handles.append(mlines.Line2D([], [], color=color, label=label))\n",
    "  if titlestr is not None:\n",
    "    ax.set_title(titlestr)\n",
    "\n",
    "\n",
    "def quiver_plot(pnts, vecs, *args, **kwargs):\n",
    "  plt.quiver(pnts[:, 0], pnts[:,1], vecs[:, 0], vecs[:, 1], *args, **kwargs)\n",
    "\n",
    "\n",
    "def gmm_pdf_contour_plot(gmm, xlim=None,ylim=None,ticks=100,logprob=False,label=None,**kwargs):\n",
    "    if xlim is None:\n",
    "        xlim = plt.xlim()\n",
    "    if ylim is None:\n",
    "        ylim = plt.ylim()\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, ticks), np.linspace(*ylim, ticks))\n",
    "    pdf = gmm.pdf(np.dstack((xx,yy)))\n",
    "    if logprob:\n",
    "        pdf = np.log(pdf)\n",
    "    plt.contour(xx, yy, pdf, **kwargs,)\n",
    "\n",
    "\n",
    "def visualize_diffusion_distr(x_traj_rev, leftT=0, rightT=-1, explabel=\"\"):\n",
    "  if rightT == -1:\n",
    "    rightT = x_traj_rev.shape[2]-1\n",
    "  figh, axs = plt.subplots(1,2,figsize=[12,6])\n",
    "  sns.kdeplot(x=x_traj_rev[:,0,leftT], y=x_traj_rev[:,1,leftT], ax=axs[0])\n",
    "  axs[0].set_title(\"Density of Gaussian Prior of $x_T$\\n before reverse diffusion\")\n",
    "  plt.axis(\"equal\")\n",
    "  sns.kdeplot(x=x_traj_rev[:,0,rightT], y=x_traj_rev[:,1,rightT], ax=axs[1])\n",
    "  axs[1].set_title(f\"Density of $x_0$ samples after {rightT} step reverse diffusion\")\n",
    "  plt.axis(\"equal\")\n",
    "  plt.suptitle(explabel)\n",
    "  return figh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "# Section 1: Understanding Score and Diffusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Notes: score-based model vs. diffusion model\n",
    "\n",
    "In the field, [**score-based model**](https://yang-song.net/blog/2021/score/) and [**diffusion models**](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/) are often used interchangeably. At first, they were developed semi-independently, so they have different formulations and notations. On the surface.\n",
    "\n",
    "* Diffusion model uses a discrete Markov chain as a forward process. The objective is derived via the Evidence Lower Bound (ELBO) of the latent model.\n",
    "* Score-based model usually uses continuous-time stochastic differential equation (SDE). The objective is derived via denoising score matching.\n",
    "\n",
    "In the end, they were found to be equivalent, as one is roughly the discretization of the other. Here we focus on the **continuous time** framework, as it's conceptual simplicity similar to [this synopsis](https://arxiv.org/abs/2206.00364)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Intro and Principles\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'a9uLb8Pf4pM'), ('Bilibili', 'BV1kV411g7gy')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Intro_and_Principles_Video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Math Behind Diffusion\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'qDXPZqYm-1g'), ('Bilibili', 'BV19a4y1c7We')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Math_behind_diffusion_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.1:  Diffusion Process\n",
    "\n",
    "In this section, we'd like to understand the forward diffusion process, and gain intuition about how diffusion turns data into \"noise\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this tutorial, we will use the process also known as Variance Exploding SDE (VPSDE) in diffusion literature.\n",
    "\n",
    "\\begin{equation}\n",
    "d\\mathbf x=g(t)d\\mathbf w\n",
    "\\end{equation}\n",
    "\n",
    "$d\\mathbf w$ is the differential of the Wiener process, which is like the Gaussian random noise; $g(t)$ is the diffusion coefficient at time $t$. In our code, we can discretize it as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{t+\\Delta t} = \\mathbf{x}_{t}+g(t) \\sqrt{\\Delta t} z_t\n",
    "\\end{equation}\n",
    "\n",
    "where $z_t\\sim \\mathcal{N} (0,I)$ are independent and identically distributed (i.i.d.) normal random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Given an initial state $\\mathbf{x}_0$ the conditional distribution of $\\mathbf{x}_t$ is a Gaussian around $\\mathbf x_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x}_t\\mid \\mathbf{x}_0) = \\mathcal N(\\mathbf{x}_t;\\mathbf{x}_0,\\sigma_t^2 I)\n",
    "\\end{equation}\n",
    "\n",
    "The key quantity to note is the $\\sigma_t$ which is the integrated noise scale at time $t$. $I$ denotes the identity matrix.\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_t^2=∫_0^t g^2(\\tau)d\\tau\n",
    "\\end{equation}\n",
    "\n",
    "Marginalizing over all the initial states, the distribution of $\\mathbf x_t$ is $p_t(x_t)$, i.e., convolving a Gaussian over the initial data distribution $p_0(\\mathbf x_0)$ which blurs the data up.\n",
    "\n",
    "\\begin{equation}\n",
    "p_t(\\mathbf{x}_t) = \\int_{\\mathbf x_0} p_0(\\mathbf x_0)\\mathcal N(\\mathbf{x}_t;\\mathbf{x}_0,\\sigma_t^2 I)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 1.1: Visualizing diffusion\n",
    "\n",
    "Here, we will examine the evolution of the density of a distribution $p_t(\\mathbf{x})$ undergoing forward diffusion. In this case, we let $g(t)=\\lambda^{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title 1D diffusion process\n",
    "@widgets.interact\n",
    "def diffusion_1d_forward(Lambda=(0, 50, 1), ):\n",
    "  np.random.seed(0)\n",
    "  timesteps = 100\n",
    "  sampleN = 200\n",
    "  t = np.linspace(0, 1, timesteps)\n",
    "  # Generate random normal samples for the Wiener process\n",
    "  dw = np.random.normal(0, np.sqrt(t[1] - t[0]), size=(len(t), sampleN))  # Three-dimensional array for multiple trajectories\n",
    "  # Sample initial positions from a bimodal distribution\n",
    "  x0 = np.concatenate((np.random.normal(-5, 1, size=(sampleN//2)),\n",
    "                       np.random.normal(5, 1, size=(sampleN - sampleN//2))), axis=-1)\n",
    "  # Compute the diffusion process for multiple trajectories\n",
    "  x = np.cumsum((Lambda**t[:,None]) * dw, axis=0) + x0.reshape(1,sampleN)  # Broadcasting x0 to match the shape of dw\n",
    "  # Plot the diffusion process\n",
    "  plt.plot(t, x[:,:sampleN//2], alpha=0.1, color=\"r\") # traj from first mode\n",
    "  plt.plot(t, x[:,sampleN//2:], alpha=0.1, color=\"b\") # traj from second mode\n",
    "  plt.xlabel('Time')\n",
    "  plt.ylabel('x')\n",
    "  plt.title('Diffusion Process with $g(t)=\\lambda^{t}$'+f' $\\lambda$={Lambda}')\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title 2D diffusion process\n",
    "# @markdown (the animation takes a while to render)\n",
    "Lambda = 26  # @param {type:\"slider\", min:1, max:50, step:1}\n",
    "timesteps = 50\n",
    "sampleN = 200\n",
    "t = np.linspace(0, 1, timesteps)\n",
    "# Generate random normal samples for the Wiener process\n",
    "dw = np.random.normal(0, np.sqrt(t[1] - t[0]), size=(len(t), 2, sampleN))  # Three-dimensional array for multiple trajectories\n",
    "# Sample initial positions from a bimodal distribution\n",
    "x0 = np.concatenate((np.random.normal(-2, .2, size=(2,sampleN//2)),\n",
    "                     np.random.normal(2, .2, size=(2,sampleN - sampleN//2))),\n",
    "                    axis=-1)\n",
    "# Compute the diffusion process for multiple trajectories\n",
    "x = np.cumsum((Lambda**t)[:, None, None] * dw, axis=0) + x0[None, :, :]  # Broadcasting x0 to match the shape of dw\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.set_xlim(-25, 25)\n",
    "ax.set_ylim(-25, 25)\n",
    "ax.axis(\"image\")\n",
    "# Create an empty scatter plot\n",
    "scatter1 = ax.scatter([], [], color=\"r\", alpha=0.5)\n",
    "scatter2 = ax.scatter([], [], color=\"b\", alpha=0.5)\n",
    "# Update function for the animation\n",
    "def update(frame):\n",
    "  ax.set_title(f'Time Step: {frame}')\n",
    "  scatter1.set_offsets(x[frame, :, :sampleN//2].T)\n",
    "  scatter2.set_offsets(x[frame, :, sampleN//2:].T)\n",
    "  return scatter1, scatter2\n",
    "\n",
    "# Create the animation\n",
    "animation = FuncAnimation(fig, update, frames=range(timesteps), interval=100, blit=True)\n",
    "# Display the animation\n",
    "plt.close()  # Prevents displaying the initial static plot\n",
    "HTML(animation.to_html5_video()) #  to_jshtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Visualizing_Diffusion_Interactive_Demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.2:  What is Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The big idea of diffusion model is to use the **\"score\" function** to reverse the diffusion process. So what is score, what's the intuition to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The **Score** is the gradient of the log data distribution, so it tells us which direction to go to increase the probability of data.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{s}(\\mathbf{x})=\\nabla \\log p(\\mathbf{x})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.2: Score for Gaussian Mixtures\n",
    "\n",
    "In this exercise, you will explore the score function of a Gaussian mixture to gain more intuition about its geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title  Custom Gaussian Mixture class\n",
    "# @markdown *Execute this cell to define the class Gaussian Mixture Model for our exercise*\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GaussianMixture:\n",
    "  def __init__(self, mus, covs, weights):\n",
    "    \"\"\"\n",
    "    mus: a list of K 1d np arrays (D,)\n",
    "    covs: a list of K 2d np arrays (D, D)\n",
    "    weights: a list or array of K unnormalized non-negative weights, signifying the possibility of sampling from each branch.\n",
    "      They will be normalized to sum to 1. If they sum to zero, it will err.\n",
    "    \"\"\"\n",
    "    self.n_component = len(mus)\n",
    "    self.mus = mus\n",
    "    self.covs = covs\n",
    "    self.precs = [np.linalg.inv(cov) for cov in covs]\n",
    "    self.weights = np.array(weights)\n",
    "    self.norm_weights = self.weights / self.weights.sum()\n",
    "    self.RVs = []\n",
    "    for i in range(len(mus)):\n",
    "      self.RVs.append(multivariate_normal(mus[i], covs[i]))\n",
    "    self.dim = len(mus[0])\n",
    "\n",
    "  def add_component(self, mu, cov, weight=1):\n",
    "    self.mus.append(mu)\n",
    "    self.covs.append(cov)\n",
    "    self.precs.append(np.linalg.inv(cov))\n",
    "    self.RVs.append(multivariate_normal(mu, cov))\n",
    "    self.weights.append(weight)\n",
    "    self.norm_weights = self.weights / self.weights.sum()\n",
    "    self.n_component += 1\n",
    "\n",
    "  def pdf_decompose(self, x):\n",
    "    \"\"\"\n",
    "      probability density (PDF) at $x$.\n",
    "    \"\"\"\n",
    "    component_pdf = []\n",
    "    prob = None\n",
    "    for weight, RV in zip(self.norm_weights, self.RVs):\n",
    "        pdf = weight * RV.pdf(x)\n",
    "        prob = pdf if prob is None else (prob + pdf)\n",
    "        component_pdf.append(pdf)\n",
    "    component_pdf = np.array(component_pdf)\n",
    "    return prob, component_pdf\n",
    "\n",
    "  def pdf(self, x):\n",
    "    \"\"\"\n",
    "      probability density (PDF) at $x$.\n",
    "    \"\"\"\n",
    "    prob = None\n",
    "    for weight, RV in zip(self.norm_weights, self.RVs):\n",
    "        pdf = weight * RV.pdf(x)\n",
    "        prob = pdf if prob is None else (prob + pdf)\n",
    "    # component_pdf = np.array([rv.pdf(x) for rv in self.RVs]).T\n",
    "    # prob = np.dot(component_pdf, self.norm_weights)\n",
    "    return prob\n",
    "\n",
    "  def score(self, x):\n",
    "    \"\"\"\n",
    "    Compute the score $\\nabla_x \\log p(x)$ for the given $x$.\n",
    "    \"\"\"\n",
    "    component_pdf = np.array([rv.pdf(x) for rv in self.RVs]).T\n",
    "    weighted_compon_pdf = component_pdf * self.norm_weights[np.newaxis, :]\n",
    "    participance = weighted_compon_pdf / weighted_compon_pdf.sum(axis=1, keepdims=True)\n",
    "\n",
    "    scores = np.zeros_like(x)\n",
    "    for i in range(self.n_component):\n",
    "      gradvec = - (x - self.mus[i]) @ self.precs[i]\n",
    "      scores += participance[:, i:i+1] * gradvec\n",
    "\n",
    "    return scores\n",
    "\n",
    "  def score_decompose(self, x):\n",
    "    \"\"\"\n",
    "    Compute the grad to each branch for the score $\\nabla_x \\log p(x)$ for the given $x$.\n",
    "    \"\"\"\n",
    "    component_pdf = np.array([rv.pdf(x) for rv in self.RVs]).T\n",
    "    weighted_compon_pdf = component_pdf * self.norm_weights[np.newaxis, :]\n",
    "    participance = weighted_compon_pdf / weighted_compon_pdf.sum(axis=1, keepdims=True)\n",
    "\n",
    "    gradvec_list = []\n",
    "    for i in range(self.n_component):\n",
    "      gradvec = - (x - self.mus[i]) @ self.precs[i]\n",
    "      gradvec_list.append(gradvec)\n",
    "      # scores += participance[:, i:i+1] * gradvec\n",
    "\n",
    "    return gradvec_list, participance\n",
    "\n",
    "  def sample(self, N):\n",
    "    \"\"\" Draw N samples from Gaussian mixture\n",
    "    Procedure:\n",
    "      Draw N samples from each Gaussian\n",
    "      Draw N indices, according to the weights.\n",
    "      Choose sample between the branches according to the indices.\n",
    "    \"\"\"\n",
    "    rand_component = np.random.choice(self.n_component, size=N, p=self.norm_weights)\n",
    "    all_samples = np.array([rv.rvs(N) for rv in self.RVs])\n",
    "    gmm_samps = all_samples[rand_component, np.arange(N),:]\n",
    "    return gmm_samps, rand_component, all_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Example: Gaussian mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Gaussian mixture\n",
    "mu1 = np.array([0, 1.0])\n",
    "Cov1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "\n",
    "mu2 = np.array([2.0, -1.0])\n",
    "Cov2 = np.array([[2.0, 0.5], [0.5, 1.0]])\n",
    "\n",
    "gmm = GaussianMixture([mu1, mu2],[Cov1, Cov2], [1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualize log density\n",
    "show_samples = True  # @param {type:\"boolean\"}\n",
    "np.random.seed(42)\n",
    "gmm_samples, _, _ = gmm.sample(5000)\n",
    "plt.figure(figsize=[8, 8])\n",
    "plt.scatter(gmm_samples[:, 0],\n",
    "            gmm_samples[:, 1],\n",
    "            s=10,\n",
    "            alpha=0.4 if show_samples else 0.0)\n",
    "gmm_pdf_contour_plot(gmm, cmap=\"Greys\", levels=20, logprob=True)\n",
    "plt.title(\"log density of gaussian mixture $\\log p(x)$\")\n",
    "plt.axis(\"image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualize Score\n",
    "set_seed(2023)\n",
    "gmm_samps_few, _, _ = gmm.sample(200)\n",
    "scorevecs_few = gmm.score(gmm_samps_few)\n",
    "gradvec_list, participance = gmm.score_decompose(gmm_samps_few)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Score for Gaussian mixture\n",
    "plt.figure(figsize=[8, 8])\n",
    "quiver_plot(gmm_samps_few, scorevecs_few,\n",
    "            color=\"black\", scale=25, alpha=0.7, width=0.003,\n",
    "            label=\"score of GMM\")\n",
    "gmm_pdf_contour_plot(gmm, cmap=\"Greys\")\n",
    "plt.title(\"Score vector field $\\\\nabla\\log p(x)$ for Gaussian Mixture\")\n",
    "plt.axis(\"image\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Score for each Gaussian mode\n",
    "plt.figure(figsize=[8, 8])\n",
    "quiver_plot(gmm_samps_few, gradvec_list[0],\n",
    "            color=\"blue\", alpha=0.4, scale=45,\n",
    "            label=\"score of gauss mode1\")\n",
    "quiver_plot(gmm_samps_few, gradvec_list[1],\n",
    "            color=\"orange\", alpha=0.4, scale=45,\n",
    "            label=\"score of gauss mode2\")\n",
    "gmm_pdf_contour_plot(gmm.RVs[0], cmap=\"Blues\")\n",
    "gmm_pdf_contour_plot(gmm.RVs[1], cmap=\"Oranges\")\n",
    "plt.title(\"Score vector field $\\\\nabla\\log p(x)$ for individual Gaussian modes\")\n",
    "plt.axis(\"image\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Compare Score of individual mode with that of the mixture.\n",
    "plt.figure(figsize=[8, 8])\n",
    "quiver_plot(gmm_samps_few, gradvec_list[0]*participance[:, 0:1],\n",
    "            color=\"blue\", alpha=0.6, scale=25,\n",
    "            label=\"weighted score of gauss mode1\")\n",
    "quiver_plot(gmm_samps_few, gradvec_list[1]*participance[:, 1:2],\n",
    "            color=\"orange\", alpha=0.6, scale=25,\n",
    "            label=\"weighted score of gauss mode2\")\n",
    "quiver_plot(gmm_samps_few, scorevecs_few, color=\"black\", scale=25, alpha=0.7,\n",
    "            width=0.003, label=\"score of GMM\")\n",
    "gmm_pdf_contour_plot(gmm.RVs[0], cmap=\"Blues\")\n",
    "gmm_pdf_contour_plot(gmm.RVs[1], cmap=\"Oranges\")\n",
    "plt.title(\"Score vector field $\\\\nabla\\log p(x)$ of mixture\")\n",
    "plt.axis(\"image\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 1.2: What does score tell us?\n",
    "\n",
    "What does the score's **magnitude and direction** tell us in general?\n",
    "\n",
    "For a **multi-modal distribution**, how does the score of the individual mode relate to the overall score?\n",
    "\n",
    "Take 2 minutes to think in silence, then discuss as a group (~10 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W2D4_GenerativeModels/solutions/W2D4_Tutorial2_Solution_df13ac7f.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_What_does_score_tell_us_Discussion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.3:  Reverse Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "After getting some intuition about the score function, we are now well-equipped to reverse the diffusion process!\n",
    "\n",
    "There is [a result in stochastic process literature](https://www.sciencedirect.com/science/article/pii/0304414982900515) that if we have the forward process\n",
    "\n",
    "\\begin{equation}\n",
    "d\\mathbf{x} = g(t)d \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "Then the following process (reverse SDE) will be its time reversal:\n",
    "\n",
    "\\begin{equation}\n",
    "d\\mathbf{x} = -g^2(t) \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x}) dt + g(t) d \\mathbf{w}.\n",
    "\\end{equation}\n",
    "\n",
    "where time $t$ runs backward.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Time Reversal**: The solution of forward SDE is a sequence of distribution $p_t(\\mathbf{x})$ from $t=0\\to T$. If we start the reverse SDE with the initial distribution $p_T(\\mathbf{x})$, then its solution will be the same sequence of distribution $p_t(\\mathbf{x})$, but only that $t=T\\to 0$.\n",
    "\n",
    "**Note:** For the general form of this result, see the Bonus section at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Implication**  This time reversal is the foundation of the Diffusion model. We can use an interesting distribution as $p_0(\\mathbf x)$ connects it with noise via forward diffusion.\n",
    "\n",
    "Then we can sample the noise and convert it back to data via the reverse diffusion process.\n",
    "\n",
    "<br>\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D1_TimeSeriesAndNaturalLanguageProcessing/static/noising_process.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.3: Score enables reversal of diffusion\n",
    "\n",
    "Here let's put our knowledge into action and see that the **score function** indeed enables the reverse diffusion and **the recovery of the initial distribution.**\n",
    "\n",
    "In the following cell, you are going to implement the discretization of the reverse diffusion equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{t-\\Delta t} = \\mathbf{x}_t + g(t)^2 s(\\mathbf{x}_t, t)\\Delta t + g(t)\\sqrt{\\Delta t} \\mathbf{z}_t\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{z}_t \\sim \\mathcal{N}(\\mathbf{0}, I)$ and $g(t)=\\lambda^t$.\n",
    "\n",
    "In fact, this is the *sampling equation* for diffusion models in its simplest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Helper functions: `sigma_t_square` and `diffuse_gmm`\n",
    "def sigma_t_square(t, Lambda):\n",
    "    \"\"\"Compute the noise variance \\sigma_t^2 of the conditional distribution\n",
    "    for forward process with g(t)=\\lambda^t\n",
    "\n",
    "    Formula\n",
    "      \\sigma_t^2 = \\frac{\\sigma^{2\\lambda} - 1}{2 \\ln(\\lambda)}\n",
    "\n",
    "    Args:\n",
    "      t (scalar or ndarray): time\n",
    "      Lambda (scalar): Lambda\n",
    "\n",
    "    Returns:\n",
    "      sigma_t^2\n",
    "    \"\"\"\n",
    "    return (Lambda**(2 * t) - 1) / (2 * np.log(Lambda))\n",
    "\n",
    "\n",
    "def sigma_t(t, Lambda):\n",
    "    \"\"\"Compute the noise std \\sigma_t of the conditional distribution\n",
    "    for forward process with g(t)=\\lambda^t\n",
    "\n",
    "    Formula\n",
    "      \\sigma_t =\\sqrt{ \\frac{\\sigma^{2\\lambda} - 1}{2 \\ln(\\lambda)}}\n",
    "\n",
    "    Args:\n",
    "      t (scalar or ndarray): time\n",
    "      Lambda (scalar): Lambda\n",
    "\n",
    "    Returns:\n",
    "      sigma_t\n",
    "    \"\"\"\n",
    "    return np.sqrt((Lambda**(2 * t) - 1) / (2 * np.log(Lambda)))\n",
    "\n",
    "\n",
    "def diffuse_gmm(gmm, t, Lambda):\n",
    "  \"\"\" Teleport a Gaussian Mixture distribution to $t$ by diffusion forward process\n",
    "\n",
    "  The distribution p_t(x) (still a Gaussian mixture)\n",
    "    following the forward diffusion SDE\n",
    "  \"\"\"\n",
    "  sigma_t_2 = sigma_t_square(t, Lambda)  # variance\n",
    "  noise_cov = np.eye(gmm.dim) * sigma_t_2\n",
    "  covs_dif = [cov + noise_cov for cov in gmm.covs]\n",
    "  return GaussianMixture(gmm.mus, covs_dif, gmm.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def reverse_diffusion_SDE_sampling_gmm(gmm, sampN=500, Lambda=5, nsteps=500):\n",
    "  \"\"\" Using exact score function to simulate the reverse SDE to sample from distribution.\n",
    "\n",
    "  gmm: Gausian Mixture model class defined above\n",
    "  sampN: Number of samples to generate\n",
    "  Lambda: the $\\lambda$ used in the diffusion coefficient $g(t)=\\lambda^t$\n",
    "  nsteps: how many discrete steps do we use to\n",
    "  \"\"\"\n",
    "  # initial distribution $N(0,sigma_T^2 I)$\n",
    "  sigmaT2 = sigma_t_square(1, Lambda)\n",
    "  xT = np.sqrt(sigmaT2) * np.random.randn(sampN, 2)\n",
    "  x_traj_rev = np.zeros((*xT.shape, nsteps, ))\n",
    "  x_traj_rev[:,:,0] = xT\n",
    "  dt = 1 / nsteps\n",
    "  for i in range(1, nsteps):\n",
    "    # note the time fly back $t$\n",
    "    t = 1 - i * dt\n",
    "\n",
    "    # Sample the Gaussian noise $z ~ N(0, I)$\n",
    "    eps_z = np.random.randn(*xT.shape)\n",
    "\n",
    "    # Transport the gmm to that at time $t$ and\n",
    "    gmm_t = diffuse_gmm(gmm, t, Lambda)\n",
    "    #################################################\n",
    "    ## TODO for students: implement the reverse SDE equation below\n",
    "    raise NotImplementedError(\"Student exercise: implement the reverse SDE equation\")\n",
    "    #################################################\n",
    "    # Compute the score at state $x_t$ and time $t$, $\\nabla \\log p_t(x_t)$\n",
    "    score_xt = gmm_t.score(...)\n",
    "    # Implement the one time step update equation\n",
    "    x_traj_rev[:, :, i] = x_traj_rev[:, :, i-1] + ...\n",
    "\n",
    "  return x_traj_rev\n",
    "\n",
    "\n",
    "## Uncomment the code below to test your function\n",
    "# set_seed(42)\n",
    "# x_traj_rev = reverse_diffusion_SDE_sampling_gmm(gmm, sampN=2500, Lambda=10, nsteps=200)\n",
    "# x0_rev = x_traj_rev[:, :, -1]\n",
    "# gmm_samples, _, _ = gmm.sample(2500)\n",
    "\n",
    "# figh, axs = plt.subplots(1, 1, figsize=[6.5, 6])\n",
    "# handles = []\n",
    "# kdeplot(x0_rev, \"Samples from Reverse Diffusion\", ax=axs, handles=handles, color=\"blue\")\n",
    "# kdeplot(gmm_samples, \"Samples from original GMM\", ax=axs, handles=handles, color=\"orange\")\n",
    "# gmm_pdf_contour_plot(gmm, cmap=\"Greys\", levels=20)  # the exact pdf contour of gmm\n",
    "# plt.legend(handles=handles)\n",
    "# figh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W2D4_GenerativeModels/solutions/W2D4_Tutorial2_Solution_c3981c22.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=625.0 height=575.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D4_GenerativeModels/static/W2D4_Tutorial2_Solution_c3981c22_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Score_enables_Reversal_of_Diffusion_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Learning the score by denoising\n",
    "\n",
    "So far, we have understood that the score function enables the time reversal of the diffusion process. But how to estimate it when we have no analytical form of the distribution?\n",
    "\n",
    "For real datasets, we usually have no access to their density, not to mention their score. However, we have a set of samples $\\{x_i\\}$ from it. The way we estimate the score is called denoising score matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "It can be shown that optimizing the upper objective, i.e., denoising score matching (DSM), is equivalent to optimizing the lower objective, i.e., explicit score matching, which minimizes the mean squared error (MSE) between the score model and the true time-dependent score.\n",
    "\n",
    "\\begin{equation}\n",
    "J_{DSM}(\\theta)=\\mathbb E_{x\\sim p_0(x)\\\\\\tilde x\\sim p_t(\\tilde x\\mid x)}\\|s_\\theta(\\tilde x)-\\nabla_\\tilde x\\log p_t(\\tilde x\\mid x)\\|^2\\\\\n",
    "J_{ESM}(\\theta)=\\mathbb E_{\\tilde x\\sim p_t(\\tilde x)}\\|s_\\theta(\\tilde x)-\\nabla_\\tilde x\\log p_t(\\tilde x)\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "In both cases, the optimal $s_\\theta(x)$ will be the same as the true score $\\nabla_\\tilde x\\log p_t(\\tilde x)$. Both objectives are equivalent in terms of their optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Using the fact that the forward process has Gaussian conditional distribution $p_t(\\tilde x\\mid x)= \\mathcal N(x,\\sigma^2_t I)$, the objective can be simplified even further!\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb E_{x\\sim p_0(x)}\\mathbb E_{z\\sim \\mathcal N(0,I)}\\|s_\\theta(x+\\sigma_t z)+\\frac{1}{\\sigma_t}z\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "To train the score model for all $t$ or noise levels, the objective is integrated over all time $t\\in[\\epsilon,1]$, with particular weighting $\\gamma_t$ of different times:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_\\epsilon^1dt \\gamma_t\\mathbb E_{x\\sim p_0(x)}\\mathbb E_{z\\sim \\mathcal N(0,I)}\\|s_\\theta(x+\\sigma_t z, t)+\\frac{1}{\\sigma_t}z\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "Here as a naive example, we choose weight $\\gamma_t=\\sigma_t^2$, which emphasizes the high noise period ($t\\sim 1$) more than the low noise period ($t\\sim 0$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_\\epsilon^1dt \\mathbb E_{x\\sim p_0(x)}\\mathbb E_{z\\sim \\mathcal N(0,I)}\\|\\sigma_t s_\\theta(x+\\sigma_t z, t)+z\\|^2\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To put it in plain language, this objective is simply doing the following steps:\n",
    "\n",
    "1. Sample clean data $x$ from training distribution $x\\sim p_0(x)$\n",
    "2. Sample noise of same shape from i.i.d. Gaussian $z\\sim \\mathcal N(0,I)$\n",
    "3. Sample time $t$ (or noise scale) and create noised data $\\tilde x=x+\\sigma_t z$\n",
    "4. Predict the scaled noise at $(\\tilde x,t)$ with neural network, minimize the MSE $\\|\\sigma_ts_\\theta(\\tilde x,t)+z\\|^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The diffusion model is a rapidly progressing field with many different formulations. So when reading papers, don't be scared! They are all the same beast under various disguises.\n",
    "\n",
    "* In many papers, including [stable diffusion](https://arxiv.org/abs/2112.10752), $-\\sigma_t$ is absorbed into the score model, such that the objective looks like $\\|\\tilde s_\\theta(x+\\sigma_t z, t)-z\\|^2$, which can be interpreted as inferring the noise from a noisy sample, highlighting the denoising nature.\n",
    "  * In our code and notebook, we used $\\|\\sigma_t s_\\theta(x+\\sigma_t z, t)+z\\|^2$ highlighting that it's matching the score.\n",
    "\n",
    "* Another kind of forward process, i.e., Variance Preserving SDE, will scale down the signal by $\\alpha_t$ while adding noise; for those, the objective will look like $\\|\\tilde s_\\theta(\\alpha_t x+\\sigma_t z, t)-z\\|^2$\n",
    "\n",
    "* What's the best weighting function $\\gamma_t$ and truncation $\\epsilon$ is still a active area of research. There are many heuristic ways of setting them in actual diffusion models; see these very recent publications:\n",
    "  * [arxiv:2303.00848](https://arxiv.org/abs/2303.00848)\n",
    "  * [arxiv:2206.00364](https://arxiv.org/abs/2206.00364)\n",
    "  * [arxiv:2106.05527](https://arxiv.org/abs/2106.05527)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 2: Denoising objective\n",
    "\n",
    "Recall the discussion on the interpretation of the score for multi-modal distribution. How does that connect to the denoising objective?\n",
    "\n",
    "In principle, can we optimize the score-matching objective to $0$, why?\n",
    "\n",
    "Take 2 minutes to think in silence, then discuss as a group (~10 minutes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W2D4_GenerativeModels/solutions/W2D4_Tutorial2_Solution_15da867a.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Denoising_objective_Discussion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2: Implementing Denoising Score Matching Objective\n",
    "\n",
    "In this exercise, you are going to implement the DSM objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def loss_fn(model, x, sigma_t_fun, eps=1e-5):\n",
    "  \"\"\"The loss function for training score-based generative models.\n",
    "\n",
    "  Args:\n",
    "    model: A PyTorch model instance that represents a\n",
    "      time-dependent score-based model.\n",
    "      it takes x, t as arguments.\n",
    "    x: A mini-batch of training data.\n",
    "    sigma_t_fun: A function that gives the standard deviation of the conditional dist.\n",
    "        p(x_t | x_0)\n",
    "    eps: A tolerance value for numerical stability, sample t uniformly from [eps, 1.0]\n",
    "  \"\"\"\n",
    "  random_t = torch.rand(x.shape[0], device=x.device) * (1. - eps) + eps\n",
    "  z = torch.randn_like(x)\n",
    "  std = sigma_t_fun(random_t, )\n",
    "  perturbed_x = x + z * std[:, None]\n",
    "  #################################################\n",
    "  ## TODO for students: Implement the denoising score matching eq.\n",
    "  raise NotImplementedError(\"Student exercise: say what they should have done\")\n",
    "  #################################################\n",
    "  # use the model to predict score at x_t and t\n",
    "  score = model(..., ...)\n",
    "  # implement the loss \\|\\sigma_t s_\\theta(x+\\sigma_t z, t) + z\\|^2\n",
    "  loss = ...\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials/W2D4_GenerativeModels/solutions/W2D4_Tutorial2_Solution_1927e938.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "A correctly implemented loss function shall pass the test below.\n",
    "\n",
    "For a dataset with a single `0` datapoint, we have the analytical score is $\\mathbf s(\\mathbf x,t)=-\\mathbf x/\\sigma_t^2$. We test that, for this case, the analytical have zero loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Test loss function\n",
    "sigma_t_test = lambda t: sigma_t(t, Lambda=10)\n",
    "score_analyt_test = lambda x_t, t: - x_t / sigma_t_test(t)[:, None]**2\n",
    "x_test = torch.zeros(10, 2)\n",
    "loss = loss_fn(score_analyt_test, x_test, sigma_t_test, eps=1e-3)\n",
    "print(f\"The loss is zero: {torch.allclose(loss, torch.zeros(1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implementing_Denoising_Score_Matching_Objective_Exercise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Define utils functions (Neural Network, and data sampling)\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\n",
    "  Basically it multiplexes a scalar `t` into a vector of `sin(2 pi k t)` and `cos(2 pi k t)` features.\n",
    "  \"\"\"\n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights during initialization. These weights are fixed\n",
    "    # during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "  def forward(self, t):\n",
    "    t_proj = t[:, None] * self.W[None, :] * 2 * np.pi\n",
    "    return torch.cat([torch.sin(t_proj), torch.cos(t_proj)], dim=-1)\n",
    "\n",
    "class ScoreModel_Time(nn.Module):\n",
    "  \"\"\"A time-dependent score-based model.\"\"\"\n",
    "\n",
    "  def __init__(self, sigma, ):\n",
    "    super().__init__()\n",
    "    self.embed = GaussianFourierProjection(10, scale=1)\n",
    "    self.net = nn.Sequential(nn.Linear(12, 50),\n",
    "               nn.Tanh(),\n",
    "               nn.Linear(50,50),\n",
    "               nn.Tanh(),\n",
    "               nn.Linear(50,2))\n",
    "    self.sigma_t_fun = lambda t: np.sqrt(sigma_t_square(t, sigma))\n",
    "\n",
    "  def forward(self, x, t):\n",
    "    t_embed = self.embed(t)\n",
    "    pred = self.net(torch.cat((x,t_embed),dim=1))\n",
    "    # this additional steps provides an inductive bias.\n",
    "    # the neural network output on the same scale,\n",
    "    pred = pred / self.sigma_t_fun(t)[:, None,]\n",
    "    return pred\n",
    "\n",
    "\n",
    "def sample_X_and_score_t_depend(gmm, trainN=10000, sigma=5, partition=20, EPS=0.02):\n",
    "  \"\"\"Uniformly partition [0,1] and sample t from it, and then\n",
    "  sample x~ p_t(x) and compute \\nabla \\log p_t(x)\n",
    "  finally return the dataset x, score, t (train and test)\n",
    "  \"\"\"\n",
    "  trainN_part = trainN // partition\n",
    "  X_train_col, y_train_col, T_train_col = [], [], []\n",
    "  for t in np.linspace(EPS, 1.0, partition):\n",
    "    gmm_dif = diffuse_gmm(gmm, t, sigma)\n",
    "    X_train,_,_ = gmm.sample(trainN_part)\n",
    "    y_train = gmm.score(X_train)\n",
    "    X_train_tsr = torch.tensor(X_train).float()\n",
    "    y_train_tsr = torch.tensor(y_train).float()\n",
    "    T_train_tsr = t * torch.ones(trainN_part)\n",
    "    X_train_col.append(X_train_tsr)\n",
    "    y_train_col.append(y_train_tsr)\n",
    "    T_train_col.append(T_train_tsr)\n",
    "  X_train_tsr = torch.cat(X_train_col, dim=0)\n",
    "  y_train_tsr = torch.cat(y_train_col, dim=0)\n",
    "  T_train_tsr = torch.cat(T_train_col, dim=0)\n",
    "  return X_train_tsr, y_train_tsr, T_train_tsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Test the Denoising Score Matching loss function\n",
    "def test_DSM_objective(gmm, epochs=500, seed=0):\n",
    "  set_seed(seed)\n",
    "  sigma = 25.0\n",
    "  print(\"sampled 10000 (X, t, score) for training\")\n",
    "  X_train_samp, y_train_samp, T_train_samp = \\\n",
    "    sample_X_and_score_t_depend(gmm, sigma=sigma, trainN=10000,\n",
    "                              partition=500, EPS=0.01)\n",
    "  print(\"sampled 2000 (X, t, score) for testing\")\n",
    "  X_test_samp, y_test_samp, T_test_samp = \\\n",
    "    sample_X_and_score_t_depend(gmm, sigma=sigma, trainN=2000,\n",
    "                              partition=500, EPS=0.01)\n",
    "  print(\"Define neural network score approximator\")\n",
    "  score_model_td = ScoreModel_Time(sigma=sigma)\n",
    "  sigma_t_f = lambda t: np.sqrt(sigma_t_square(t, sigma))\n",
    "  optim = Adam(score_model_td.parameters(), lr=0.005)\n",
    "  print(\"Minimize the denoising score matching objective\")\n",
    "  stats = []\n",
    "  pbar = trange(epochs)  # 5k samples for 500 iterations.\n",
    "  for ep in pbar:\n",
    "    loss = loss_fn(score_model_td, X_train_samp, sigma_t_f, eps=0.01)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    pbar.set_description(f\"step {ep} DSM objective loss {loss.item():.3f}\")\n",
    "    if ep % 25==0 or ep==epochs-1:\n",
    "      # test the score prediction against the analytical score of the gmm.\n",
    "      y_pred_train = score_model_td(X_train_samp, T_train_samp)\n",
    "      MSE_train = MSELoss()(y_train_samp, y_pred_train)\n",
    "\n",
    "      y_pred_test = score_model_td(X_test_samp, T_test_samp)\n",
    "      MSE_test = MSELoss()(y_test_samp, y_pred_test)\n",
    "      print(f\"step {ep} DSM loss {loss.item():.3f} train score MSE {MSE_train.item():.3f} \"+\\\n",
    "          f\"test score MSE {MSE_test.item():.3f}\")\n",
    "      stats.append((ep, loss.item(), MSE_train.item(), MSE_test.item()))\n",
    "  stats_df = pd.DataFrame(stats, columns=['ep', 'DSM_loss', 'MSE_train', 'MSE_test'])\n",
    "  return score_model_td, stats_df\n",
    "\n",
    "\n",
    "score_model_td, stats_df = test_DSM_objective(gmm, epochs=500, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot the Loss\n",
    "stats_df.plot(x=\"ep\", y=['DSM_loss', 'MSE_train', 'MSE_test'])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Test the Learned Score by Reverse Diffusion\n",
    "def reverse_diffusion_SDE_sampling(score_model_td, sampN=500, Lambda=5,\n",
    "                                   nsteps=200, ndim=2, exact=False):\n",
    "  \"\"\"\n",
    "  score_model_td: if `exact` is True, use a gmm of class GaussianMixture\n",
    "                  if `exact` is False. use a torch neural network that takes vectorized x and t as input.\n",
    "  \"\"\"\n",
    "  sigmaT2 = sigma_t_square(1, Lambda)\n",
    "  xT = np.sqrt(sigmaT2) * np.random.randn(sampN, ndim)\n",
    "  x_traj_rev = np.zeros((*xT.shape, nsteps, ))\n",
    "  x_traj_rev[:, :, 0] = xT\n",
    "  dt = 1 / nsteps\n",
    "  for i in range(1, nsteps):\n",
    "    t = 1 - i * dt\n",
    "    tvec = torch.ones((sampN)) * t\n",
    "    eps_z = np.random.randn(*xT.shape)\n",
    "    if exact:\n",
    "      gmm_t = diffuse_gmm(score_model_td, t, Lambda)\n",
    "      score_xt = gmm_t.score(x_traj_rev[:, :, i-1])\n",
    "    else:\n",
    "      with torch.no_grad():\n",
    "        score_xt = score_model_td(torch.tensor(x_traj_rev[:, :, i-1]).float(), tvec).numpy()\n",
    "    x_traj_rev[:, :, i] = x_traj_rev[:, :, i-1] + eps_z * (Lambda ** t) * np.sqrt(dt) + score_xt * dt * Lambda**(2*t)\n",
    "  return x_traj_rev\n",
    "\n",
    "\n",
    "print(\"Sample with reverse SDE using the trained score model\")\n",
    "x_traj_rev_appr_denois = reverse_diffusion_SDE_sampling(score_model_td,\n",
    "                                                        sampN=1000,\n",
    "                                                        Lambda=25,\n",
    "                                                        nsteps=200,\n",
    "                                                        ndim=2)\n",
    "print(\"Sample with reverse SDE using the exact score of Gaussian mixture\")\n",
    "x_traj_rev_exact = reverse_diffusion_SDE_sampling(gmm, sampN=1000,\n",
    "                                                  Lambda=25,\n",
    "                                                  nsteps=200,\n",
    "                                                  ndim=2,\n",
    "                                                  exact=True)\n",
    "print(\"Sample from original Gaussian mixture\")\n",
    "X_samp, _, _ = gmm.sample(1000)\n",
    "\n",
    "print(\"Compare the distributions\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[7, 7])\n",
    "handles = []\n",
    "kdeplot(x_traj_rev_appr_denois[:, :, -1],\n",
    "        label=\"Reverse diffusion (NN score learned DSM)\", handles=handles, color=\"blue\")\n",
    "kdeplot(x_traj_rev_exact[:, :, -1],\n",
    "        label=\"Reverse diffusion (Exact score)\", handles=handles, color=\"orange\")\n",
    "kdeplot(X_samp, label=\"Samples from original GMM\", handles=handles, color=\"gray\")\n",
    "plt.axis(\"image\")\n",
    "plt.legend(handles=handles)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "Bravo, we have come a long way today! We learned about:\n",
    "\n",
    "* The forward and reverse diffusion processes connect the data and noise distributions.\n",
    "* Sampling involves transforming noise into data through the reverse diffusion process.\n",
    "* Score function is the gradient to the data distribution, enabling the diffusion process's time reversal.\n",
    "* By learning to denoise, we can learn the score function of data by a function approximator, e.g., neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The math that empowers the diffusion models is the reversibility of this stochastic process. Here is the general result that, given a forward diffusion process,\n",
    "\n",
    "\\begin{equation}\n",
    "d\\mathbf{x} = \\mathbf{f}(\\mathbf{x}, t)dt + g(t)d \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "There exists a reverse time stochastic process (reverse SDE)\n",
    "\n",
    "\\begin{equation}\n",
    "d\\mathbf{x} =  \\bigg[\\mathbf{f}(\\mathbf{x}, t) - g^2(t) \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x}) \\bigg]dt + g(t) d \\mathbf{w}.\n",
    "\\end{equation}\n",
    "\n",
    "and a probability flow Ordinary Differential Equation (ODE)\n",
    "\n",
    "\\begin{equation}\n",
    "d\\mathbf{x} = \\bigg[\\mathbf{f}(\\mathbf{x}, t) - \\frac{1}{2}g^2(t) \\nabla_\\mathbf{x} \\log p_t(\\mathbf{x})\\bigg] dt.\n",
    "\\end{equation}\n",
    "\n",
    "such that solving the reverse SDE or the probability flow ODE amounts to the time reversal of the solution to the forward SDE.\n",
    "\n",
    "By this math, simulating both the ODE and the SDE can sample from diffusion models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**References**\n",
    "\n",
    "* [Brian Anderson, (1986) Reverse-time diffusion equation models](https://www.sciencedirect.com/science/article/pii/0304414982900515)\n",
    "* [Yang Song, et al. (2020) Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus: The Math behind Score Matching Objective\n",
    "\n",
    "How to fit the score based on the samples, when we have no access to the exact scores?\n",
    "\n",
    "This objective is called denoising score matching. Mathematically, it utilized this [equivalence relationship](https://animadversio.github.io/academic_notes/note-on-equiv-score-matching-objective) of the following objectives.\n",
    "\n",
    "\\begin{align}\n",
    "J_{DSM}(\\theta) &=\\mathbb E_{\\tilde x,x\\sim p_t(\\tilde x,x)} \\|s_\\theta(\\tilde x)-\\nabla_\\tilde x\\log p_t(\\tilde x\\mid x)\\|^2\\\\\n",
    "J_{ESM}(\\theta) &=\\mathbb E_{\\tilde x\\sim p_t(\\tilde x)} \\|s_\\theta(\\tilde x)-\\nabla_\\tilde x\\log p_t(\\tilde x)\\|^2\n",
    "\\end{align}\n",
    "\n",
    "In practise, it's to sample $x$ from data distribution, add noise with $\\sigma$ and denoise it. Since we have at time $t$, $p_t(\\tilde x\\mid x)= \\mathcal N(\\tilde x;x,\\sigma^2_t I)$, then $\\tilde x=x+\\sigma_t z,z\\sim \\mathcal N(0,I)$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\tilde x\\log p_t(\\tilde x|x)=-\\frac{1}{\\sigma_t^2}(x+\\sigma_t z -x)=-\\frac{1}{\\sigma_t}z\n",
    "\\end{equation}\n",
    "\n",
    "The objective simplifies into\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb E_{x\\sim p_0(x)}\\mathbb E_{z\\sim \\mathcal N(0,I)} \\|s_\\theta(x+\\sigma_t z)+\\frac{1}{\\sigma_t}z\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "Finally, in the time dependent score model $s(x,t)$, to learn this for any time $t\\in [\\epsilon,1]$, we integrate over all $t$ with a certain weighting function $\\gamma_t$ to emphasize certain part.\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_\\epsilon^1dt \\gamma_t\\mathbb E_{x\\sim p_0(x)}\\mathbb E_{z\\sim \\mathcal N(0,I)} \\|s_\\theta(x+\\sigma_t z, t)+\\frac{1}{\\sigma_t}z\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "(the $\\epsilon$ is set to ensure numerical stability, as $t\\to 0,\\sigma_t\\to 0$)\n",
    "Now all the expectations could be easily evaluated by sampling.\n",
    "\n",
    "A commonly used weighting is the following:\n",
    "\n",
    "\\begin{equation}\n",
    "\\int_\\epsilon^1dt \\mathbb E_{x\\sim p_0(x)}\\mathbb E_{z\\sim \\mathcal N(0,I)} \\|\\sigma_t s_\\theta(x+\\sigma_t z, t)+z\\|^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Reference**\n",
    "\n",
    "* [Pascal Vincent (2011), A connection between score matching and denoising autoencoders](https://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D4_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
