{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D4_GenerativeModels/instructor/W2D4_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D4_GenerativeModels/instructor/W2D4_Tutorial3.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 3: Conditional GANs and Implications of GAN Technology\n",
    "\n",
    "**Week 2, Day 4: Generative Models**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Seungwook Han, Kai Xu, Akash Srivastava\n",
    "\n",
    "__Content reviewers:__ Polina Turishcheva, Melvin Selim Atay, Hadi Vafaei, Deepak Raya, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Arush Tagade, Gagana B, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "## Tutorial Objectives\n",
    "\n",
    "The goal of this tutorial is to understand conditional GANs. Then you will have the opportunity to experience first-hand how effective GANs are at modeling the data distribution and to question what the consequences of this technology may be.\n",
    "\n",
    "By the end of this tutorial you will be able to:\n",
    "- Understand the differences in conditional GANs.\n",
    "- Generate high-dimensional natural images from a BigGAN.\n",
    "- Understand the efficacy of GANs in modeling the data distribution (e.g., faces).\n",
    "- Understand the energy inefficiency / environmental impact of training these large generative models.\n",
    "- Understand the implications of this technology (ethics, environment, *etc*.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/ps28k/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These are the slides for the videos in this tutorial. If you want to locally download the slides, click [here](https://osf.io/ps28k/download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown Install *Huggingface BigGAN* library\n",
    "!pip install pytorch-pretrained-biggan --quiet\n",
    "!pip install Pillow libsixel-python --quiet\n",
    "!pip install nltk --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pytorch_pretrained_biggan import BigGAN\n",
    "from pytorch_pretrained_biggan import one_hot_from_names\n",
    "from pytorch_pretrained_biggan import truncated_noise_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # Interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download `wordnet` dataset\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "import os, requests, zipfile\n",
    "\n",
    "os.environ['NLTK_DATA'] = 'nltk_data/'\n",
    "\n",
    "fnames = ['wordnet.zip', 'omw-1.4.zip']\n",
    "urls = ['https://osf.io/ekjxy/download', 'https://osf.io/kuwep/download']\n",
    "\n",
    "for fname, url in zip(fnames, urls):\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "  with open(fname, 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "\n",
    "  with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "    zip_ref.extractall('nltk_data/corpora')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Generating with a conditional GAN (BigGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Conditional Generative Models\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')  \n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'lV6zH2xDZck'), ('Bilibili', 'BV1f54y1E79D')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this section, we will load a pre-trained conditional GAN, BigGAN, which is the state-of-the-art model in conditional high-dimensional natural image generation, and generate samples from it. Since it is a class conditional model, we will be able to use the class label to generate images from the different classes of objects.\n",
    "\n",
    "Read here for more details on BigGAN: [Brock et al., 2019](https://arxiv.org/abs/1809.11096)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def load_biggan(model_res):\n",
    "  \"\"\"\n",
    "  Load respective BigGAN model for the specified resolution (biggan-deep-128, biggan-deep-256, biggan-deep-512)\n",
    "  \"\"\"\n",
    "  return BigGAN.from_pretrained('biggan-deep-{}'.format(model_res))\n",
    "\n",
    "\n",
    "def create_class_noise_vectors(class_str, trunc, num_samples):\n",
    "  \"\"\"\n",
    "  Create class and noise vectors for sampling from BigGAN\n",
    "\n",
    "  Args:\n",
    "    class_str: string\n",
    "      Class\n",
    "    trunc: float\n",
    "      Truncation factor\n",
    "    num_samples: int\n",
    "      Number of samples\n",
    "\n",
    "  Returns:\n",
    "    class_vector: np.ndarray\n",
    "      Class vector sampled from BigGan\n",
    "    noise_vector: np.ndarray\n",
    "      Noise vector\n",
    "  \"\"\"\n",
    "  class_vector = one_hot_from_names([class_str]*num_samples, batch_size=num_samples)\n",
    "  noise_vector = truncated_noise_sample(truncation=trunc, batch_size=num_samples)\n",
    "\n",
    "  return class_vector, noise_vector\n",
    "\n",
    "def generate_biggan_samples(model, class_vector, noise_vector, device,\n",
    "                            truncation=0.4):\n",
    "  \"\"\"\n",
    "  Generate samples from BigGAN\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      Model\n",
    "    device: string\n",
    "      GPU if available. CPU otherwise.\n",
    "    truncation: float\n",
    "      Truncation factor\n",
    "    class_vector: np.ndarray\n",
    "      Class vector sampled from BigGan\n",
    "    noise_vector: np.ndarray\n",
    "      Noise vector\n",
    "\n",
    "  Returns:\n",
    "    output_grid: torch.tensor\n",
    "      Make grid and display generated samples\n",
    "  \"\"\"\n",
    "  # Convert to tensor\n",
    "  noise_vector = torch.from_numpy(noise_vector)\n",
    "  class_vector = torch.from_numpy(class_vector)\n",
    "\n",
    "  # Move to GPU\n",
    "  noise_vector = noise_vector.to(device)\n",
    "  class_vector = class_vector.to(device)\n",
    "  model.to(device)\n",
    "\n",
    "  # Generate an image\n",
    "  with torch.no_grad():\n",
    "      output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "  # Back to CPU\n",
    "  output = output.to('cpu')\n",
    "\n",
    "  # The output layer of BigGAN has a tanh layer, resulting the range of [-1, 1] for the output image\n",
    "  # Therefore, we normalize the images properly to [0, 1] range.\n",
    "  # Clipping is only in case of numerical instability problems\n",
    "\n",
    "  output = torch.clip(((output.detach().clone() + 1) / 2.0), 0, 1)\n",
    "  output = output\n",
    "\n",
    "  # Make grid and show generated samples\n",
    "  output_grid = torchvision.utils.make_grid(output,\n",
    "                                            nrow=min(4, output.shape[0]),\n",
    "                                            padding=5)\n",
    "  plt.imshow(output_grid.permute(1, 2, 0))\n",
    "\n",
    "  return output_grid\n",
    "\n",
    "\n",
    "def generate(b):\n",
    "  \"\"\"\n",
    "  Generation function\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  # Create BigGAN model\n",
    "  model = load_biggan(MODEL_RESOLUTION)\n",
    "\n",
    "  # Use specified parameters (resolution, class, number of samples, etc) to generate from BigGAN\n",
    "  class_vector, noise_vector = create_class_noise_vectors(CLASS, TRUNCATION,\n",
    "                                                          NUM_SAMPLES)\n",
    "  samples_grid = generate_biggan_samples(model, class_vector, noise_vector,\n",
    "                                         DEVICE, TRUNCATION)\n",
    "  torchvision.utils.save_image(samples_grid, 'samples.png')\n",
    "  ### If CUDA out of memory issue, lower NUM_SAMPLES (number of samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.1:  Define configurations\n",
    "\n",
    "We will now define the configurations (resolution of model, number of samples, class to sample from, truncation level) under which we will sample from BigGAN. \n",
    "\n",
    "***Question***: What is the truncation trick employed by BigGAN? How does sample variety and fidelity change by varying the truncation level? (Hint: play with the truncation slider and try sampling at different levels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title { run: \"auto\" }\n",
    "\n",
    "### RUN THIS BLOCK EVERY TIME YOU CHANGE THE PARAMETERS FOR GENERATION\n",
    "\n",
    "# Resolution at which to generate\n",
    "MODEL_RESOLUTION = \"128\" # @param [128, 256, 512]\n",
    "\n",
    "# Number of images to generate\n",
    "NUM_SAMPLES = 4 # @param {type:\"slider\", min:4, max:12, step:4}\n",
    "\n",
    "# Class of images to generate\n",
    "CLASS = 'German shepherd'  # @param ['tench', 'magpie', 'jellyfish', 'German shepherd', 'bee', 'acoustic guitar', 'coffee mug', 'minibus', 'monitor']\n",
    "\n",
    "# Truncation level of the normal distribution we sample z from\n",
    "TRUNCATION = 0.4  # @param {type:\"slider\", min:0.1, max:1, step:0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Generate\n",
    "# Create generate button, given parameters specified above\n",
    "button = widgets.Button(description=\"GENERATE!\",\n",
    "                        layout=widgets.Layout(width='30%', height='80px'),\n",
    "                        button_style='danger')\n",
    "output = widgets.Output()\n",
    "display(button, output)\n",
    "button.on_click(generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Think! 1: BigGANs\n",
    "\n",
    "1. How does BigGAN differ from previous state-of-the-art generative models for high-dimensional natural images? In other words, how does BigGAN solve high-dimensional image generation? (Hint: look into model architecture and training configurations) (BigGAN paper: [Brock et al., 2018](https://arxiv.org/abs/1809.11096))\n",
    "2. Continuing from Question 1, what are the drawbacks of introducing such techniques into training large models for high-dimensional, diverse datasets?\n",
    "3. Play with other pre-trained generative models like StyleGAN here -- where code for sampling and interpolation in the latent space is available [here](https://github.com/NVlabs/stylegan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "1\n",
    "  - Very large mini-batch sizes (>= 2048) -- along with synchronous batch\n",
    "    normalization across all GPUs / mini-batches to address the high diversity\n",
    "    of the ImageNet dataset\n",
    "  - Increased number of parameters (at least double of previous state-of-the-art\n",
    "    models like SAGAN) (from increased widths of the layers)\n",
    "  - Spectral normalization, orthogonal regularization penalty, and dropouts\n",
    "    in final layer of D to stabilize the highly unstable training of BigGAN.\n",
    "    Even with these stabilization techniques, BigGAN still suffers from chronic\n",
    "    instability (as described in the paper) and requires multiple restarts.\n",
    "    The model almost always collapses at some point in the training, but the key\n",
    "     is to stop the training before it does.\n",
    "\n",
    "2. Huge compute requirement to allow for large mini-batch sizes and therefore\n",
    "   high energy consumption\n",
    "\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Ethical issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Ethical Issues\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')  \n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'ZtWFeUZgfVk'), ('Bilibili', 'BV11L411H7pr')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Faces Quiz \n",
    "\n",
    "Now is your turn to test your abilities on recognizing a real vs. a fake image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Real or Fake?\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://docs.google.com/forms/d/e/1FAIpQLSeGjn2S2bn6Q1qWjVgDS5LG7G1GsQQh2Q0T9dEUO1z5_W0yYg/viewform?usp=sf_link', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Energy Efficiency Quiz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Make a guess\n",
    "IFrame(src='https://docs.google.com/forms/d/e/1FAIpQLSe8suNt4ZmadSr_6IWq6s_nUYxC1VCpjR2cBBmQ7cR_5znCZw/viewform?usp=sf_link', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Hooray! You have finished the second week of NMA-DL!!!\n",
    "\n",
    "In the first section of this tutorial, we have learned: \n",
    "- How conditional GANs differ from unconditional models\n",
    "- How to use a pre-trained BigGAN model to generate high-dimensional photo-realistic images and its tricks to modulate diversity and image fidelity\n",
    "\n",
    "In the second section, we learned about the broader ethical implications of GAN technology on society through deepfakes and their tremendous energy inefficiency.\n",
    "\n",
    "On the brighter side, as we learned throughout the week, GANs are very effective in modeling the data distribution and have many practical applications.\n",
    "\n",
    "For example, as personalized healthcare and applications of AI in healthcare rise, the need to remove any Personally Identifiable Information (PII) becomes more important. As shown in [Piacentino and Angulo, 2020](https://doi.org/10.1007/978-3-030-45385-5_36), GANs can be leveraged to anonymize healthcare data.\n",
    "\n",
    "As a food for thought, what are some other practical applications of GANs that you can think of? Discuss with your pod your ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title (Bonus) Video 3: Recap and advanced topics\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')  \n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', '7nUjFG3N04I'), ('Bilibili', 'BV1Uo4y1D7Nj')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D4_Tutorial3",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
