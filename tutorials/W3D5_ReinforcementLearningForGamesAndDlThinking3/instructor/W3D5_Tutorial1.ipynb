{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/instructor/W3D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/instructor/W3D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Reinforcement Learning For Games\n",
    "\n",
    "**Week 3, Day 5: Learning to Play Games & DL Thinking 3**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Mandana Samiei, Raymond Chua, Kushaan Gupta, Tim Lilicrap, Blake Richards\n",
    "\n",
    "__Content reviewers:__ Arush Tagade, Lily Cheng, Melvin Selim Atay, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Melvin Selim Atay, Spiros Chavlis, Gunnar Blohm\n",
    "\n",
    "__Production editors:__ Namrata Bafna, Gagana B, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "In this tutorial, you will learn how to implement a game loop, create a random player, and improve the performance of player using different reinforcement learning methods.\n",
    "\n",
    "The specific objectives for this tutorial:\n",
    "*   Understand the format of two-players games, Othello specifically\n",
    "*   Understand how to create random players\n",
    "*   Understand how to implement a value-based player\n",
    "*   Understand how to implement a policy-based player\n",
    "*   Understand how to implement a player with Monte Carlo planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "from IPython.display import IFrame\n",
    "link_id = \"3zn9w\"\n",
    "print(f\"If you want to download the slides: https://osf.io/download/{link_id}/\")\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/{link_id}/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install coloredlogs --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip3 install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_dl\",\n",
    "            \"user_key\": \"f379rz8y\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W3D5_T1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import coloredlogs\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pickle import Unpickler\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "coloredlogs.install(level='INFO')  # Change this to DEBUG to see more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "def loadTrainExamples(folder, filename):\n",
    "  \"\"\"\n",
    "  Helper function to load training examples\n",
    "\n",
    "  Args:\n",
    "    folder: string\n",
    "      Path specifying training examples\n",
    "    filename: string\n",
    "      File name of training examples\n",
    "\n",
    "  Returns:\n",
    "    trainExamplesHistory: list\n",
    "      Returns examples based on the model were already collected (loaded)\n",
    "  \"\"\"\n",
    "  trainExamplesHistory = []\n",
    "  modelFile = os.path.join(folder, filename)\n",
    "  examplesFile = modelFile + \".examples\"\n",
    "  if not os.path.isfile(examplesFile):\n",
    "    print(f'File \"{examplesFile}\" with trainExamples not found!')\n",
    "    r = input(\"Continue? [y|n]\")\n",
    "    if r != \"y\":\n",
    "      sys.exit()\n",
    "  else:\n",
    "    print(\"File with train examples found. Loading it...\")\n",
    "    with open(examplesFile, \"rb\") as f:\n",
    "      trainExamplesHistory = Unpickler(f).load()\n",
    "    print('Loading done!')\n",
    "    return trainExamplesHistory\n",
    "\n",
    "\n",
    "def save_model_checkpoint(folder, filename, nnet):\n",
    "  filepath = os.path.join(folder, filename)\n",
    "\n",
    "  if not os.path.exists(folder):\n",
    "    print(f\"Checkpoint Directory does not exist! Making directory {folder}\")\n",
    "    os.mkdir(folder)\n",
    "  else:\n",
    "    print(\"Checkpoint Directory exists!\")\n",
    "\n",
    "  torch.save({'state_dict': nnet.state_dict()}, filepath)\n",
    "  print(\"Model saved!\")\n",
    "\n",
    "\n",
    "def load_model_checkpoint(folder, filename, nnet, device):\n",
    "  filepath = os.path.join(folder, filename)\n",
    "\n",
    "  if not os.path.exists(filepath):\n",
    "    raise FileNotFoundError(f\"No model in path {filepath}\")\n",
    "\n",
    "  checkpoint = torch.load(filepath, map_location=device)\n",
    "  nnet.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook. \\n\"\n",
    "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
    "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download the modules\n",
    "\n",
    "# @markdown Run this cell!\n",
    "\n",
    "# @markdown Download from OSF. The original repo is https://github.com/raymondchua/nma_rl_games.git\n",
    "\n",
    "import os, io, sys, shutil, zipfile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# download from github repo directly\n",
    "#!git clone git://github.com/raymondchua/nma_rl_games.git --quiet\n",
    "REPO_PATH = 'nma_rl_games'\n",
    "\n",
    "if not os.path.exists(REPO_PATH):\n",
    "  download_string = \"Downloading\"\n",
    "  zipurl = 'https://osf.io/kf4p9/download'\n",
    "  print(f\"{download_string} and unzipping the file... Please wait.\")\n",
    "  with urlopen(zipurl) as zipresp:\n",
    "    with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n",
    "      zfile.extractall()\n",
    "  print(\"Download completed.\")\n",
    "\n",
    "# add the repo in the path\n",
    "sys.path.append('nma_rl_games/alpha-zero')\n",
    "print(f\"Added the {REPO_PATH} in the path and imported the modules.\")\n",
    "\n",
    "# @markdown Import modules designed for use in this notebook\n",
    "import Arena\n",
    "\n",
    "from utils import *\n",
    "from Game import Game\n",
    "from MCTS import MCTS\n",
    "from NeuralNet import NeuralNet\n",
    "\n",
    "# from othello.OthelloPlayers import *\n",
    "from othello.OthelloLogic import Board\n",
    "# from othello.OthelloGame import OthelloGame\n",
    "from othello.pytorch.NNet import NNetWrapper as NNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The hyperparameters used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "    'numIters': 1,            # In training, number of iterations = 1000 and num of episodes = 100\n",
    "    'numEps': 1,              # Number of complete self-play games to simulate during a new iteration.\n",
    "    'tempThreshold': 15,      # To control exploration and exploitation\n",
    "    'updateThreshold': 0.6,   # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n",
    "    'maxlenOfQueue': 200,     # Number of game examples to train the neural networks.\n",
    "    'numMCTSSims': 15,        # Number of games moves for MCTS to simulate.\n",
    "    'arenaCompare': 10,       # Number of games to play during arena play to determine if new net will be accepted.\n",
    "    'cpuct': 1,\n",
    "    'maxDepth': 5,             # Maximum number of rollouts\n",
    "    'numMCsims': 5,           # Number of monte carlo simulations\n",
    "    'mc_topk': 3,             # Top k actions for monte carlo rollout\n",
    "\n",
    "    'checkpoint': './temp/',\n",
    "    'load_model': False,\n",
    "    'load_folder_file': ('/dev/models/8x100x50','best.pth.tar'),\n",
    "    'numItersForTrainExamplesHistory': 20,\n",
    "\n",
    "    # Define neural network arguments\n",
    "    'lr': 0.001,               # lr: Learning Rate\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'device': DEVICE,\n",
    "    'num_channels': 512,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 0: Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 0: Introduction\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', '5kQ-xGbjlJo'), ('Bilibili', 'BV1kq4y1H7MQ')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Introduction_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Create a game/agent loop for RL\n",
    "\n",
    "*Time estimate: ~15 mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: A game loop for RL\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'aH2Hs8f6KrQ'), ('Bilibili', 'BV1iw411979L')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_A_game_loop_for_RL_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.1: Introduction to OthelloGame\n",
    "\n",
    "Othello is a board game played by two players on a board of 64 squares arranged in an eight-by-eight grid, with 64 playing pieces that are black on one side and white on the other.\n",
    "\n",
    "**Setup**:\n",
    "The board will start with 2 black discs and 2 white discs at the centre of the board. They are arranged with black forming a North-East to South-West direction. White is forming a North-West to South-East direction. Each player gets 32 discs and black always starts the game.\n",
    "\n",
    "**Game rules**:\n",
    "* Players take turns placing a single disk at a time.\n",
    "* A move is made by placing a disc of the player's color on the board to surround (i.e. \"outflank\") discs of the opposite color. In other words, the player with black discs must place on so that there is a straight line between the newly placed disc and another black disc, with one or more white pieces between them.\n",
    "* Surrounded disks get flipped (i.e. change color).\n",
    "* If a player does not have a valid move (they cannot place their disc to outflank the oppponent's discs), they pass on their turn\n",
    "* A player can not voluntarily forfeit his turn.\n",
    "* When both players can not make a valid move the game ends.\n",
    "\n",
    "If you're interested, you can explore this website, https://www.eothello.com/, where you will find a collection of useful rules and diagrams. Additionally, you can even play a sample Othello game on the site if you wish!\n",
    "\n",
    "**Note**: we will use a 6x6 board to speed computations up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "\n",
    "***Goal***: Setup a game environment with multiple players for reinforcement learning experiments.\n",
    "\n",
    "***Exercise***:\n",
    "\n",
    "*   Build an agent that plays random moves\n",
    "*   Make agents play games and compute wins and losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Execute the following code to enable the `OthelloGame` class. This class represents a game board and has methods such `getInitBoard` to create the intial board, `getValidMove` to return the options of valid moves, and other helpful functionality to play the game. You do not need to understand every line of code in this tutorial, but try to develop an intuitive understanding of the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class OthelloGame(Game):\n",
    "  \"\"\"\n",
    "  Othello game board\n",
    "  \"\"\"\n",
    "  square_content = {\n",
    "      -1: \"X\",\n",
    "      +0: \"-\",\n",
    "      +1: \"O\"\n",
    "      }\n",
    "\n",
    "  @staticmethod\n",
    "  def getSquarePiece(piece):\n",
    "    return OthelloGame.square_content[piece]\n",
    "\n",
    "  def __init__(self, n):\n",
    "    self.n = n\n",
    "\n",
    "  def getInitBoard(self):\n",
    "    b = Board(self.n)\n",
    "    return np.array(b.pieces)\n",
    "\n",
    "  def getBoardSize(self):\n",
    "    return (self.n, self.n)\n",
    "\n",
    "  def getActionSize(self):\n",
    "    # Return number of actions, n is the board size and +1 is for no-op action\n",
    "    return self.n * self.n + 1\n",
    "\n",
    "  def getCanonicalForm(self, board, player):\n",
    "    # Return state if player==1, else return -state if player==-1\n",
    "    return player * board\n",
    "\n",
    "  def stringRepresentation(self, board):\n",
    "    return board.tobytes()\n",
    "\n",
    "  def stringRepresentationReadable(self, board):\n",
    "    board_s = \"\".join(self.square_content[square] for row in board for square in row)\n",
    "    return board_s\n",
    "\n",
    "  def getScore(self, board, player):\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    return b.countDiff(player)\n",
    "\n",
    "  @staticmethod\n",
    "  def display(board):\n",
    "    n = board.shape[0]\n",
    "    print(\"   \", end=\"\")\n",
    "    for y in range(n):\n",
    "      print(y, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for y in range(n):\n",
    "      print(y, \"|\", end=\"\")    # Print the row\n",
    "      for x in range(n):\n",
    "        piece = board[y][x]    # Get the piece to print\n",
    "        print(OthelloGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "  @staticmethod\n",
    "  def displayValidMoves(moves):\n",
    "    A=np.reshape(moves[0:-1], board.shape)\n",
    "    n = board.shape[0]\n",
    "    print(\"  \")\n",
    "    print(\"possible moves\")\n",
    "    print(\"   \", end=\"\")\n",
    "    for y in range(n):\n",
    "      print(y, end=\" \")\n",
    "    print(\"\")\n",
    "    print(\"-----------------------\")\n",
    "    for y in range(n):\n",
    "      print(y, \"|\", end=\"\")    # Print the row\n",
    "      for x in range(n):\n",
    "        piece = A[y][x]    # Get the piece to print\n",
    "        print(OthelloGame.square_content[piece], end=\" \")\n",
    "      print(\"|\")\n",
    "    print(\"-----------------------\")\n",
    "\n",
    "  def getNextState(self, board, player, action):\n",
    "    \"\"\"\n",
    "    Make valid move. If player takes action on board, return next (board,player)\n",
    "    and action must be a valid move\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      player: Integer\n",
    "        ID of current player\n",
    "      action: np.ndarray\n",
    "        Space of actions\n",
    "\n",
    "    Returns:\n",
    "      (board, player): tuple\n",
    "        Next state representation\n",
    "    \"\"\"\n",
    "    if action == self.n*self.n:\n",
    "      return (board, -player)\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    move = (int(action/self.n), action%self.n)\n",
    "    b.execute_move(move, player)\n",
    "    return (b.pieces, -player)\n",
    "\n",
    "  def getValidMoves(self, board, player):\n",
    "    \"\"\"\n",
    "    Get all valid moves for player\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      player: Integer\n",
    "        ID of current player\n",
    "      action: np.ndarray\n",
    "        Space of action\n",
    "\n",
    "    Returns:\n",
    "      valids: np.ndarray\n",
    "        Valid moves for player\n",
    "    \"\"\"\n",
    "    valids = [0]*self.getActionSize()\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    legalMoves =  b.get_legal_moves(player)\n",
    "    if len(legalMoves)==0:\n",
    "      valids[-1]=1\n",
    "      return np.array(valids)\n",
    "    for x, y in legalMoves:\n",
    "      valids[self.n*x+y]=1\n",
    "    return np.array(valids)\n",
    "\n",
    "  def getGameEnded(self, board, player):\n",
    "    \"\"\"\n",
    "    Check if game ended\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      player: Integer\n",
    "        ID of current player\n",
    "\n",
    "    Returns:\n",
    "      0 if not ended, 1 if player 1 won, -1 if player 1 lost\n",
    "    \"\"\"\n",
    "    b = Board(self.n)\n",
    "    b.pieces = np.copy(board)\n",
    "    if b.has_legal_moves(player):\n",
    "      return 0\n",
    "    if b.has_legal_moves(-player):\n",
    "      return 0\n",
    "    if b.countDiff(player) > 0:\n",
    "      return 1\n",
    "    return -1\n",
    "\n",
    "  def getSymmetries(self, board, pi):\n",
    "    \"\"\"\n",
    "    Get mirror/rotational configurations of board\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "      pi: np.ndarray\n",
    "        Dimension of board\n",
    "\n",
    "    Returns:\n",
    "      l: list\n",
    "        90 degree of board, 90 degree of pi_board\n",
    "    \"\"\"\n",
    "    assert(len(pi) == self.n**2+1)  # 1 for pass\n",
    "    pi_board = np.reshape(pi[:-1], (self.n, self.n))\n",
    "    l = []\n",
    "\n",
    "    for i in range(1, 5):\n",
    "      for j in [True, False]:\n",
    "        newB = np.rot90(board, i)\n",
    "        newPi = np.rot90(pi_board, i)\n",
    "        if j:\n",
    "          newB = np.fliplr(newB)\n",
    "          newPi = np.fliplr(newPi)\n",
    "        l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Below, we initialize and view a board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Set up the game\n",
    "game = OthelloGame(6)\n",
    "\n",
    "# Get the initial board\n",
    "board = game.getInitBoard()\n",
    "\n",
    "# Display the board\n",
    "game.display(board)\n",
    "\n",
    "# Observe the game board size\n",
    "print(f'Board size = {game.getBoardSize()}')\n",
    "\n",
    "# Observe the action size\n",
    "print(f'Action size = {game.getActionSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now let's look at the valid actions for player 1 (the circles) and compare the valid actions to the board above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Get valid moves\n",
    "valids = game.getValidMoves(board, 1)\n",
    "print(valids)\n",
    "\n",
    "# Visualize the moves\n",
    "game.displayValidMoves(valids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "`game.getValidMoves` returns 1s and 0s for every position on the board, 1 indicates if it is a valid place to put a new disc. Note that it turns a list (this could be reshaped into the board shape).\n",
    "\n",
    "We also have a method to visualize the valid actions on the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.2: Create a random player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's start by setting up the game loop using a random player to start with so that we we can test the game loop and make sure it works correctly.\n",
    "\n",
    "To do so, we will first implement a random player in 3 steps:\n",
    "1. determine which moves are possible at all\n",
    "2. assign a uniform probability to each more (remember, this is a random player): 1/N for N valid moves\n",
    "3. randomly choose a move from the possible moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 1.2: Implement a random player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class RandomPlayer():\n",
    "\n",
    "  def __init__(self, game):\n",
    "    self.game = game\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      a: int\n",
    "        Randomly chosen move\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## TODO for students: ##\n",
    "    ## 1. Please compute the valid moves using getValidMoves() and the game class self.game. ##\n",
    "    ## 2. Compute the probability over actions.##\n",
    "    ## 3. Pick a random action based on the probability computed above.##\n",
    "    # Fill out function and remove ##\n",
    "    raise NotImplementedError(\"Implement the random player\")\n",
    "    #################################################\n",
    "\n",
    "    # Compute the valid moves using getValidMoves()\n",
    "    valids = ...\n",
    "\n",
    "    # Compute the probability of each move being played (random player means\n",
    "    # this should be uniform for valid moves, 0 for others)\n",
    "    prob = ...\n",
    "\n",
    "    # Pick a random action based on the probabilities (hint: np.choice is useful)\n",
    "    a = ...\n",
    "\n",
    "    return a\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class RandomPlayer():\n",
    "\n",
    "  def __init__(self, game):\n",
    "    self.game = game\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      a: int\n",
    "        Randomly chosen move\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the valid moves using getValidMoves()\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "\n",
    "    # Compute the probability of each move being played (random player means\n",
    "    # this should be uniform for valid moves, 0 for others)\n",
    "    prob = valids/valids.sum()\n",
    "\n",
    "    # Pick a random action based on the probabilities (hint: np.choice is useful)\n",
    "    a = np.random.choice(self.game.getActionSize(), p=prob)\n",
    "\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implement_a_Random_Player_Excercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.3: Compete random agents\n",
    "\n",
    "Now we create 2 random players and let them play against one another for a number of times... We will use some nice functionality we imported above, including the `Arena` class that allows multiple game plays. You can check out the code here if you want, but it is not necessary: https://github.com/raymondchua/nma_rl_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Define the random players\n",
    "player1 = RandomPlayer(game).play  # note the .play here to pass a function to Arena!\n",
    "player2 = RandomPlayer(game).play\n",
    "\n",
    "# Define number of games\n",
    "num_games = 20\n",
    "\n",
    "# Start the competition\n",
    "set_seed(seed=SEED)\n",
    "arena = Arena.Arena(player1, player2, game, display=None)  # To see the steps of the competition set \"display=OthelloGame.display\"\n",
    "result = arena.playGames(num_games, verbose=False)  # returns (Number of player1 wins, number of player2 wins, number of ties)\n",
    "\n",
    "# Compute win rate for the random player (player 1)\n",
    "print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over 20 games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 13, Number of games won by player2 = 7 out of 20 games\n",
    "\n",
    "Win rate for player1 over 20 games: 65.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note**: the random player is purely policy-based. It contains no estimates of value. Next we'll see how to estimate and use value functions for game playing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Train a value function from expert game data\n",
    "\n",
    "*Time estimate: ~35 mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have the game set up and working, we can build a (hopefully) smarter\n",
    "player by learning a value function using expert game data. Our player can then\n",
    "use this value function to decide what moves to make.\n",
    "\n",
    "**Goal:** Learn how to train a value function from a dataset of games played by\n",
    "an expert.\n",
    "\n",
    "**Exercise:**\n",
    "\n",
    "* Load a dataset of expert generated games.\n",
    "* Train a network to minimize MSE for win/loss predictions given board states\n",
    "sampled throughout the game. This will be done on a very small number of games.\n",
    "We will provide a network trained on a larger dataset.\n",
    "* Learn how to use a value function in order to make a player that works better\n",
    "than a random player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Train a value function\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'f9lZq0WQJFg'), ('Bilibili', 'BV1jf4y157xQ')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Train_a_Value_Function_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Load expert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "path = \"nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Define the Neural Network Architecture for Othello\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We will (somewhat arbitrarily) use a deep CNN with 4 convolutional layers and 4 linear layers with ReLU transfer functions and batch normalization.  One reason why convolutions are interesting here is because they can extract the local value of moves on the board regardless of board position; convolution would thus be able to extract the translation-invariant aspects of the play.\n",
    "\n",
    "For the Value Network network, the 3rd linear layer represents the policy and the 4th linear layer (output) represents the value function. The value function is a weighted sum over all policies.\n",
    "\n",
    "We can do this by assuming that the weights between linear layers 3 and 4 approximate the value-action function $w_{l_{34}}=Q^{\\pi}(s,a)$ in:\n",
    "\n",
    "\\begin{equation}\n",
    "V^{\\pi}(s) = \\sum_{a}{\\pi(a,s) \\cdot Q^{\\pi}(s,a)}\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: `OthelloNet` has 2 outputs:\n",
    "1. log-softmax of linear layer 3\n",
    "2. tanh of linear layer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/static/CNN.jpg\">\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2: Implement `OthelloNNet` for playing Othello\n",
    "\n",
    "We implement most of OthelloNNet below but please complete the code to get the final outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class OthelloNNet(nn.Module):\n",
    "\n",
    "  def __init__(self, game, args):\n",
    "    \"\"\"\n",
    "    Initialise game parameters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.args = args\n",
    "\n",
    "    super(OthelloNNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=args.num_channels,\n",
    "                           kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(in_channels=args.num_channels,\n",
    "                           out_channels=args.num_channels, kernel_size=3,\n",
    "                           stride=1, padding=1)\n",
    "    self.conv3 = nn.Conv2d(in_channels=args.num_channels,\n",
    "                           out_channels=args.num_channels, kernel_size=3,\n",
    "                           stride=1)\n",
    "    self.conv4 = nn.Conv2d(in_channels=args.num_channels,\n",
    "                           out_channels=args.num_channels, kernel_size=3,\n",
    "                           stride=1)\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "    self.bn3 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "    self.bn4 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=args.num_channels * (self.board_x - 4) * (self.board_y - 4),\n",
    "                         out_features=1024)\n",
    "    self.fc_bn1 = nn.BatchNorm1d(num_features=1024)\n",
    "\n",
    "    self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "    self.fc_bn2 = nn.BatchNorm1d(num_features=512)\n",
    "\n",
    "    self.fc3 = nn.Linear(in_features=512, out_features=self.action_size)\n",
    "\n",
    "    self.fc4 = nn.Linear(in_features=512, out_features=1)\n",
    "\n",
    "\n",
    "  def forward(self, s):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      s: np.ndarray\n",
    "        Array of size (batch_size x board_x x board_y)\n",
    "\n",
    "    Returns:\n",
    "      prob, v: tuple of torch.Tensor\n",
    "        Probability distribution over actions at the current state and the value\n",
    "        of the current state.\n",
    "    \"\"\"\n",
    "    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n",
    "\n",
    "    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
    "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "    pi = self.fc3(s)  # batch_size x action_size\n",
    "    v = self.fc4(s)   # batch_size x 1\n",
    "    #################################################\n",
    "    ## TODO for students: Compute the outputs of OthelloNNet in this order\n",
    "    # 1. Log softmax of linear layer 3\n",
    "    # 2. tanh of linear layer 4\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Calculate the probability distribution and the value\")\n",
    "    #################################################\n",
    "    # Returns probability distribution over actions at the current state and the value of the current state.\n",
    "    return ..., ...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class OthelloNNet(nn.Module):\n",
    "\n",
    "  def __init__(self, game, args):\n",
    "    \"\"\"\n",
    "    Initialise game parameters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.args = args\n",
    "\n",
    "    super(OthelloNNet, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=args.num_channels,\n",
    "                           kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv2d(in_channels=args.num_channels,\n",
    "                           out_channels=args.num_channels, kernel_size=3,\n",
    "                           stride=1, padding=1)\n",
    "    self.conv3 = nn.Conv2d(in_channels=args.num_channels,\n",
    "                           out_channels=args.num_channels, kernel_size=3,\n",
    "                           stride=1)\n",
    "    self.conv4 = nn.Conv2d(in_channels=args.num_channels,\n",
    "                           out_channels=args.num_channels, kernel_size=3,\n",
    "                           stride=1)\n",
    "\n",
    "    self.bn1 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "    self.bn2 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "    self.bn3 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "    self.bn4 = nn.BatchNorm2d(num_features=args.num_channels)\n",
    "\n",
    "    self.fc1 = nn.Linear(in_features=args.num_channels * (self.board_x - 4) * (self.board_y - 4),\n",
    "                         out_features=1024)\n",
    "    self.fc_bn1 = nn.BatchNorm1d(num_features=1024)\n",
    "\n",
    "    self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
    "    self.fc_bn2 = nn.BatchNorm1d(num_features=512)\n",
    "\n",
    "    self.fc3 = nn.Linear(in_features=512, out_features=self.action_size)\n",
    "\n",
    "    self.fc4 = nn.Linear(in_features=512, out_features=1)\n",
    "\n",
    "  def forward(self, s):\n",
    "    \"\"\"\n",
    "    Controls forward pass of OthelloNNet\n",
    "\n",
    "    Args:\n",
    "      s: np.ndarray\n",
    "        Array of size (batch_size x board_x x board_y)\n",
    "\n",
    "    Returns:\n",
    "      prob, v: tuple of torch.Tensor\n",
    "        Probability distribution over actions at the current state and the value\n",
    "        of the current state.\n",
    "    \"\"\"\n",
    "    s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n",
    "    s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n",
    "    s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n",
    "    s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n",
    "    s = s.view(-1, self.args.num_channels * (self.board_x - 4) * (self.board_y - 4))\n",
    "\n",
    "    s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n",
    "    s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n",
    "\n",
    "    pi = self.fc3(s)  # batch_size x action_size\n",
    "    v = self.fc4(s)   # batch_size x 1\n",
    "    # Returns probability distribution over actions at the current state and the value of the current state.\n",
    "    return F.log_softmax(pi, dim=1), torch.tanh(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implement_OtheloNN_Excercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: Define the Value network\n",
    "\n",
    "Next we need to implement the training of the network we created above. We want to train it to approximate the value function - we will use real examples (the expert data from above) to train it. So we need to specify the standard initialization, training, prediction and loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.3: Implement the `ValueNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class ValueNetwork(NeuralNet):\n",
    "\n",
    "  def __init__(self, game):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame\n",
    "        Instance of the OthelloGame class above\n",
    "    \"\"\"\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.nnet.to(args.device)\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples with each example is of form (board, pi, v)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        v_losses = []   # To store the losses per epoch\n",
    "        batch_count = int(len(examples) / args.batch_size)  # len(examples)=200, batch-size=64, batch_count=3\n",
    "        t = tqdm(range(batch_count), desc='Training Value Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # Read the ground truth information from MCTS simulation using the loaded examples\n",
    "          boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # Length of boards, pis, vis = 64\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "          # Predict\n",
    "          # To run on GPU if available\n",
    "          boards, target_vs = boards.contiguous().to(args.device), target_vs.contiguous().to(args.device)\n",
    "\n",
    "          #################################################\n",
    "          ## TODO for students:\n",
    "          # 1. Compute the value predicted by OthelloNNet()\n",
    "          # 2. First implement the loss_v() function below and then use it to update the value loss. ##\n",
    "          # Fill out function and remove\n",
    "          raise NotImplementedError(\"Compute the output\")\n",
    "          #################################################\n",
    "          # Compute output\n",
    "          _, out_v = ...\n",
    "          l_v = ...  # Total loss\n",
    "\n",
    "          # Record loss\n",
    "          v_losses.append(l_v.item())\n",
    "          t.set_postfix(Loss_v=l_v.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          l_v.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      v: OthelloNet instance\n",
    "        Data of the OthelloNet class instance above;\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "        _, v = self.nnet(board)\n",
    "    return v.data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_v(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates mean squared error\n",
    "\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth variables corresponding to input\n",
    "      outputs: np.ndarray\n",
    "        Predictions of Network\n",
    "\n",
    "    Returns:\n",
    "      MSE loss averaged across the whole dataset\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## TODO for students: Please compute Mean squared error and return as output. ##\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Calculate the loss\")\n",
    "    #################################################\n",
    "    # Mean squared error (MSE)\n",
    "    return ...\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    save_model_checkpoint(folder, filename, self.nnet)\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    load_model_checkpoint(folder, filename, self.nnet, args.device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class ValueNetwork(NeuralNet):\n",
    "\n",
    "  def __init__(self, game):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame\n",
    "        Instance of the OthelloGame class above\n",
    "    \"\"\"\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.nnet.to(args.device)\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples with each example is of form (board, pi, v)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        v_losses = []   # To store the losses per epoch\n",
    "        batch_count = int(len(examples) / args.batch_size)  # len(examples)=200, batch-size=64, batch_count=3\n",
    "        t = tqdm(range(batch_count), desc='Training Value Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)  # Read the ground truth information from MCTS simulation using the loaded examples\n",
    "          boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))  # Length of boards, pis, vis = 64\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "          # Predict\n",
    "          # To run on GPU if available\n",
    "          boards, target_vs = boards.contiguous().to(args.device), target_vs.contiguous().to(args.device)\n",
    "\n",
    "          # Compute output\n",
    "          _, out_v = self.nnet(boards)\n",
    "          l_v = self.loss_v(target_vs, out_v)  # Total loss\n",
    "\n",
    "          # Record loss\n",
    "          v_losses.append(l_v.item())\n",
    "          t.set_postfix(Loss_v=l_v.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          l_v.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      v: OthelloNet instance\n",
    "        Data of the OthelloNet class instance above;\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "        _, v = self.nnet(board)\n",
    "    return v.data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_v(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth variables corresponding to input\n",
    "      outputs: np.ndarray\n",
    "        Predictions of Network\n",
    "\n",
    "    Returns:\n",
    "      MSE Loss averaged across the whole dataset\n",
    "    \"\"\"\n",
    "    # Mean squared error (MSE)\n",
    "    return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    save_model_checkpoint(folder, filename, self.nnet)\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    load_model_checkpoint(folder, filename, self.nnet, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implement_the_Value_Network_Excercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.4: Train the value network and observe the MSE loss progress\n",
    "\n",
    "**Important:** Following cell will train the value network if you do not have access to the pretrained models in the `rl_for_games` repository.\n",
    "\n",
    "It will take a while to complete so we recommend using the fully trained value network provided in the `rl_for_games` repository that will be automatically loaded later below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  vnet = ValueNetwork(game)\n",
    "  vnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.5: Use a trained value network to play games\n",
    "\n",
    "Now that we have our value network all set up and trained, we're ready to test it by using it to play games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Play games using a value function\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'tvmzVHPBKKs'), ('Bilibili', 'BV1u54y1J7E6')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Play_games_using_a_value_function_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.5: Implement the Value-based player\n",
    "\n",
    "**Exercise:**\n",
    "* Sample random valid moves and use the value function to rank them\n",
    "* Choose the best move as the action and play it\n",
    "Show that doing so beats the random player\n",
    "\n",
    "**Hint:** You might need to change the sign of the value based on the player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's first initialize a new game and load in a pre-trained Value network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, we can create a player that makes use of the value function to decide what best action to take next.\n",
    "\n",
    "How do we choose the best move using our value network? We will simply compute the expected value (predicted value) of all possible moves and then select the best one based on which next state has the highest value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class ValueBasedPlayer():\n",
    "\n",
    "  def __init__(self, game, vnet):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      vnet: Value Network instance\n",
    "        Instance of the Value Network class above;\n",
    "\n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.vnet = vnet\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      candidates: List\n",
    "        Collection of tuples describing action and values of future predicted\n",
    "        states\n",
    "    \"\"\"\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    candidates = []\n",
    "    max_num_actions = 4\n",
    "    va = np.where(valids)[0]\n",
    "    va_list = va.tolist()\n",
    "    random.shuffle(va_list)\n",
    "    #################################################\n",
    "    ## TODO for students:\n",
    "    # 1. Use getNextState() to obtain the next board state,\n",
    "    # 2. Predict the value of the next state using the value network\n",
    "    # 3. Add the value and action as a tuple to the candidate list, after\n",
    "    #    reversing the sign of the value.\n",
    "\n",
    "    # Note: In zero-sum games, the players alternate turns, and the value\n",
    "    # function is trained from the perspective of one player (either black or\n",
    "    # white). To estimate the value for the other player, we need to negate the\n",
    "    # output of the value function.\n",
    "    # For example, if the value function trained for the perspective of white\n",
    "    # suggests a high likelihood (0.75) of winning from the current state, it\n",
    "    # implies that black is very unlikely (-0.75) to win from the same state. ##\n",
    "\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Implement the value-based player\")\n",
    "    #################################################\n",
    "    for a in va_list:\n",
    "      # Return next board state using getNextState() function\n",
    "      nextBoard, _ = ...\n",
    "      # Predict the value of next state using value network\n",
    "      value = ...\n",
    "      # Add the value and the action as a tuple to the candidate lists, note that you might need to change the sign of the value based on the player\n",
    "      candidates += ...\n",
    "\n",
    "      if len(candidates) == max_num_actions:\n",
    "        break\n",
    "\n",
    "    # Sort by the values\n",
    "    candidates.sort()\n",
    "\n",
    "    # Return action associated with highest value\n",
    "    return candidates[0][1]\n",
    "\n",
    "\n",
    "# Playing games between a value-based player and a random player\n",
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "player1 = ValueBasedPlayer(game, vnet).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "\n",
    "## Uncomment the code below to check your code!\n",
    "## Compute win rate for the value-based player (player 1)\n",
    "# result = arena.playGames(num_games, verbose=False)\n",
    "# print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "#       f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "# win_rate_player1 = result[0]/num_games\n",
    "# print(f\"\\nWin rate for player1 over 20 games: {round(win_rate_player1*100, 1)}%\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 15, Number of games won by player2 = 5 out of 20 games\n",
    "\n",
    "Win rate for player1 over 20 games: 75.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class ValueBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Value Based Player\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, vnet):\n",
    "    \"\"\"\n",
    "    Initialise value based player parameters\n",
    "\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      vnet: Value Network instance\n",
    "        Instance of the Value Network class above;\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.vnet = vnet\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Simulate game play\n",
    "\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      candidates: List\n",
    "        Collection of tuples describing action and values of future predicted states\n",
    "    \"\"\"\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    candidates = []\n",
    "    max_num_actions = 4\n",
    "    va = np.where(valids)[0]\n",
    "    va_list = va.tolist()\n",
    "    random.shuffle(va_list)\n",
    "    for a in va_list:\n",
    "      # Return next board state using getNextState() function\n",
    "      nextBoard, _ = self.game.getNextState(board, 1, a)\n",
    "      # Predict the value of next state using value network\n",
    "      value = self.vnet.predict(nextBoard)\n",
    "      # Add the value and the action as a tuple to the candidate lists, note that you might need to change the sign of the value based on the player\n",
    "      candidates += [(-value, a)]\n",
    "\n",
    "      if len(candidates) == max_num_actions:\n",
    "        break\n",
    "\n",
    "    # Sort by the values\n",
    "    candidates.sort()\n",
    "\n",
    "    # Return action associated with highest value\n",
    "    return candidates[0][1]\n",
    "\n",
    "\n",
    "# Playing games between a value-based player and a random player\n",
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "player1 = ValueBasedPlayer(game, vnet).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "\n",
    "## Uncomment the code below to check your code!\n",
    "## Compute win rate for the value-based player (player 1)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over 20 games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implement_the_Value_based_player_Excercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Train a policy network from expert game data\n",
    "\n",
    "*Time estimate: ~35 mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Goal**: Train a policy network via supervised learning / behavioural cloning.\n",
    "\n",
    "**Exercise**:\n",
    "* Train a network to predict the next move in an expert dataset by maximizing\n",
    "the log likelihood of the next action.\n",
    "* Use a policy network to play games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "A reminder of the network architecture\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/static/CNN.jpg\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Train a policy network\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'vj9gKNJ19D8'), ('Bilibili', 'BV1tg411M7Rg')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Train_a_policy_network_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.1: Define the Policy network\n",
    "\n",
    "In section 2 we simply chose to move based on the highest predicted value of the next step. Here, we will use a different approach. We will train a network to directly produce a policy function as a distribution over all possible discrete actions, given the current state. Learning will be based  on expert moves; thus, we call this behavioral cloning.\n",
    "\n",
    "We will use the exact same network that we have used above for the value function learning. But now we will train the network explicitly on every single move of expert players."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.1: Implement the `PolicyNetwork`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "For computing our objective function, we will use the negative log-likelihood of targets $t_i$ by using the cross-entropy function:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{CE} = - \\frac{1}{N} \\sum_i^N t_i \\cdot \\log(output_i)\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: remember that the OthelloNet already returns the **Log**-softmax of the output from the 3rd linear layer..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class PolicyNetwork(NeuralNet):\n",
    "\n",
    "  def __init__(self, game):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame\n",
    "        Instance of the OthelloGame class\n",
    "    \"\"\"\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.nnet.to(args.device)\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples where each example is of form (board, pi, v)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        pi_losses = []\n",
    "\n",
    "        batch_count = int(len(examples) / args.batch_size)\n",
    "\n",
    "        t = tqdm(range(batch_count), desc='Training Policy Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "          boards, pis, _ = list(zip(*[examples[i] for i in sample_ids]))\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_pis = torch.FloatTensor(np.array(pis))\n",
    "\n",
    "          # Predict\n",
    "          boards, target_pis = boards.contiguous().to(args.device), target_pis.contiguous().to(args.device)\n",
    "\n",
    "          #################################################\n",
    "          ## TODO for students:\n",
    "          # 1. Compute the policy (pi) predicted by OthelloNNet() ##\n",
    "          # 2. Implement the loss_pi() function below and then use it to update the policy loss. ##\n",
    "\n",
    "          # Fill out function and remove\n",
    "          raise NotImplementedError(\"Compute the output\")\n",
    "          #################################################\n",
    "          # Compute output\n",
    "          out_pi, _ = ...\n",
    "          l_pi = ...\n",
    "\n",
    "          # Record loss\n",
    "          pi_losses.append(l_pi.item())\n",
    "          t.set_postfix(Loss_pi=l_pi.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          l_pi.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      Data from the OthelloNet instance\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "      pi,_ = self.nnet(board)\n",
    "    return torch.exp(pi).data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_pi(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Negative Log Likelihood(NLL) of Targets\n",
    "\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth variables corresponding to input\n",
    "      outputs: np.ndarray\n",
    "        Predictions of Network\n",
    "\n",
    "    Returns:\n",
    "      Negative Log Likelihood calculated as: When training a model, we aspire to\n",
    "      find the minima of a loss function given a set of parameters (in a neural\n",
    "      network, these are the weights and biases).\n",
    "      Sum the loss function to all the correct classes. So, whenever the network\n",
    "      assigns high confidence at the correct class, the NLL is low, but when the\n",
    "      network assigns low confidence at the correct class, the NLL is high.\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    ## TODO for students: Return the negative log likelihood of targets.\n",
    "    # For more information, here is a reference that connects the expression to\n",
    "    # the neg-log-prob: https://gombru.github.io/2018/05/23/cross_entropy_loss/ ##\n",
    "\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Compute the loss\")\n",
    "    #################################################\n",
    "    return ...\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    save_model_checkpoint(folder, filename, self.nnet)\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    load_model_checkpoint(folder, filename, self.nnet, args.device)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class PolicyNetwork(NeuralNet):\n",
    "\n",
    "  def __init__(self, game):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame\n",
    "        Instance of the OthelloGame class\n",
    "    \"\"\"\n",
    "    self.nnet = OthelloNNet(game, args)\n",
    "    self.board_x, self.board_y = game.getBoardSize()\n",
    "    self.action_size = game.getActionSize()\n",
    "    self.nnet.to(args.device)\n",
    "\n",
    "  def train(self, games):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      games: list\n",
    "        List of examples where each example is of form (board, pi, v)\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(self.nnet.parameters())\n",
    "\n",
    "    for examples in games:\n",
    "      for epoch in range(args.epochs):\n",
    "        print('EPOCH ::: ' + str(epoch + 1))\n",
    "        self.nnet.train()\n",
    "        pi_losses = []\n",
    "\n",
    "        batch_count = int(len(examples) / args.batch_size)\n",
    "\n",
    "        t = tqdm(range(batch_count), desc='Training Policy Network')\n",
    "        for _ in t:\n",
    "          sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "          boards, pis, _ = list(zip(*[examples[i] for i in sample_ids]))\n",
    "          boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "          target_pis = torch.FloatTensor(np.array(pis))\n",
    "\n",
    "          # Predict\n",
    "          boards, target_pis = boards.contiguous().to(args.device), target_pis.contiguous().to(args.device)\n",
    "\n",
    "          # Compute output\n",
    "          out_pi, _ = self.nnet(boards)\n",
    "          l_pi = self.loss_pi(target_pis, out_pi)\n",
    "\n",
    "          # Record loss\n",
    "          pi_losses.append(l_pi.item())\n",
    "          t.set_postfix(Loss_pi=l_pi.item())\n",
    "\n",
    "          # Compute gradient and do SGD step\n",
    "          optimizer.zero_grad()\n",
    "          l_pi.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "  def predict(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      Data from the OthelloNet instance\n",
    "    \"\"\"\n",
    "    # Timing\n",
    "    start = time.time()\n",
    "\n",
    "    # Preparing input\n",
    "    board = torch.FloatTensor(board.astype(np.float64))\n",
    "    board = board.contiguous().to(args.device)\n",
    "    board = board.view(1, self.board_x, self.board_y)\n",
    "    self.nnet.eval()\n",
    "    with torch.no_grad():\n",
    "      pi,_ = self.nnet(board)\n",
    "    return torch.exp(pi).data.cpu().numpy()[0]\n",
    "\n",
    "  def loss_pi(self, targets, outputs):\n",
    "    \"\"\"\n",
    "    Calculates Negative Log Likelihood(NLL) of Targets\n",
    "\n",
    "    Args:\n",
    "      targets: np.ndarray\n",
    "        Ground Truth variables corresponding to input\n",
    "      outputs: np.ndarray\n",
    "        Predictions of Network\n",
    "\n",
    "    Returns:\n",
    "      Negative Log Likelihood calculated as: When training a model, we aspire to\n",
    "      find the minima of a loss function given a set of parameters (in a neural\n",
    "      network, these are the weights and biases).\n",
    "      Sum the loss function to all the correct classes. So, whenever the network\n",
    "      assigns high confidence at the correct class, the NLL is low, but when the\n",
    "      network assigns low confidence at the correct class, the NLL is high.\n",
    "    \"\"\"\n",
    "    ## For more information, here is a reference that connects the expression to\n",
    "    # the neg-log-prob: https://gombru.github.io/2018/05/23/cross_entropy_loss/\n",
    "    return -torch.sum(targets * outputs) / targets.size()[0]\n",
    "\n",
    "  def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    save_model_checkpoint(folder, filename, self.nnet)\n",
    "\n",
    "  def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "    load_model_checkpoint(folder, filename, self.nnet, args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implement_the_Policy_Network_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.2: Train the policy network\n",
    "\n",
    "**Important:** Following cell will only train if you do not have access to the\n",
    "pretrained models in the `rl_for_games` repository.\n",
    "\n",
    "It will take a while to complete so we recommend using the fully trained policy\n",
    "network provided in the `rl_for_games` repository that will be automatically\n",
    "loaded later below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  pnet = PolicyNetwork(game)\n",
    "  pnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.3: Use a trained policy network to play games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 6: Play games using a policy network\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'yHtVqT2Nstk'), ('Bilibili', 'BV1DU4y1n7gD')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note**: in the video's softmax function, $T=1$ is the softmax kernel and $z_i$ is the networks output before softmax transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Play_games_using_a_Policy_Network_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.3: Implement the `PolicyBasedPlayer`\n",
    "\n",
    "**Exercise:**\n",
    "* Use the policy network to give probabilities for the next move.\n",
    "* Build a player that takes the move given the maximum probability by the network.\n",
    "* Compare this to another player that samples moves according to the probability distribution output by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "First we initialize the game and load in the pre-trained policy net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next we create our policy-based player by using the policy network to produce a set of action probabilities for all valid board positions.\n",
    "\n",
    "There are at least 2 ways then to choose the next action:\n",
    "1. **Sampling-based player**: we sample from the action probability distribution. This will result in actions with higher probabilities to be randomly selected more often than actions with lower probabilities.\n",
    "2. **Greedy player**: we always choose the action with the highest action probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class PolicyBasedPlayer():\n",
    "\n",
    "  def __init__(self, game, pnet, greedy=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      pnet: Policy Network instance\n",
    "        Instance of the Policy Network class above\n",
    "      greedy: Boolean\n",
    "        If true, implement greedy approach\n",
    "        Else, implement random sample policy based player\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.pnet = pnet\n",
    "    self.greedy = greedy\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      a: np.ndarray\n",
    "        If greedy, implement greedy policy player\n",
    "        Else, implement random sample policy based player\n",
    "    \"\"\"\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    #################################################\n",
    "    ## TODO for students:\n",
    "    # 1. Compute the action probabilities using policy network pnet()\n",
    "    # 2. Mask invalid moves (set their action probability to 0) using valids\n",
    "    #     variable and the action probabilites computed above.\n",
    "    # 3. Compute the sum over the probabilities of the valid actions and store\n",
    "    #    them in sum_vap. ##\n",
    "\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Define the play\")\n",
    "    #################################################\n",
    "    action_probs = ...\n",
    "    vap = ...  # Masking invalid moves\n",
    "    sum_vap = ...\n",
    "\n",
    "    if sum_vap > 0:\n",
    "      vap /= sum_vap  # Renormalize\n",
    "    else:\n",
    "      # If all valid moves were masked we make all valid moves equally probable\n",
    "      print(\"All valid moves were masked, doing a workaround.\")\n",
    "      vap = vap + valids\n",
    "      vap /= np.sum(vap)\n",
    "\n",
    "    if self.greedy:\n",
    "      # Greedy policy player\n",
    "      a = np.where(vap == np.max(vap))[0][0]\n",
    "    else:\n",
    "      # Sample-based policy player\n",
    "      a = np.random.choice(self.game.getActionSize(), p=vap)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "# Playing games\n",
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "player1 = PolicyBasedPlayer(game, pnet, greedy=True).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "## Uncomment below to test!\n",
    "# result = arena.playGames(num_games, verbose=False)\n",
    "# print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "#       f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "# win_rate_player1 = result[0] / num_games\n",
    "# print(f\"\\nWin rate for greedy policy player 1 (vs random player 2) over {num_games} games: {round(win_rate_player1*100, 1)}%\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 16, Number of games won by player2 = 4 out of 20 games\n",
    "\n",
    "Win rate for greedy policy player 1 (vs random player 2) over 20 games: 80.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class PolicyBasedPlayer():\n",
    "\n",
    "  def __init__(self, game, pnet, greedy=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      pnet: Policy Network instance\n",
    "        Instance of the Policy Network class above\n",
    "      greedy: Boolean\n",
    "        If true, implement greedy approach\n",
    "        Else, implement random sample policy based player\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.pnet = pnet\n",
    "    self.greedy = greedy\n",
    "\n",
    "  def play(self, board):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      board: np.ndarray\n",
    "        Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      a: np.ndarray\n",
    "        If greedy, implement greedy policy player\n",
    "        Else, implement random sample policy based player\n",
    "    \"\"\"\n",
    "    valids = self.game.getValidMoves(board, 1)\n",
    "    action_probs = self.pnet.predict(board)\n",
    "    vap = action_probs*valids  # Masking invalid moves\n",
    "    sum_vap = np.sum(vap)\n",
    "\n",
    "    if sum_vap > 0:\n",
    "      vap /= sum_vap  # Renormalize\n",
    "    else:\n",
    "      # If all valid moves were masked we make all valid moves equally probable\n",
    "      print(\"All valid moves were masked, doing a workaround.\")\n",
    "      vap = vap + valids\n",
    "      vap /= np.sum(vap)\n",
    "\n",
    "    if self.greedy:\n",
    "      # Greedy policy player\n",
    "      a = np.where(vap == np.max(vap))[0][0]\n",
    "    else:\n",
    "      # Sample-based policy player\n",
    "      a = np.random.choice(self.game.getActionSize(), p=vap)\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "# Playing games\n",
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "player1 = PolicyBasedPlayer(game, pnet, greedy=True).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "## Uncomment below to test!\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "\n",
    "win_rate_player1 = result[0] / num_games\n",
    "print(f\"\\nWin rate for greedy policy player 1 (vs random player 2) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implement_the_Policy_Based_Player_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Section 4: Player comparisons\n",
    "\n",
    "Time estimate: ~5 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next we want to compare how our different players fair, i.e. random vs. value-based vs. policy-based (greedy or sampling-based)... Feel free to explore some of the comparisons we have not explicitly provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Comparing a sampling-based policy based player versus a random player\n",
    "\n",
    "There's often randomness in the results as we are running the players for a low number of games (only 20 games due to compute + time costs). So, when students are running the cells they might not get the expected result. To better measure the strength of players you can run more games!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet, greedy=False).play\n",
    "player2 = RandomPlayer(game).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "\n",
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"\\nWin rate for sample-based policy based player 1 (vs random player 2) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 12, Number of games won by player2 = 8 out of 20 games\n",
    "\n",
    "Win rate for sample-based policy based player 1 (vs random player 2) over 20 games: 60.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Compare greedy policy based player versus value based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Load in trained value network\n",
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "# Alternatively, if the downloading of trained model didn't work (will train the model)\n",
    "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  path = \"nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "  loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  vnet = ValueNetwork(game)\n",
    "  vnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet).play\n",
    "player2 = ValueBasedPlayer(game, vnet).play\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "\n",
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"\\nWin rate for greedy policy based player 1 (vs value based player) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 7, Number of games won by player2 = 13 out of 20 games\n",
    "\n",
    "Win rate for sample-based policy based player 1 (vs random player 2) over 20 games: 35.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Compare greedy policy based player versus sampling-based policy player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "set_seed(seed=SEED)\n",
    "num_games = 20\n",
    "game = OthelloGame(6)\n",
    "player1 = PolicyBasedPlayer(game, pnet).play # greedy player\n",
    "player2 = PolicyBasedPlayer(game, pnet, greedy=False).play # sample-based player\n",
    "arena = Arena.Arena(player1, player2, game, display=OthelloGame.display)\n",
    "result = arena.playGames(num_games, verbose=False)\n",
    "print(f\"\\nNumber of games won by player1 = {result[0]}, \"\n",
    "      f\"Number of games won by player2 = {result[1]} out of {num_games} games\")\n",
    "\n",
    "win_rate_player1 = result[0]/num_games\n",
    "print(f\"\\nWin rate for greedy policy player 1 (vs sample based policy player) over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 14, Number of games won by player2 = 6 out of 20 games\n",
    "\n",
    "Win rate for greedy policy player 1 (vs sample based policy player) over 20 games: 70.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Take a few minutes to recap what the different players are with your group and how they're choosing their actions (random player, value player, greedy policy player, sample-based policy player)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 5: Plan using Monte Carlo Rollouts\n",
    "\n",
    "*Time estimate: ~35 mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Goal**: Teach the students the core idea behind using simulated rollouts to understand the future and value actions.\n",
    "\n",
    "**Exercise**:\n",
    "* Build a loop to run Monte Carlo simulations using the policy network.\n",
    "* Use simulated rollouts to obtain better estimates of the value of moves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "A reminder of the network architecture\n",
    "\n",
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D5_ReinforcementLearningForGamesAndDlThinking3/static/CNN.jpg\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 7: Play using Monte-Carlo rollouts\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'DtCWDIlSo18'), ('Bilibili', 'BV1MM4y1T77C')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Play_using_Monte_Carlo_Rollouts_Video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Load in trained value and policy networks\n",
    "model_save_name = 'ValueNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "vnet = ValueNetwork(game)\n",
    "vnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "model_save_name = 'PolicyNetwork.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\"\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "pnet = PolicyNetwork(game)\n",
    "pnet.load_checkpoint(folder=path, filename=model_save_name)\n",
    "\n",
    "\n",
    "# Alternative if the downloading of trained model didn't work (will train the model)\n",
    "if not os.listdir('nma_rl_games/alpha-zero/pretrained_models/models/'):\n",
    "  path = \"nma_rl_games/alpha-zero/pretrained_models/data/\"\n",
    "  loaded_games = loadTrainExamples(folder=path, filename='checkpoint_1.pth.tar')\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  vnet = ValueNetwork(game)\n",
    "  vnet.train(loaded_games)\n",
    "\n",
    "  set_seed(seed=SEED)\n",
    "  game = OthelloGame(6)\n",
    "  pnet = PolicyNetwork(game)\n",
    "  pnet.train(loaded_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 5.1: Define the Monte Carlo planner\n",
    "\n",
    "To recapitulate, the goal of the Monte Carlo algorithm here is to evaluate the outcome of different plans according to the policy used, i.e., what future Value function do we expect to end up with. So we will iterate the game forward in time (according to the game rules and with specific strategies) and return the Value function output at the end of that iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 5.1: Implement the `MonteCarlo` planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, we will first set up the Monte Carlo planner.\n",
    "\n",
    "**Note**: because we will be simulating different actions into the future (planning), we want to distinguish potential board positions from actual ones that are taken. Therefore, the current (actual) position of the board that is used as a starting point for Monte-Carlo simulations will be labeled \"canonical board\" to avoid confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class MonteCarlo():\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      nnet: OthelloNet instance\n",
    "        Instance of the OthelloNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "\n",
    "    self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "    self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "\n",
    "  def simulate(self, canonicalBoard):\n",
    "    \"\"\"\n",
    "    Simulate one Monte Carlo rollout\n",
    "\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      temp_v:\n",
    "        Terminal State\n",
    "    \"\"\"\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    init_start_state = s\n",
    "    temp_v = 0\n",
    "    isfirstAction = None\n",
    "    current_player = -1  # opponent's turn (the agent has already taken an action before the simulation)\n",
    "    self.Ps[s], _ = self.nnet.predict(canonicalBoard)\n",
    "\n",
    "    for i in range(self.args.maxDepth):  # maxDepth\n",
    "\n",
    "      if s not in self.Es:\n",
    "        self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "      if self.Es[s] != 0:\n",
    "        # Terminal state\n",
    "        temp_v = self.Es[s] * current_player\n",
    "        break\n",
    "\n",
    "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "      valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "      self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "      sum_Ps_s = np.sum(self.Ps[s])\n",
    "\n",
    "      if sum_Ps_s > 0:\n",
    "        self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "      else:\n",
    "        # If all valid moves were masked make all valid moves equally probable\n",
    "        # NB! All valid moves may be masked if either your NNet architecture is\n",
    "        # insufficient or you've get overfitting or something else.\n",
    "        # If you have got dozens or hundreds of these messages you should pay\n",
    "        # attention to your NNet and/or training process.\n",
    "        log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "        self.Ps[s] = self.Ps[s] + valids\n",
    "        self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "      ##########################################################################\n",
    "      ## TODO for students: Take a action according to the policy distribution.\n",
    "      #  1. Sample action according to the policy distribution.\n",
    "      #  2. Find the next state and the next player from the environment.\n",
    "      #  3. Get the canonical form of the next state.\n",
    "      # Fill out function and remove\n",
    "      raise NotImplementedError(\"Take the action, find the next state\")\n",
    "      ##########################################################################\n",
    "      # Choose action according to the policy distribution\n",
    "      a = ...\n",
    "      # Find the next state and the next player\n",
    "      next_s, next_player = self.game.getNextState(..., ..., ...)\n",
    "      canonicalBoard = self.game.getCanonicalForm(..., ...)\n",
    "      s = self.game.stringRepresentation(next_s)\n",
    "      current_player *= -1\n",
    "      # Initial policy\n",
    "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "      temp_v = v.item() * current_player\n",
    "\n",
    "    return temp_v\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class MonteCarlo():\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      nnet: OthelloNet instance\n",
    "        Instance of the OthelloNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "\n",
    "    self.Ps = {}  # Stores initial policy (returned by neural net)\n",
    "    self.Es = {}  # Stores game.getGameEnded ended for board s\n",
    "\n",
    "  # Call this rollout\n",
    "  def simulate(self, canonicalBoard):\n",
    "    \"\"\"\n",
    "    Simulate one Monte Carlo rollout\n",
    "\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      temp_v:\n",
    "        Terminal State\n",
    "    \"\"\"\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    init_start_state = s\n",
    "    temp_v = 0\n",
    "    isfirstAction = None\n",
    "    current_player = -1  # opponent's turn (the agent has already taken an action before the simulation)\n",
    "    self.Ps[s], _ = self.nnet.predict(canonicalBoard)\n",
    "\n",
    "    for i in range(self.args.maxDepth):  # maxDepth\n",
    "\n",
    "      if s not in self.Es:\n",
    "        self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n",
    "      if self.Es[s] != 0:\n",
    "        # Terminal state\n",
    "        temp_v = self.Es[s] * current_player\n",
    "        break\n",
    "\n",
    "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "      valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "      self.Ps[s] = self.Ps[s] * valids  # Masking invalid moves\n",
    "      sum_Ps_s = np.sum(self.Ps[s])\n",
    "\n",
    "      if sum_Ps_s > 0:\n",
    "        self.Ps[s] /= sum_Ps_s  # Renormalize\n",
    "      else:\n",
    "        # If all valid moves were masked make all valid moves equally probable\n",
    "        # NB! All valid moves may be masked if either your NNet architecture is\n",
    "        # insufficient or you've get overfitting or something else.\n",
    "        # If you have got dozens or hundreds of these messages you should pay\n",
    "        # attention to your NNet and/or training process.\n",
    "        log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "        self.Ps[s] = self.Ps[s] + valids\n",
    "        self.Ps[s] /= np.sum(self.Ps[s])\n",
    "\n",
    "      # Choose action according to the policy distribution\n",
    "      a = np.random.choice(self.game.getActionSize(), p=self.Ps[s])\n",
    "      # Find the next state and the next player\n",
    "      next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n",
    "      canonicalBoard = self.game.getCanonicalForm(next_s, next_player)\n",
    "      s = self.game.stringRepresentation(next_s)\n",
    "      current_player *= -1\n",
    "      # Initial policy\n",
    "      self.Ps[s], v = self.nnet.predict(canonicalBoard)\n",
    "      temp_v = v.item() * current_player\n",
    "\n",
    "    return temp_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Implement_the_Monte_Carlo_Planner_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 5.2: Use Monte Carlo simulations to play games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Goal:** Use simple Monte Carlo planning to play games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 8: Play with planning\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'plmFzAy3H5s'), ('Bilibili', 'BV1Kg411M78Y')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Play_with_planning_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 5.2: Monte-Carlo simulations\n",
    "\n",
    "Now we can run Monte-Carlo simulations. We essentially evaluate for a given action taken now what the potential future outcome will be. So we want to choose different future actions according to the policy (random vs. value-based vs. policy-based) and evaluate the outcomes. We will simulate potential future outcomes and compute their value and then average those values to get a sense of the averge value of our policy used for a given immediate (current) action. This iteration (rollout) can be expressed in the following pseudo-code:\n",
    "\n",
    "---\n",
    "\n",
    "for $i$ in $1$ to $k$:\n",
    "1. Choose the ith ranked action $a^i$ for the current state $s_t$ according to our specific policy function.\n",
    "2. Run N Monte Carlo rollouts from $s_{t+1}$ following the application of $a^i$, $j$ steps into the future (depth)\n",
    "3. Average the estimated values for each rollout to get: $V_{i}^{AVG}$\n",
    "4. Build an array of $[V_{i}^{AVG}, a^i]$ pairs.\n",
    "\n",
    "To act, choose the action associated with the highest average value, i.e., $\\underset{a^i}{\\operatorname{argmax}}(V_{i}^{AVG})$. We will use $k=3$, $j=3$ and $N=10$.\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "**Exercise**:\n",
    "* Incorporate Monte Carlo simulations into an agent.\n",
    "* Run the resulting player versus the random, value-based, and policy-based players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load MC model from the repository\n",
    "mc_model_save_name = 'MC.pth.tar'\n",
    "path = \"nma_rl_games/alpha-zero/pretrained_models/models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "class MonteCarloBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Player based on Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      nnet: OthelloNet instance\n",
    "        Instance of the OthelloNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    ############################################################################\n",
    "    ## TODO for students: Instantiate the Monte Carlo class.\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Use Monte Carlo!\")\n",
    "    ############################################################################\n",
    "    self.mc = ...\n",
    "    self.K = self.args.mc_topk\n",
    "\n",
    "  def play(self, canonicalBoard):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      best_action: tuple\n",
    "        (avg_value, action) i.e., Average value associated with corresponding\n",
    "        action, i.e., Action with the highest topK probability\n",
    "    \"\"\"\n",
    "    self.qsa = []\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    Ps, v = self.nnet.predict(canonicalBoard)\n",
    "    valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "    Ps = Ps * valids  # Masking invalid moves\n",
    "    sum_Ps_s = np.sum(Ps)\n",
    "\n",
    "    if sum_Ps_s > 0:\n",
    "      Ps /= sum_Ps_s  # Renormalize\n",
    "    else:\n",
    "      # If all valid moves were masked make all valid moves equally probable\n",
    "      # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "      # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
    "      log = logging.getLogger(__name__)\n",
    "      log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "      Ps = Ps + valids\n",
    "      Ps /= np.sum(Ps)\n",
    "\n",
    "    num_valid_actions = np.shape(np.nonzero(Ps))[1]\n",
    "\n",
    "    if num_valid_actions < self.K:\n",
    "      top_k_actions = np.argpartition(Ps,-num_valid_actions)[-num_valid_actions:]\n",
    "    else:\n",
    "      top_k_actions = np.argpartition(Ps,-self.K)[-self.K:]  # To get actions that belongs to top k prob\n",
    "    ############################################################################\n",
    "    ## TODO for students:\n",
    "    #  1. For each action in the top-k actions\n",
    "    #  2. Get the next state using getNextState() function.\n",
    "    #     You can find the implementation of this function in Tutorial 1 in\n",
    "    #     `OthelloGame()` class.\n",
    "    #  3. Get the canonical form of the getNextState().\n",
    "    # Fill out function and remove\n",
    "    raise NotImplementedError(\"Loop for the top actions\")\n",
    "    ############################################################################\n",
    "    for action in ...:\n",
    "      next_s, next_player = self.game.getNextState(..., ..., ...)\n",
    "      next_s = self.game.getCanonicalForm(..., ...)\n",
    "\n",
    "      values = []\n",
    "\n",
    "      # Do some rollouts\n",
    "      for rollout in range(self.args.numMCsims):\n",
    "        value = self.mc.simulate(next_s)\n",
    "        values.append(value)\n",
    "\n",
    "      # Average out values\n",
    "      avg_value = np.mean(values)\n",
    "      self.qsa.append((avg_value, action))\n",
    "\n",
    "    self.qsa.sort(key=lambda a: a[0])\n",
    "    self.qsa.reverse()\n",
    "    best_action = self.qsa[0][1]\n",
    "    return best_action\n",
    "\n",
    "  def getActionProb(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Get probabilities associated with each action\n",
    "\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      action_probs: List\n",
    "        Probability associated with corresponding action\n",
    "    \"\"\"\n",
    "    if self.game.getGameEnded(canonicalBoard, 1) != 0:\n",
    "      return np.zeros((self.game.getActionSize()))\n",
    "\n",
    "    else:\n",
    "      action_probs = np.zeros((self.game.getActionSize()))\n",
    "      best_action = self.play(canonicalBoard)\n",
    "      action_probs[best_action] = 1\n",
    "\n",
    "    return action_probs\n",
    "\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "# Run the resulting player versus the random player\n",
    "rp = RandomPlayer(game).play\n",
    "num_games = 20  # Feel free to change this number\n",
    "\n",
    "n1 = NNet(game)  # nNet players\n",
    "n1.load_checkpoint(folder=path, filename=mc_model_save_name)\n",
    "args1 = dotdict({'numMCsims': 10, 'maxRollouts':5, 'maxDepth':5, 'mc_topk': 3})\n",
    "\n",
    "## Uncomment below to check Monte Carlo agent!\n",
    "# print('\\n******MC player versus random player******')\n",
    "# mc1 = MonteCarloBasedPlayer(game, n1, args1)\n",
    "# n1p = lambda x: np.argmax(mc1.getActionProb(x))\n",
    "# arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
    "# MC_result = arena.playGames(num_games, verbose=False)\n",
    "# print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "#       f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "# win_rate_player1 = MC_result[0]/num_games\n",
    "# print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 19, number of games won by player2 = 1, out of 20 games\n",
    "\n",
    "Win rate for player1 over 20 games: 95.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class MonteCarloBasedPlayer():\n",
    "  \"\"\"\n",
    "  Simulate Player based on Monte Carlo Algorithm\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, game, nnet, args):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      game: OthelloGame instance\n",
    "        Instance of the OthelloGame class above;\n",
    "      nnet: OthelloNet instance\n",
    "        Instance of the OthelloNNet class above;\n",
    "      args: dictionary\n",
    "        Instantiates number of iterations and episodes, controls temperature threshold, queue length,\n",
    "        arena, checkpointing, and neural network parameters:\n",
    "        learning-rate: 0.001, dropout: 0.3, epochs: 10, batch_size: 64,\n",
    "        num_channels: 512\n",
    "    \"\"\"\n",
    "    self.game = game\n",
    "    self.nnet = nnet\n",
    "    self.args = args\n",
    "    self.mc = MonteCarlo(game, nnet, args)\n",
    "    self.K = self.args.mc_topk\n",
    "\n",
    "  def play(self, canonicalBoard):\n",
    "    \"\"\"\n",
    "    Simulate Play on Canonical Board\n",
    "\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "\n",
    "    Returns:\n",
    "      best_action: tuple\n",
    "        (avg_value, action) i.e., Average value associated with corresponding action\n",
    "        i.e., Action with the highest topK probability\n",
    "    \"\"\"\n",
    "    self.qsa = []\n",
    "    s = self.game.stringRepresentation(canonicalBoard)\n",
    "    Ps, v = self.nnet.predict(canonicalBoard)\n",
    "    valids = self.game.getValidMoves(canonicalBoard, 1)\n",
    "    Ps = Ps * valids  # Masking invalid moves\n",
    "    sum_Ps_s = np.sum(Ps)\n",
    "\n",
    "    if sum_Ps_s > 0:\n",
    "      Ps /= sum_Ps_s  # Renormalize\n",
    "    else:\n",
    "      # If all valid moves were masked make all valid moves equally probable\n",
    "      # NB! All valid moves may be masked if either your NNet architecture is insufficient or you've get overfitting or something else.\n",
    "      # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.\n",
    "      log = logging.getLogger(__name__)\n",
    "      log.error(\"All valid moves were masked, doing a workaround.\")\n",
    "      Ps = Ps + valids\n",
    "      Ps /= np.sum(Ps)\n",
    "\n",
    "    num_valid_actions = np.shape(np.nonzero(Ps))[1]\n",
    "\n",
    "    if num_valid_actions < self.K:\n",
    "      top_k_actions = np.argpartition(Ps,-num_valid_actions)[-num_valid_actions:]\n",
    "    else:\n",
    "      top_k_actions = np.argpartition(Ps,-self.K)[-self.K:]  # To get actions that belongs to top k prob\n",
    "\n",
    "    for action in top_k_actions:\n",
    "      next_s, next_player = self.game.getNextState(canonicalBoard, 1, action)\n",
    "      next_s = self.game.getCanonicalForm(next_s, next_player)\n",
    "\n",
    "      values = []\n",
    "\n",
    "      # Do some rollouts\n",
    "      for rollout in range(self.args.numMCsims):\n",
    "        value = self.mc.simulate(next_s)\n",
    "        values.append(value)\n",
    "\n",
    "      # Average out values\n",
    "      avg_value = np.mean(values)\n",
    "      self.qsa.append((avg_value, action))\n",
    "\n",
    "    self.qsa.sort(key=lambda a: a[0])\n",
    "    self.qsa.reverse()\n",
    "    best_action = self.qsa[0][1]\n",
    "    return best_action\n",
    "\n",
    "  def getActionProb(self, canonicalBoard, temp=1):\n",
    "    \"\"\"\n",
    "    Get probabilities associated with each action\n",
    "\n",
    "    Args:\n",
    "      canonicalBoard: np.ndarray\n",
    "        Canonical Board of size n x n [6x6 in this case]\n",
    "      temp: Integer\n",
    "        Signifies if game is in terminal state\n",
    "\n",
    "    Returns:\n",
    "      action_probs: List\n",
    "        Probability associated with corresponding action\n",
    "    \"\"\"\n",
    "    if self.game.getGameEnded(canonicalBoard, 1) != 0:\n",
    "      return np.zeros((self.game.getActionSize()))\n",
    "\n",
    "    else:\n",
    "      action_probs = np.zeros((self.game.getActionSize()))\n",
    "      best_action = self.play(canonicalBoard)\n",
    "      action_probs[best_action] = 1\n",
    "\n",
    "    return action_probs\n",
    "\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "game = OthelloGame(6)\n",
    "# Run the resulting player versus the random player\n",
    "rp = RandomPlayer(game).play\n",
    "num_games = 20  # Feel free to change this number\n",
    "\n",
    "n1 = NNet(game)  # nNet players\n",
    "n1.load_checkpoint(folder=path, filename=mc_model_save_name)\n",
    "args1 = dotdict({'numMCsims': 10, 'maxRollouts':5, 'maxDepth':5, 'mc_topk': 3})\n",
    "\n",
    "## Uncomment below to check Monte Carlo agent!\n",
    "print('\\n******MC player versus random player******')\n",
    "mc1 = MonteCarloBasedPlayer(game, n1, args1)\n",
    "n1p = lambda x: np.argmax(mc1.getActionProb(x))\n",
    "arena = Arena.Arena(n1p, rp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note**: the Monte-Carlo player doesn't seem to be doing much better than the random player... This is because training of a good MC player is VERY compute intensive and we have not done extensive training here. In Bonus 2 below, you can play with a Monte-Carlo Tree Search (MCTS) player that has been trained well and you'll see that it performs much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Monte_Carlo_simulations_Exercise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Monte-Carlo player against Value-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print('\\n******MC player versus value-based player******')\n",
    "set_seed(seed=SEED)\n",
    "vp = ValueBasedPlayer(game, vnet).play  # Value-based player\n",
    "arena = Arena.Arena(n1p, vp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 17, number of games won by player2 = 3, out of 20 games\n",
    "\n",
    "Win rate for player1 over 20 games: 85.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Monte-Carlo player against Policy-based player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print('\\n******MC player versus policy-based player******')\n",
    "set_seed(seed=SEED)\n",
    "pp = PolicyBasedPlayer(game, pnet).play  # Policy player\n",
    "arena = Arena.Arena(n1p, pp, game, display=OthelloGame.display)\n",
    "MC_result = arena.playGames(num_games, verbose=False)\n",
    "\n",
    "print(f\"\\nNumber of games won by player1 = {MC_result[0]}, \"\n",
    "      f\"number of games won by player2 = {MC_result[1]}, out of {num_games} games\")\n",
    "win_rate_player1 = MC_result[0]/num_games\n",
    "print(f\"\\nWin rate for player1 over {num_games} games: {round(win_rate_player1*100, 1)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Number of games won by player1 = 18, number of games won by player2 = 2, out of 20 games\n",
    "\n",
    "Win rate for player1 over 20 games: 90.0%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 6: Ethical aspects\n",
    "\n",
    "*Time estimate: ~5 mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 9: Unbeatable opponents\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'q7181lvoNpM'), ('Bilibili', 'BV19M4y1K75Z')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Unbeatable_opponents_Video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, you have learned about:\n",
    "*   the Othello game, how to implement a game loop, and create a random player.\n",
    "*   value-based players and compared them to a random player.\n",
    "*   policy-based players and compared them to random and value-based players.\n",
    "*   players with Monte Carlo planner and compared them to random, value-based and policy-based players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 19: Outro\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')\n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'uQ26iIUzmtw'), ('Bilibili', 'BV1w64y167qd')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Submit your feedback\n",
    "content_review(f\"{feedback_prefix}_Outro_Video\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "W3D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
