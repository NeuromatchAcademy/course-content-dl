{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/instructor/W2D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/instructor/W2D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Modeling sequencies and encoding text\n",
    "**Week 2, Day 3: Modern RNNs**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Bhargav Srinivasa Desikan, Anis Zahedifard, James Evans\n",
    "\n",
    "__Content reviewers:__ Lily Cheng, Melvin Selim Atay, Ezekiel Williams, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Nina Kudryashova, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Roberto Guidotti, Gagana B, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "----\n",
    "# Tutorial objectives\n",
    "\n",
    "Before we begin with exploring how RNNs excel at modelling sequences, we will explore some of the other ways we can model sequences, encode text, and make meaningful measurements using such encodings and embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/n263c/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These are the slides for the videos in this tutorial. If you want to locally download the slides, click [here](https://osf.io/n263c/download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "\n",
    "# @markdown #### There may be *errors* and/or *warnings* reported during the installation. However, they are to be ignored.\n",
    "!pip install torchtext==0.10.0 --quiet\n",
    "!pip install --upgrade gensim --quiet\n",
    "!pip install unidecode --quiet\n",
    "!pip install hmmlearn --quiet\n",
    "!pip install fasttext --quiet\n",
    "!pip install nltk --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install python-Levenshtein --quiet\n",
    "\n",
    "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
    "from evaltools.airtable import AirtableForm\n",
    "\n",
    "# Generate airtable form\n",
    "atform = AirtableForm('appn7VdPRseSoMXEG','W2D3_T1','https://portal.neuromatchacademy.org/api/redirect/to/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "from torchtext.legacy import data, datasets\n",
    "from torchtext.vocab import FastText\n",
    "\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure Settings\n",
    "import ipywidgets as widgets\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title  Load Dataset from `nltk`\n",
    "# No critical warnings, so we suppress it\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('webtext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "import requests\n",
    "\n",
    "def cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between vec_a and vec_b\n",
    "\n",
    "    Args:\n",
    "      vec_a: np.ndarray\n",
    "        Vector #1\n",
    "      vec_b: np.ndarray\n",
    "        Vector #2\n",
    "\n",
    "    Returns:\n",
    "      Cosine similarity between vec_a and vec_b\n",
    "    \"\"\"\n",
    "    return np.dot(vec_a, vec_b) / (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "\n",
    "def tokenize(sentences):\n",
    "  \"\"\"\n",
    "  Tokenize the sentence\n",
    "  ~ from nltk.tokenize library use word_tokenize\n",
    "\n",
    "  Args:\n",
    "    sentences: string\n",
    "      Sentence to be tokenized\n",
    "\n",
    "  Returns:\n",
    "    token: list\n",
    "      List of tokens generated from sentences\n",
    "  \"\"\"\n",
    "  token = word_tokenize(sentences)\n",
    "\n",
    "  return token\n",
    "\n",
    "\n",
    "def plot_train_val(x, train, val, train_label, val_label,\n",
    "                   title, y_label, color):\n",
    "  \"\"\"\n",
    "  Plots training/validation performance per epoch\n",
    "\n",
    "  Args:\n",
    "    x: np.ndarray\n",
    "      Input data\n",
    "    train: list\n",
    "      Training data performance\n",
    "    val: list\n",
    "      Validation data performance\n",
    "    train_label: string\n",
    "      Train Label [specifies training criterion]\n",
    "    color: string\n",
    "      Specifies color of plot\n",
    "    val_label: string\n",
    "      Validation Label [specifies validation criterion]\n",
    "    title: string\n",
    "      Specifies title of plot\n",
    "    y_label: string\n",
    "      Specifies performance criterion\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  plt.plot(x, train, label=train_label, color=color)\n",
    "  plt.plot(x, val, label=val_label, color=color, linestyle='--')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel(y_label)\n",
    "  plt.title(title)\n",
    "\n",
    "\n",
    "def load_dataset(emb_vectors, sentence_length=50, seed=2021):\n",
    "  \"\"\"\n",
    "  Load dataset\n",
    "\n",
    "  Args:\n",
    "    sentence_length: int\n",
    "      Length of sentence\n",
    "    seed: int\n",
    "      Set seed for reproducibility\n",
    "    emb_vectors: FastText type\n",
    "      Embedding vectors of size 111051\n",
    "\n",
    "  Returns:\n",
    "    TEXT: Field instance\n",
    "      Text\n",
    "    vocab_size: int\n",
    "      Specifies size of TEXT\n",
    "    train_iter: BucketIterator\n",
    "      Training iterator\n",
    "    valid_iter: BucketIterator\n",
    "      Validation iterator\n",
    "    test_iter: BucketIterator\n",
    "      Test iterator\n",
    "  \"\"\"\n",
    "  TEXT = data.Field(sequential=True,\n",
    "                    tokenize=tokenize,\n",
    "                    lower=True,\n",
    "                    include_lengths=True,\n",
    "                    batch_first=True,\n",
    "                    fix_length=sentence_length)\n",
    "  LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "  train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "  TEXT.build_vocab(train_data, vectors=emb_vectors)\n",
    "  LABEL.build_vocab(train_data)\n",
    "\n",
    "  train_data, valid_data = train_data.split(split_ratio=0.7,\n",
    "                                            random_state=random.seed(seed))\n",
    "  train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data,\n",
    "                                                                  valid_data,\n",
    "                                                                  test_data),\n",
    "                                                                  batch_size=32,\n",
    "                                                                  sort_key=lambda x: len(x.text),\n",
    "                                                                  repeat=False,\n",
    "                                                                  shuffle=True)\n",
    "  vocab_size = len(TEXT.vocab)\n",
    "\n",
    "  print(f'Data are loaded. sentence length: {sentence_length} '\n",
    "        f'seed: {seed}')\n",
    "\n",
    "  return TEXT, vocab_size, train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness.\n",
    "  NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "DEVICE = set_device()\n",
    "SEED = 2021\n",
    "set_seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Sequences, Markov Chains & HMMs\n",
    "\n",
    "*Time estimate: ~45mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Sequences & Markov Processes\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1jg411774B\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"ApkE7UFaJAQ\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 1: Sequences & Markov Processes')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this notebook we will be exploring the world of sequences - thinking of what kind of data can be thought of as sequences, and how these sequences can be represented as Markov Chains and Hidden Markov Models. These ideas and methods were an important part of natural language processing and language modelling, and serve as a useful way to ground ourselves before we dive into neural network methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Why is this relevant? How are these sequences related to modern recurrent neural networks?\n",
    "\n",
    "Like we mentioned before, the notion of modelling sequences of data - in this particular case, **language**, is an ideal place to start. RNNs themselves were constructed keeping in mind sequences, and the ability to temporally model sequences is what inspired RNNs (and the family of LSTM, GRUs - we will see this in the next notebook).\n",
    "\n",
    "Markov models and hidden markov models serve as an introduction to these concepts because they were some of the earliest ways to think about sequences. They do not capture a lot of the complexity that RNNs excel at, but are an useful way of thinking of sequences, probabilities, and how we can use these concepts to perform  tasks such as text generation, or classification - tasks that RNNs excel at today. \n",
    "\n",
    "Think of this section as an introduction to thinking with sequences and text data, and as a historical introduction to the world of modelling sequential data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.1: What data are sequences?\n",
    "\n",
    "Native Sequences:\n",
    "\n",
    "- Temporally occurring events (e.g., history, stock prices)\n",
    "- Temporally processed events (e.g., communication)\n",
    "- Topologically connected components (e.g., polymers, peptides)\n",
    "\n",
    "Synthetic Sequences: \n",
    "\n",
    "- Anything processed as a sequence (e.g., scanned pixels in an image)\n",
    "\n",
    "Sequences can be represented as a Markov Process - since this notion of sequential data is intrinsically linked to RNNs, it is a good place for us to start, and natural language (text!) will be our sequence of choice. \n",
    "\n",
    "We will be using the Brown corpus which comes loaded with NLTK, and using the entire corpus - this requires a lot of RAM for some of the methods, so we recommend using a smaller subset of categories if you do not have enough RAM.\n",
    "\n",
    "We will be using some of the code from this [tutorial](https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html) and this [Jupyter notebook](https://github.com/StrikingLoo/ASOIAF-Markov/blob/master/ASOIAF.ipynb)\n",
    "\n",
    "The first few cells of code all involve set-up; some of this code will be hidden because they are not necessary to understand the ideas of markov models, but the way data is setup can be vital to the way the model performs (something in common with neural network models!).\n",
    "\n",
    "Let us start with loading our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "category = ['editorial', 'fiction', 'government', 'news', 'religion']\n",
    "sentences = brown.sents(categories=category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have our sentences, let us look at some statistics to get an idea of what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "lengths = [len(sentence) for sentence in sentences]\n",
    "lengths = pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Find the 80-th percentile: the minimal length of such a sentence, which is longer than at least 80% of sentences in the *Brown corpus*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "lengths.quantile(.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This gives us an idea of what our dataset looks like, along with some average lengths. This kind of quick data exploration can be very useful - we know how long different sequences are, and how we might want to collect these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Since we will be modeling words as sequences in sentences, let us first collect all the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "corpus_words = []\n",
    "for sentence in sentences:\n",
    "  for word in sentence:\n",
    "    if \"''\" not in word and \"``\" not in word:\n",
    "      corpus_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Corpus length: {len(corpus_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "corpus_words[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We'll now get distinct (unique) words and create a matrix to represent all these words. This is necessary because we will be using this matrix to look at the probability of the words in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Creating Matrices and Distinct Words\n",
    "distinct_words = list(set(corpus_words))\n",
    "word_idx_dict = {word: i for i, word in enumerate(distinct_words)}\n",
    "distinct_words_count = len(list(set(corpus_words)))\n",
    "next_word_matrix = np.zeros([distinct_words_count, distinct_words_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(\"Number of distinct words: \" + str(distinct_words_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In the following lines of code we are populating the matrix that tracks the next word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Populating Matrix that tracks next word\n",
    "for i, word in enumerate(corpus_words[:-1]):\n",
    "  first_word_idx = word_idx_dict[word]\n",
    "  next_word_idx = word_idx_dict[corpus_words[i+1]]\n",
    "  next_word_matrix[first_word_idx][next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we have the information ready to construct a markov chain. The next word matrix is crucial in this, as it allows us to go from one word in the sequence to the next. We will soon see how this is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.2: What is a Markov Chain or Model?\n",
    "\n",
    "A Markov Chain (or Model) is a:\n",
    "- stochastic model describing a sequence of possible events\n",
    "- the probability of each event depends only on the state attained in the previous event.\n",
    "- a countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC) [vs. a continuous-time process or CTMC].\n",
    "- The classic formal language model is a Markov Model\n",
    "\n",
    "*Helpful explanations from [eric mjl's tutorial](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/#non-autoregressive-homoskedastic-emissions)*!\n",
    "\n",
    "The simplest Markov models assume that we have a _system_ that contains a finite set of states,\n",
    "and that the _system_ transitions between these states with some probability at each time step $t$,\n",
    "thus generating a sequence of states over time.\n",
    "Let's call these states $S$, where\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\{s_1, s_2, ..., s_n\\}\n",
    "\\end{equation}\n",
    "\n",
    "To keep things simple, let's start with three states:\n",
    "\n",
    "\\begin{equation}\n",
    "S = \\{s_1, s_2, s_3\\}\n",
    "\\end{equation}\n",
    "\n",
    "A Markov model generates a sequence of states, with one possible realization being:\n",
    "\n",
    "\\begin{equation}\n",
    "\\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\}\n",
    "\\end{equation}\n",
    "\n",
    "And generically, we represent it as a sequence of states $x_t, x_{t+1}... x_{t+n}$.(We have chosen a different symbol to not confuse the \"generic\" state with the specific realization.) Graphically, a plain and simple Markov model looks like the following:\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernRecurrentNeuralNetworks/static/cell_chain.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Modeling transitions between states\n",
    "\n",
    "To know how a system transitions between states, we now need a **transition matrix**.\n",
    "\n",
    "The transition matrix describes the probability of transitioning from one state to another (The probability of staying in the same state is semantically equivalent to transitioning to the same state).\n",
    "\n",
    "By convention, transition matrix rows correspond to the state at time $t$,\n",
    "while columns correspond to state at time $t+1$.\n",
    "Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated.\n",
    "\n",
    "Let's call the transition matrix $P_{transition}$:\n",
    "\n",
    "\\begin{equation}\n",
    "P_{transition} = \n",
    "  \\begin{pmatrix}\n",
    "  p_{11} & p_{12} & p_{13} \\\\\n",
    "  p_{21} & p_{22} & p_{23} \\\\\n",
    "  p_{31} & p_{32} & p_{33} \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Using the transition matrix, we can express different behaviors of the system. For example:\n",
    "1. by assigning larger probability mass to the diagonals, we can express that the system likes to stay in the current state;\n",
    "2. by assigning larger probability mass to the off-diagonal, we can express that the system likes to transition out of its current state.\n",
    "\n",
    "In our case, this matrix is created by measuring how often one word appeared after another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function for most likely word\n",
    "def most_likely_word_after(word):\n",
    "  # We check for the word most likely\n",
    "  # to occur using the matrix\n",
    "  most_likely = next_word_matrix[word_idx_dict[word]].argmax()\n",
    "  return distinct_words[most_likely]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Using our most likely word function, we can begin to create chains of words and create sequences. In the code below we create a naive chain that simply choses the most likely word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function for building Naive Chain\n",
    "def naive_chain(word, length=15):\n",
    "  current_word = word\n",
    "  sentence = word\n",
    "  # We now build a naive chain by\n",
    "  # picking up the most likely word\n",
    "  for _ in range(length):\n",
    "    sentence += ' '\n",
    "    next_word = most_likely_word_after(current_word)\n",
    "    sentence += next_word\n",
    "    current_word = next_word\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let us now use this naive chain to see what comes up, using some simple words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(naive_chain('the'))\n",
    "print(naive_chain('I'))\n",
    "print(naive_chain('What'))\n",
    "print(naive_chain('park'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We notice that after the word `the`, `United States` comes up each time. All the other sequencies starting from other words also end up at `the` quite often. Since we use a *deterministic* markov chain model, its next state only depends on the previous one. Therefore, once the sequence comes to `the`, it inevitably continues the sequence with the `United States`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now be a little more sophisticated, and return words in a sequence using a *weighted choice*, which randomly selects the next word from a set of words with some probability (weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function for weighted choice\n",
    "def weighted_choice(objects, weights):\n",
    "  \"\"\"\n",
    "  Returns a random element from the sequence of 'objects',\n",
    "  the likelihood of the objects is weighted according\n",
    "  to the sequence of 'weights', i.e. percentages.\n",
    "\n",
    "  Args:\n",
    "    objects: list\n",
    "      Sequence of objects\n",
    "    weights: list\n",
    "      Sequence of weights per object\n",
    "\n",
    "  Returns:\n",
    "    Random element from the sequence of 'objects',\n",
    "    the likelihood of the objects is weighted according\n",
    "    to the sequence of 'weights', i.e. percentages.\n",
    "  \"\"\"\n",
    "\n",
    "  weights = np.array(weights, dtype=np.float64)\n",
    "  sum_of_weights = weights.sum()\n",
    "  # Standardization:\n",
    "  np.multiply(weights, 1 / sum_of_weights)\n",
    "  weights = weights.cumsum()\n",
    "  x = random.random()\n",
    "  for i in range(len(weights)):\n",
    "    if x < weights[i]:\n",
    "      return objects[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function for sampling next word with weights\n",
    "def sample_next_word_after(word, alpha=0):\n",
    "  \"\"\"\n",
    "  Helper function to sample next word with weights based on present word\n",
    "\n",
    "  Args:\n",
    "    word: string\n",
    "      Current word\n",
    "    alpha: int\n",
    "      Offset\n",
    "\n",
    "  Returns:\n",
    "    Next word\n",
    "  \"\"\"\n",
    "  next_word_vector = next_word_matrix[word_idx_dict[word]] + alpha\n",
    "  likelihoods = next_word_vector/next_word_vector.sum()\n",
    "  return weighted_choice(distinct_words, likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "sample_next_word_after('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "sample_next_word_after('The')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "There! We don't see the same word twice, because of the added randomisation (i.e., stochasticity). Our algorithm calculates how likely it is to find a certain word after a given word (`The` in this case) in the corpus, and then generates 1 sample of the next word with a matching probability. \n",
    "\n",
    "In this example, we generated only one next word. Now, using this function, we'll build a chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function for a stochastic chain using weighted choice\n",
    "def stochastic_chain(word, length=15):\n",
    "  \"\"\"\n",
    "  Helper function to build stochastic chain using\n",
    "  weighted choices.\n",
    "\n",
    "  Args:\n",
    "    word: string\n",
    "      Word\n",
    "    length: int\n",
    "      Length of sentence\n",
    "\n",
    "  Returns:\n",
    "    sentence: string\n",
    "      Sentence built using stochastic chain\n",
    "  \"\"\"\n",
    "  current_word = word\n",
    "  sentence = word\n",
    "\n",
    "  for _ in range(length):\n",
    "    sentence += ' '\n",
    "    next_word = sample_next_word_after(current_word)\n",
    "    sentence += next_word\n",
    "    current_word = next_word\n",
    "\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "stochastic_chain('Hospital')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Neat - we can create stochastic chains for a single word. For a more effective language model, we would want to model sets of words - in the following cells, we create sets of words to predict a chain after a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def sequences_matrices(k=3):\n",
    "  # @title Code to build sets of words for more realistic sequences\n",
    "  \"\"\"\n",
    "  Code to build sets of words for more realistic sequences\n",
    "\n",
    "  Args:\n",
    "    k: int\n",
    "      Sequence length\n",
    "\n",
    "  Returns:\n",
    "    k_words_idx_dict: dict\n",
    "      Dictionary of words and corresponding indices\n",
    "    distinct_sets_of_k_words: list\n",
    "      Set of k-word sets\n",
    "    next_after_k_words_matrix: list\n",
    "      Transition matrix\n",
    "  \"\"\"\n",
    "  sets_of_k_words = [' '.join(corpus_words[i:i+k]) for i, _ in enumerate(corpus_words[:-k])]\n",
    "  sets_count = len(list(set(sets_of_k_words)))\n",
    "  next_after_k_words_matrix = dok_matrix((sets_count, len(distinct_words)))\n",
    "  distinct_sets_of_k_words = list(set(sets_of_k_words))\n",
    "  k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n",
    "  distinct_k_words_count = len(list(set(sets_of_k_words)))\n",
    "  for i, word in tqdm(enumerate(sets_of_k_words[:-k])):\n",
    "    word_sequence_idx = k_words_idx_dict[word]\n",
    "    next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
    "    next_after_k_words_matrix[word_sequence_idx, next_word_idx] += 1\n",
    "  return k_words_idx_dict, distinct_sets_of_k_words,next_after_k_words_matrix\n",
    "\n",
    "k_words_idx_dict, distinct_sets_of_k_words, next_after_k_words_matrix = sequences_matrices(k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's have a look at what that bit of code did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "distinct_sets_of_k_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Great! Now we are going to create a transition matrix for the sets of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Code to populate matrix of sets of words\n",
    "for i, word in tqdm(enumerate(distinct_sets_of_k_words[:-k])):\n",
    "  word_sequence_idx = k_words_idx_dict[word]\n",
    "  next_word_idx = word_idx_dict[corpus_words[i+k]]\n",
    "  next_after_k_words_matrix[word_sequence_idx, next_word_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We now have what we need to build a stochastic chain over a `K` set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function for Stochastic Chain for sets of words\n",
    "def stochastic_chain_sequence(words, chain_length=15, k=2):\n",
    "  \"\"\"\n",
    "  Function for stochastic Chain for sets of words\n",
    "\n",
    "  Args:\n",
    "    words: string\n",
    "      Sentence\n",
    "    chain_length: int\n",
    "      Length of stochastic chain [default: 15]\n",
    "    k: int\n",
    "      Sequence length [default: 2]\n",
    "\n",
    "  Returns:\n",
    "    sentence: string\n",
    "      Sentence generated using stochastic chain\n",
    "  \"\"\"\n",
    "  current_words = words.split(' ')\n",
    "  if len(current_words) != k:\n",
    "    raise ValueError(f'Wrong number of words, expected {k}')\n",
    "  sentence = words\n",
    "\n",
    "  # Pre-calculate seq embedding + transition matrix for a given k\n",
    "  matrices = sequences_matrices(k=k)\n",
    "\n",
    "  for _ in range(chain_length):\n",
    "    sentence += ' '\n",
    "    next_word = sample_next_word_after_sequence(matrices,' '.join(current_words))\n",
    "    sentence += next_word\n",
    "    current_words = current_words[1:]+[next_word]\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function to sample next word in sequence\n",
    "def sample_next_word_after_sequence(matrices,\n",
    "                                    word_sequence,\n",
    "                                    alpha=0):\n",
    "  # Unpack a tuple of matrices\n",
    "  \"\"\"\n",
    "  Function to sample next word in sequence\n",
    "\n",
    "  Args:\n",
    "    matrices: list\n",
    "      Transition matrix\n",
    "    word_sequence: list\n",
    "      Word sequence\n",
    "    alpha: int\n",
    "      Offset\n",
    "\n",
    "  Returns:\n",
    "    Weighted choice of distinct words based on likelihoods\n",
    "  \"\"\"\n",
    "  k_words_idx_dict,distinct_sets_of_k_words, next_after_k_words_matrix = matrices\n",
    "\n",
    "  next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n",
    "  likelihoods = next_word_vector/next_word_vector.sum()\n",
    "  return weighted_choice(distinct_words, likelihoods.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "stochastic_chain_sequence('Judges under the', chain_length=3, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Great! This sentence was created using two of the techniques we recently saw - creating sets of words, and using a weighted average stochastic chain. Both of these methods contributed in making it a more meaningful sequence of words. Some of these notions are also captured by Recurrent Neural Networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 1.2: How does changing parameters affect the generated sentences?\n",
    "\n",
    "Try and use a set of words but using a naive chain, and try a stochastic chain with a low value of k (i.e., 2), and a higher value (i.e., 5). How do these different configurations change the quality of the sequences produced? Below you have sample code to try these out.\n",
    "\n",
    "```python\n",
    "stochastic_chain_sequence(..., chain_length=..., k=...)\n",
    "```\n",
    "\n",
    "You should be able to use these matrices and the previous functions to be able to create the necessary configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q1', text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 1.3: What is a Hidden Markov Model?\n",
    "\n",
    "A 1960s advance (by Leonard Baum and colleagues): Hidden Markov Models are:\n",
    "- a Markov model in which the system modeled is assumed to be a Markov process/chain with unobservable (\"hidden\") states. \n",
    "- HMM assumes there is another surrogate process whose behavior \"depends\" on the state--you learn about the state by observing the surrogate process. \n",
    "- HMMs have successfully been applied in fields where the goal is to recover a data sequence not immediately observable (but other data that depend on the sequence are).\n",
    "- The first dominant application: Speech and text processing (1970s)\n",
    "\n",
    "In this sub-section we will use the python library [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/tutorial.html#training-hmm-parameters-and-inferring-the-hidden-states), which is part of the *scikit-learn* ecosystem. [nlg-with-hmmlearn](https://github.com/mfilej/nlg-with-hmmlearn) offers useful code snippets to adapt ```hmmlearn``` for text data. Because we are using a package that offers many out of the box implementations for HMMs, we don't have to worry about the states, transition matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "sentences = brown.sents(categories=category)\n",
    "words = [word.lower() for sentence in sentences for word in sentence]\n",
    "lengths = [len(sentence) for sentence in sentences]\n",
    "alphabet = set(words)\n",
    "\n",
    "# Encode words\n",
    "le = LabelEncoder()\n",
    "_ = le.fit(list(alphabet))\n",
    "\n",
    "# Find word freqeuncies\n",
    "seq = le.transform(words)\n",
    "features = np.fromiter(seq, np.int64) # Create 1D array\n",
    "features = np.atleast_2d(features).T # View input as at least 2D array\n",
    "fd = FreqDist(seq) # Returns frequency distribution of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now that we have our data setup, we can create our model. We use a multinomial HMM with 8 states, and can either do a random initialisation or use word frequences. We recommend trying both options!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function to create Default Multinomial HMM model\n",
    "def get_model(num_states):\n",
    "  \"\"\"\n",
    "  Function to create Default Multinomial HMM model\n",
    "\n",
    "  Args:\n",
    "    num_states: int\n",
    "      Specifies number of states in HMM model\n",
    "\n",
    "  Returns:\n",
    "    model: HMM instance\n",
    "      Default Multinomial HMM model\n",
    "  \"\"\"\n",
    "  print(\"Initial parameter estimation using built-in method\")\n",
    "  model = hmm.MultinomialHMM(n_components=num_states, init_params='ste')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function to create Default Multinomial HMM model information of relative frequencies of words\n",
    "def frequencies(num_states):\n",
    "  \"\"\"\n",
    "  Function to create Default Multinomial HMM\n",
    "  model information of relative frequencies of words\n",
    "\n",
    "  Args:\n",
    "    num_states: int\n",
    "      Specifies number of states in HMM model\n",
    "\n",
    "  Returns:\n",
    "    model: HMM instance\n",
    "      Default Multinomial HMM model\n",
    "      replete with relative frequencies of words\n",
    "      and emission probabilities\n",
    "  \"\"\"\n",
    "  print(\"Initial parameter estimation using relative frequencies\")\n",
    "\n",
    "  frequencies = np.fromiter((fd.freq(i) for i in range(len(alphabet))),\n",
    "                            dtype=np.float64)\n",
    "  emission_prob = np.stack([frequencies]*num_states)\n",
    "\n",
    "  model = hmm.MultinomialHMM(n_components=num_states, init_params='st')\n",
    "  model.emissionprob_ = emission_prob\n",
    "  return model\n",
    "\n",
    "\n",
    "print(frequencies(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note**:\n",
    "\n",
    "The following lines of code are commented out because they take a long time (~17 mins for default Brown corpus categories). \n",
    "\n",
    "If you do not have that time, you can download the default model to try to generate text. You have to uncomment the appropriate lines.\n",
    "\n",
    "**Note:** Either you may want to uncomment Line 11 or Line 14, not both, as the output variable `model` will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Fitting a default multinomial HMM. This might take a while to run(~17 mins)\n",
    "def run_model(features, length, num_states):\n",
    "  model = get_model(num_states)\n",
    "  model = model.fit(features, lengths)\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "num_states = 8\n",
    "## Uncomment, if you have time!\n",
    "# model = run_model(features, lengths, num_states)\n",
    "\n",
    "## Another way to get a model is to use default frequencies when initialising the model\n",
    "# model = frequencies(num_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Alternatively, you could use a saved model. Here is a [link](https://drive.google.com/file/d/1IymcmcO48V6q3x-6dhf7-OU5NByo5W2F/view?usp=sharing) to the default model, which you can download and then upload into Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Execute this cell to download the saved model.\n",
    "import pickle\n",
    "\n",
    "url = \"https://osf.io/5k6cs/download\"\n",
    "r = requests.get(url)\n",
    "with open('model_w2d3_t1.pkl', 'wb') as fd:\n",
    "  fd.write(r.content)\n",
    "\n",
    "# Load the pickle file\n",
    "with open(\"model_w2d3_t1.pkl\", \"rb\") as file:\n",
    "  model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Function to generate words given HMM model\n",
    "def generate_text(model, num_lines = 5, random_len=15):\n",
    "  \"\"\"\n",
    "  Function to generate words from given HMM model\n",
    "\n",
    "  Args:\n",
    "    model: HMM instance\n",
    "      Multinomial HMM Model\n",
    "    num_lines: int\n",
    "      Specifies number of lines [default: 5]\n",
    "    random_len: int\n",
    "      Specifies random sequence length [default: 15]\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  for _i in range(num_lines):\n",
    "    set_seed(_i)\n",
    "    symbols, _states = model.sample(random_len)\n",
    "\n",
    "    # To scale transformation to original representation\n",
    "    output = le.inverse_transform(np.squeeze(symbols))\n",
    "\n",
    "    for word in output:\n",
    "      print(word, end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "generate_text(model, num_lines=2, random_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We see that a hidden markov model also does well in generating text. We encourage you to try out different initialisations and hyperparameters to see how the model does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### (Bonus) Exercise 1.3: Transition probabilities \n",
    "\n",
    "\n",
    "We have seen how we can use sequences of text to form probability chains, as well as how we can use out of the box models to generate text. In this exercise, you will be using your own data to generate sequences using ```hmmlearn``` or any other implementation of a markov model. Explore the transition probabilities in your corpus and generate sentences. For example, one such exploration can be - How does using a model with the word frequencies compare to a default model?\n",
    "\n",
    "Perform any one such comparison or exploration, and generate 3 sentences or 50 words using your model. You should be able to use all the existing functions defined for this exercise.\n",
    "\n",
    "**Note:** We suggest to do this exercise after the completion of both tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# load your own dataset and create a model using the frequencies based HMM model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Useful links for Markov Models and HMM:\n",
    "\n",
    "Here are some useful links if you wish to explore this topic further.\n",
    "\n",
    "- [Markov Chain Text](https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6)\n",
    "\n",
    "- [Python QuantEcon: Finite Markov Chains with Finance](https://python.quantecon.org/finite_markov.html)\n",
    "\n",
    "- [Markov Models from the ground up, with python](https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/)\n",
    "\n",
    "- [GenTex](https://github.com/nareshkumar66675/GenTex)\n",
    "\n",
    "- [HMM learn](https://hmmlearn.readthedocs.io/en/latest/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: Word Embeddings\n",
    "\n",
    "*Time estimate: ~60mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Textual Dimension Reduction\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1oM4y1P7Mn\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"kweySXAZ1os\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 2: Textual Dimension Reduction')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Words or subword units such as morphemes are the basic units that we use to express meaning  in language. The technique of mapping words to vectors of real numbers is known as word embedding. \n",
    "\n",
    "Word2vec is based on theories of distributional semantics - words that appear around each other are more likely to mean similar things than words that do not appear around each other. Keeping this in mind, our job is to create a high dimensional space where these semantic relations are preserved. The innovation in word2vec is the realisation that we can use unlabelled, running text in sentences as inputs for a supervised learning algorithm--as a self-supervision task. It is supervised because we use the words in a sentence to serve as positive and negative examples. Lets break this down:\n",
    "\n",
    "... \"use the kitchen knife to chop the vegetables\"\n",
    "\n",
    "**C1   C2   C3   T   C4   C5   C6   C7**\n",
    "\n",
    "Here, the target word is knife, and the context words are the ones in its immediate (6-word) window. \n",
    "The first word2vec method well see is called skipgram, where the task is to assign a probability for how likely it is that the context window appears around the target word. In the training process, positive examples are samples of words and their context words, and negative examples are created by sampling from pairs of words that do not appear nearby one another. \n",
    "\n",
    "This method of implementing word2vec is called skipgram with negative sampling. So while the algorithm tries to better learn which context words are likely to appear around a target word, it ends up pushing the embedded representations for every word so that they are located optimally (e.g., with minimal semantic distortion). In this process of adjusting embedding values, the algorithm brings semantically similar words close together in the resulting high dimensional space, and dissimilar words far away. \n",
    "\n",
    "Another word2vec training method, Continuous Bag of Words (CBOW), works in a similar fashion, and tries to predict the target word, given context. This is converse of skipgram, which tries to predict the context, given the target word. Skip-gram represents rare words and phrases well, often requiring more data for stable representations, while CBOW is several times faster to train than the skip-gram, but with slightly better accuracy for the frequent words in its prediction task. The popular gensim implementation of word2vec has both the methods included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Creating Word Embeddings\n",
    "\n",
    "We will create embeddings for a subset of categories in [Brown corpus](https://www1.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html).  In order to achieve this task we will use [gensim](https://radimrehurek.com/gensim/) library to create word2vec embeddings. Gensims word2vec expects a sequence of sentences as its input. Each sentence is a list of words.\n",
    "Calling `Word2Vec(sentences, iter=1)` will run two passes over the sentences iterator (or, in general iter+1 passes). The first pass collects words and their frequencies to build an internal dictionary tree structure. The second and subsequent passes train the neural model. \n",
    "`Word2vec` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "One of them is for pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, theres not enough data to make any meaningful training on those words, so its best to ignore them:\n",
    "\n",
    "`model = Word2Vec(sentences, min_count=10)  # default value is 5`\n",
    "\n",
    "\n",
    "A reasonable value for min_count is between 0-100, depending on the size of your dataset.\n",
    "\n",
    "Another parameter is the size of the NN layers, which correspond to the degrees of freedom the training algorithm has:\n",
    "\n",
    "`model = Word2Vec(sentences, size=200)  # default value is 100`\n",
    "\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds.\n",
    "\n",
    "The last of the major parameters (full list [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) is for training parallelization, to speed up training:\n",
    "\n",
    "`model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "category = ['editorial', 'fiction', 'government', 'mystery', 'news', 'religion',\n",
    "            'reviews', 'romance', 'science_fiction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def create_word2vec_model(category='news', size=50, sg=1, min_count=5):\n",
    "  \"\"\"\n",
    "  Helper function to create word2vec model\n",
    "\n",
    "  Args:\n",
    "    category: string\n",
    "      Specifies category (editorial/fiction/government/mystery/news/religion/reviews/romance/science_fiction)\n",
    "    size: int\n",
    "      Specifies size [default: 50]\n",
    "    min_count: int\n",
    "      Minimum sentence length [default: 5]\n",
    "    sg: int\n",
    "      Skip gram length\n",
    "\n",
    "  Returns:\n",
    "    model: Word2Vec instance\n",
    "      Word2Vec model\n",
    "  \"\"\"\n",
    "  try:\n",
    "    sentences = brown.sents(categories=category)\n",
    "    model = Word2Vec(sentences, vector_size=size, sg=sg, min_count=min_count)\n",
    "\n",
    "  except (AttributeError, TypeError):\n",
    "      raise AssertionError('Input variable \"category\" should be a string or list,'\n",
    "      '\"size\", \"sg\", \"min_count\" should be integers')\n",
    "\n",
    "  return model\n",
    "\n",
    "def model_dictionary(model):\n",
    "  \"\"\"\n",
    "  Helper function to build model dictionary\n",
    "\n",
    "  Args:\n",
    "    model:  Word2Vec instance\n",
    "      Word2Vec model\n",
    "\n",
    "  Returns:\n",
    "    words: list\n",
    "      Maps word to index position\n",
    "  \"\"\"\n",
    "  words = list(model.wv.key_to_index)\n",
    "  return words\n",
    "\n",
    "def get_embedding(word, model):\n",
    "  \"\"\"\n",
    "  Helper function to get embedding\n",
    "\n",
    "  Args:\n",
    "    model:  Word2Vec instance\n",
    "      Word2Vec model\n",
    "    word: string\n",
    "      Word for which embedding is to be extracted\n",
    "\n",
    "  Returns:\n",
    "    Word-index if word is in model_dictionary\n",
    "    None otherwise\n",
    "  \"\"\"\n",
    "  if word in model.wv.key_to_index:\n",
    "    return model.wv[word]\n",
    "  else:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "all_categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "w2vmodel = create_word2vec_model(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(model_dictionary(w2vmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(get_embedding('weather', w2vmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Visualizing Word Embedding\n",
    "\n",
    "We can now obtain the word embeddings for any word in the dictionary using word2vec. Let's visualize these embeddings to get an intuition of what these embeddings mean. The word embeddings obtained from word2vec model are in high dimensional space. We will use `tSNE` (t-distributed stochastic neighbor embedding), a statistical method for dimensionality deduction that allow us to visualize high-dimensional data in a 2D or 3D space. Here, we will use `tSNE` from [`scikit-learn`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) module to project our high dimensional embeddings in the 2D space (if you are not familiar with this method, think about `PCA`).\n",
    "\n",
    "\n",
    "For each word in `keys`, we pick the top 10 similar words (using cosine similarity) and plot them.  \n",
    "\n",
    " What should be the arrangement of similar words?\n",
    " What should be arrangement of the key clusters with respect to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "keys = ['voters', 'magic', 'love', 'God', 'evidence', 'administration', 'governments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def get_cluster_embeddings(keys):\n",
    "  \"\"\"\n",
    "  Function to cluster embeddings\n",
    "\n",
    "  Args:\n",
    "    keys: list\n",
    "      Keys from model_dictionary\n",
    "\n",
    "  Returns:\n",
    "    word_clusters: list\n",
    "      Word clusters\n",
    "    embedding_clusters: list\n",
    "      Embeddings for words in clusters\n",
    "  \"\"\"\n",
    "  embedding_clusters = []\n",
    "  word_clusters = []\n",
    "\n",
    "  # Find closest words and add them to cluster\n",
    "  for word in keys:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    if not word in w2vmodel.wv.key_to_index:\n",
    "      print('The word ', word, 'is not in the dictionary')\n",
    "      continue\n",
    "\n",
    "    for similar_word, _ in w2vmodel.wv.most_similar(word, topn=10):\n",
    "      words.append(similar_word)\n",
    "      embeddings.append(w2vmodel.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "\n",
    "  # Get embeddings for the words in clusers\n",
    "  embedding_clusters = np.array(embedding_clusters)\n",
    "  n, m, k = embedding_clusters.shape\n",
    "  tsne_model_en_2d = TSNE(perplexity=10, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "  embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "  return embeddings_en_2d, word_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def tsne_plot_similar_words(title, labels, embedding_clusters,\n",
    "                            word_clusters, a, filename=None):\n",
    "  \"\"\"\n",
    "  Generates tSNE plot for words\n",
    "\n",
    "  Args:\n",
    "    a: int\n",
    "      Offset\n",
    "    filename: string\n",
    "      Filename to save tSNE plot [default: None]\n",
    "    title: string\n",
    "      Specifies title of plot\n",
    "    label: list\n",
    "      List of labels\n",
    "    word_clusters: list\n",
    "      Word clusters\n",
    "    embedding_clusters: list\n",
    "      Embeddings for words in clusters\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(16, 9))\n",
    "  colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "  for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "    x = embeddings[:, 0]\n",
    "    y = embeddings[:, 1]\n",
    "    plt.scatter(x, y, color=color, alpha=a, label=label)\n",
    "    for i, word in enumerate(words):\n",
    "      plt.annotate(word,\n",
    "                   alpha=0.5,\n",
    "                   xy=(x[i], y[i]),\n",
    "                   xytext=(5, 2),\n",
    "                   textcoords='offset points',\n",
    "                   ha='right',\n",
    "                   va='bottom',\n",
    "                   size=10)\n",
    "  plt.legend(loc=\"lower left\")\n",
    "  plt.title(title)\n",
    "  plt.grid(True)\n",
    "  if filename:\n",
    "    plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "embeddings_en_2d, word_clusters = get_cluster_embeddings(keys)\n",
    "tsne_plot_similar_words('Similar words from Brown Corpus', keys, embeddings_en_2d, word_clusters, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: Exploring meaning with word embeddings\n",
    "\n",
    "While word2vec was the method that started it all, research has since boomed, and we now have more sophisticated ways to represent words. One such method is FastText, developed at Facebook AI research, which breaks words into sub-words: such a technique also allows us to create embedding representations for unseen words. In this section, we will explore how semantics and meaning are captured using embedidngs, after downloading a pre-trained FastText model. Downloading pre-trained models is a way for us to plug in word embeddings and explore them without training them ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download FastText English Embeddings of Dimension 100\n",
    "import os, io, zipfile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "zipurl = 'https://osf.io/w9sr7/download'\n",
    "print(f\"Downloading and unzipping the file... Please wait.\")\n",
    "with urlopen(zipurl) as zipresp:\n",
    "  with zipfile.ZipFile(io.BytesIO(zipresp.read())) as zfile:\n",
    "    zfile.extractall('.')\n",
    "print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load 100 dimension FastText Vectors using FastText library\n",
    "ft_en_vectors = fasttext.load_model('cc.en.100.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "print(f\"Length of the embedding is: {len(ft_en_vectors.get_word_vector('king'))}\")\n",
    "print(f\"Embedding for the word King is: {ft_en_vectors.get_word_vector('king')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Cosine similarity is used for similarities between words. Similarity is a scalar between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now find the 10 most similar words to \"King\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "ft_en_vectors.get_nearest_neighbors(\"king\", 10)  # Most similar by key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Semantic Measurements\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV15w411R7SW\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"Y45KIAOw4OY\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 3: Semantic Measurements')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "More on similarity between words. Let's check how similar different pairs of word are. Feel free to play around.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def getSimilarity(word1, word2):\n",
    "  \"\"\"\n",
    "  Defines similarity between set of words\n",
    "\n",
    "  Args:\n",
    "    word1: string\n",
    "      Word 1\n",
    "    word2: string\n",
    "      Word 2\n",
    "\n",
    "  Returns:\n",
    "    Cosine similarity between word1 vector and word2 vector\n",
    "  \"\"\"\n",
    "  v1 = ft_en_vectors.get_word_vector(word1)\n",
    "  v2 = ft_en_vectors.get_word_vector(word2)\n",
    "  return cosine_similarity(v1, v2)\n",
    "\n",
    "print(\"Similarity between the words King and Queen: \", getSimilarity(\"king\", \"queen\"))\n",
    "print(\"Similarity between the words King and Knight: \", getSimilarity(\"king\", \"knight\"))\n",
    "print(\"Similarity between the words King and Rock: \", getSimilarity(\"king\", \"rock\"))\n",
    "print(\"Similarity between the words King and Twenty: \", getSimilarity(\"king\", \"twenty\"))\n",
    "\n",
    "## Try the same for two more pairs\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Homonym Words$^\\dagger$\n",
    "\n",
    "Find the similarity for homonym words with their different meanings. The first one has been implemented for you.\n",
    "\n",
    "$^\\dagger$: Two or more words having the same spelling or pronunciation but different meanings and origins are called *homonyms*. E.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#######################     Words with multiple meanings     ##########################\n",
    "print(\"Similarity between the words Cricket and Insect: \", getSimilarity(\"cricket\", \"insect\"))\n",
    "print(\"Similarity between the words Cricket and Sport: \", getSimilarity(\"cricket\", \"sport\"))\n",
    "\n",
    "## Try the same for two more pairs\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))\n",
    "# print(\"Similarity between the words ___ and ___: \", getSimilarity(...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Word Analogies\n",
    "\n",
    "Embeddings can be used to find word analogies.\n",
    "Let's try it:\n",
    "1. Man : Woman  ::  King : _____\n",
    "2. Germany: Berlin :: France : ______\n",
    "3. Leaf : Tree  ::  Petal : _____\n",
    "4. Hammer : Nail  ::  Comb : _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Use get_analogies() funnction. The words have to be in the order Positive, Negative, Positve\n",
    "\n",
    "# Man : Woman  ::  King : _____\n",
    "# Positive=(woman, king), Negative=(man)\n",
    "print(ft_en_vectors.get_analogies(\"woman\", \"man\", \"king\",1))\n",
    "\n",
    "# Germany: Berlin :: France : ______\n",
    "# Positive=(berlin, frannce), Negative=(germany)\n",
    "print(ft_en_vectors.get_analogies(\"berlin\", \"germany\", \"france\",1))\n",
    "\n",
    "# Leaf : Tree  ::  Petal : _____\n",
    "# Positive=(tree, petal), Negative=(leaf)\n",
    "print(ft_en_vectors.get_analogies(\"tree\", \"leaf\", \"petal\",1))\n",
    "\n",
    "# Hammer : Nail  ::  Comb : _____\n",
    "# Positive=(nail, comb), Negative=(hammer)\n",
    "print(ft_en_vectors.get_analogies(\"nail\", \"hammer\", \"comb\",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "But, does it always work?\n",
    "\n",
    "\n",
    "1.   Poverty : Wealth  :: Sickness : _____\n",
    "2.   train : board :: horse : _____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Poverty : Wealth  :: Sickness : _____\n",
    "print(ft_en_vectors.get_analogies(\"wealth\", \"poverty\", \"sickness\",1))\n",
    "\n",
    "# train : board :: horse : _____\n",
    "print(ft_en_vectors.get_analogies(\"train\", \"board\", \"horse\",1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Neural Net with word embeddings\n",
    "\n",
    "*Time estimate: ~16mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's use the pretrained FastText embeddings to train a neural network on the IMDB dataset. \n",
    "\n",
    "To recap, the data consists of reviews and sentiments attached to it. It is a binary classification task. As a simple preview of the upcoming neural networks, we are going to introduce neural net with word embeddings. We'll see detailed networks in the next tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 3.1: Simple Feed Forward Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This will load 300 dim FastText embeddings. It will take around 2-3 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Define a vanilla neural network with linear layers. Then average the word embeddings to get an embedding for the entire review.\n",
    "The neural net will have one hidden layer of size 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download embeddings and clear old variables to clean memory.\n",
    "# @markdown #### Execute this cell!\n",
    "if 'ft_en_vectors' in locals():\n",
    "  del ft_en_vectors\n",
    "if 'w2vmodel' in locals():\n",
    "  del w2vmodel\n",
    "\n",
    "embedding_fasttext = FastText('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Load the Dataset\n",
    "TEXT, vocab_size, train_iter, valid_iter, test_iter = load_dataset(embedding_fasttext, seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Neural Network with following structure:\n",
    "  nn.Embedding(vocab_size, embedding_length)\n",
    "  + nn.Parameter(word_embeddings, requires_grad=False) # Embedding Layer\n",
    "  nn.Linear(embedding_length, hidden_size) # Fully connected layer #1\n",
    "  nn.Linear(hidden_size, output_size) # Fully connected layer #2\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, output_size, hidden_size, vocab_size, embedding_length,\n",
    "               word_embeddings):\n",
    "    \"\"\"\n",
    "    Initialize parameters of NeuralNet\n",
    "\n",
    "    Args:\n",
    "      output_size: int\n",
    "        Size of final fully connected layer\n",
    "      hidden_size: int\n",
    "        Size of hidden/first fully connected layer\n",
    "      vocab_size: int\n",
    "        Size of vocabulary\n",
    "      embedding_length: int\n",
    "        Length of embedding\n",
    "      word_embeddings: TEXT.vocab.vectors instance\n",
    "        Word Embeddings\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(NeuralNet, self).__init__()\n",
    "\n",
    "    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "    self.word_embeddings.weight = nn.Parameter(word_embeddings,\n",
    "                                               requires_grad=False)\n",
    "    self.fc1 = nn.Linear(embedding_length, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    \"\"\"\n",
    "    Forward pass of NeuralNet\n",
    "\n",
    "    Args:\n",
    "      Inputs: list\n",
    "        Text\n",
    "\n",
    "    Returns:\n",
    "      output: torch.tensor\n",
    "        Outputs/Predictions\n",
    "    \"\"\"\n",
    "    input = self.word_embeddings(inputs)  # Convert text to embeddings\n",
    "    ####################################################################\n",
    "    # Fill in missing code below (...)\n",
    "    raise NotImplementedError(\"Fill in the Neural Net\")\n",
    "    ####################################################################\n",
    "    # Average the word embeddings in a sentence\n",
    "    # Use torch.nn.functional.avg_pool2d to compute the averages\n",
    "    pooled = ...\n",
    "\n",
    "    # Pass the embeddings through the neural net\n",
    "    # A fully-connected layer\n",
    "    x = ...\n",
    "    # ReLU activation\n",
    "    x = ...\n",
    "    # Another fully-connected layer\n",
    "    x = ...\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 3.1: Simple Feed Forward Net')\n",
    "\n",
    "# Uncomment to check your code\n",
    "# nn_model = NeuralNet(2, 128, 100, 300, TEXT.vocab.vectors)\n",
    "# print(nn_model)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class NeuralNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Neural Network with following structure:\n",
    "  nn.Embedding(vocab_size, embedding_length)\n",
    "  + nn.Parameter(word_embeddings, requires_grad=False) # Embedding Layer\n",
    "  nn.Linear(embedding_length, hidden_size) # Fully connected layer #1\n",
    "  nn.Linear(hidden_size, output_size) # Fully connected layer #2\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, output_size, hidden_size, vocab_size, embedding_length,\n",
    "               word_embeddings):\n",
    "    \"\"\"\n",
    "    Initialize parameters of NeuralNet\n",
    "\n",
    "    Args:\n",
    "      output_size: int\n",
    "        Size of final fully connected layer\n",
    "      hidden_size: int\n",
    "        Size of hidden/first fully connected layer\n",
    "      vocab_size: int\n",
    "        Size of vocabulary\n",
    "      embedding_length: int\n",
    "        Length of embedding\n",
    "      word_embeddings: TEXT.vocab.vectors instance\n",
    "        Word Embeddings\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(NeuralNet, self).__init__()\n",
    "\n",
    "    self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "    self.word_embeddings.weight = nn.Parameter(word_embeddings,\n",
    "                                               requires_grad=False)\n",
    "    self.fc1 = nn.Linear(embedding_length, hidden_size)\n",
    "    self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    \"\"\"\n",
    "    Forward pass of NeuralNet\n",
    "\n",
    "    Args:\n",
    "      Inputs: list\n",
    "        Text\n",
    "\n",
    "    Returns:\n",
    "      output: torch.tensor\n",
    "        Outputs/Predictions\n",
    "    \"\"\"\n",
    "    input = self.word_embeddings(inputs)  # Convert text to embeddings\n",
    "    # Average the word embeddings in a sentence\n",
    "    # Use torch.nn.functional.avg_pool2d to compute the averages\n",
    "    pooled = F.avg_pool2d(input, (input.shape[1], 1)).squeeze(1)\n",
    "\n",
    "    # Pass the embeddings through the neural net\n",
    "    # A fully-connected layer\n",
    "    x = self.fc1(pooled)\n",
    "\n",
    "    # ReLU activation\n",
    "    x = F.relu(x)\n",
    "    # Another fully-connected layer\n",
    "    x = self.fc2(x)\n",
    "    output = F.log_softmax(x, dim=1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 3.1: Simple Feed Forward Net')\n",
    "\n",
    "# Uncomment to check your code\n",
    "nn_model = NeuralNet(2, 128, 100, 300, TEXT.vocab.vectors)\n",
    "print(nn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "NeuralNet(\n",
    "  (word_embeddings): Embedding(100, 300)\n",
    "  (fc1): Linear(in_features=300, out_features=128, bias=True)\n",
    "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Training and Testing Functions\n",
    "\n",
    "# @markdown #### `train(model, device, train_iter, valid_iter, epochs, learning_rate)`\n",
    "# @markdown #### `test(model, device, test_iter)`\n",
    "\n",
    "def train(model, device, train_iter, valid_iter,\n",
    "          epochs, learning_rate):\n",
    "  \"\"\"\n",
    "  Training pass\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      NeuralNet instance\n",
    "    device: string\n",
    "      GPU if available, CPU otherwise\n",
    "    epochs: int\n",
    "      Number of epochs to train model for\n",
    "    learning_rate: float\n",
    "      Learning rate\n",
    "    train_iter: BucketIterator\n",
    "      Training iterator\n",
    "    valid_iter: BucketIterator\n",
    "      Validation iterator\n",
    "\n",
    "  Returns:\n",
    "    train_loss: list\n",
    "      Log of training loss\n",
    "    validation_loss: list\n",
    "      Log of validation loss\n",
    "    train_acc: list\n",
    "      Log of training accuracy\n",
    "    validation_acc: list\n",
    "      Log of validation accuracy\n",
    "  \"\"\"\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    # train\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "    steps = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "      text = batch.text[0]\n",
    "      target = batch.label\n",
    "      target = torch.autograd.Variable(target).long()\n",
    "      text, target = text.to(device), target.to(device)\n",
    "\n",
    "      # Add micro for coding training loop\n",
    "      optimizer.zero_grad()\n",
    "      output = model(text)\n",
    "      loss = criterion(output, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      steps += 1\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # Get accuracy\n",
    "\n",
    "      # To get predicted values for each row\n",
    "      # across the entire batch\n",
    "      _, predicted = torch.max(output, 1)\n",
    "\n",
    "      total += target.size(0)\n",
    "      correct += (predicted == target).sum().item()\n",
    "    train_loss.append(running_loss/len(train_iter))\n",
    "    train_acc.append(correct/total)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1}, '\n",
    "          f'Training Loss: {running_loss/len(train_iter):.4f}, '\n",
    "          f'Training Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "    # Evaluate on validation data\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for idx, batch in enumerate(valid_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        text, target = text.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    validation_loss.append(running_loss/len(valid_iter))\n",
    "    validation_acc.append(correct/total)\n",
    "\n",
    "    print (f'Validation Loss: {running_loss/len(valid_iter):.4f}, '\n",
    "           f'Validation Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc\n",
    "\n",
    "\n",
    "def test(model, device, test_iter):\n",
    "  \"\"\"\n",
    "  Test loop\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      NeuralNet instance\n",
    "    device: string\n",
    "      GPU if available,\n",
    "    test_iter: BucketIterator\n",
    "      Test iterator\n",
    "\n",
    "  Returns:\n",
    "    acc: float\n",
    "      Test Accuracy\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  with torch.no_grad():\n",
    "    for idx, batch in enumerate(test_iter):\n",
    "      text = batch.text[0]\n",
    "      target = batch.label\n",
    "      target = torch.autograd.Variable(target).long()\n",
    "      text, target = text.to(device), target.to(device)\n",
    "\n",
    "      outputs = model(text)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      total += target.size(0)\n",
    "      correct += (predicted == target).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "learning_rate = 0.0003\n",
    "output_size = 2\n",
    "hidden_size = 128\n",
    "embedding_length = 300\n",
    "epochs = 15\n",
    "word_embeddings = TEXT.vocab.vectors\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "# Model set-up\n",
    "nn_model = NeuralNet(output_size,\n",
    "                     hidden_size,\n",
    "                     vocab_size,\n",
    "                     embedding_length,\n",
    "                     word_embeddings)\n",
    "nn_model.to(DEVICE)\n",
    "nn_start_time = time.time()\n",
    "set_seed(522)\n",
    "nn_train_loss, nn_train_acc, nn_validation_loss, nn_validation_acc = train(nn_model,\n",
    "                                                                           DEVICE,\n",
    "                                                                           train_iter,\n",
    "                                                                           valid_iter,\n",
    "                                                                           epochs,\n",
    "                                                                           learning_rate)\n",
    "print(f\"--- Time taken to train = {(time.time() - nn_start_time)} seconds ---\")\n",
    "test_accuracy = test(nn_model, DEVICE, test_iter)\n",
    "print(f'\\n\\nTest Accuracy: {test_accuracy}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Plot accuracy curves\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "plot_train_val(np.arange(0, epochs), nn_train_acc, nn_validation_acc,\n",
    "               'train accuracy', 'val accuracy',\n",
    "               'Neural Net on IMDB text classification', 'accuracy',\n",
    "               color='C0')\n",
    "plt.legend(loc='upper left')\n",
    "plt.subplot(212)\n",
    "plot_train_val(np.arange(0, epochs), nn_train_loss,\n",
    "               nn_validation_loss,\n",
    "               'train loss', 'val loss',\n",
    "               '',\n",
    "               'loss [a.u.]',\n",
    "               color='C0')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, we explored two different concepts linked to sequences, and text in particular, that will be the conceptual foundation for Recurrent Neural Networks.\n",
    "\n",
    "The first concept was that of sequences and probabilities. We saw how we can model language as sequences of text, and use this analogy to generate text. Such a setup is also used to classify text or identify parts of speech. We can either build chains manually using simple python and numerical computation, or use a package such as ```hmmlearn``` that allows us to train models a lot easier. These notions of sequences and probabilities (i.e, creating language models!) are key to the internals of a recurrent neural network as well. \n",
    "\n",
    "The second concept is that of word embeddings, now a mainstay of natural language processing. By using a neural network to predict context of words, these neural networks learn internal representions of words that are a decent approximation of semantic meaning (i.e embeddings!). We saw how these embeddings can be visualised, as well as how they capture meaning. We finally saw how they can be integrated into neural networks to better classify text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Airtable Submission Link\n",
    "from IPython import display as IPydisplay\n",
    "IPydisplay.HTML(\n",
    "   f\"\"\"\n",
    " <div>\n",
    "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
    "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/AirtableSubmissionButton.png?raw=1\"\n",
    " alt=\"button link to Airtable\" style=\"width:410px\"></a>\n",
    "   </div>\"\"\" )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "W2D3_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
