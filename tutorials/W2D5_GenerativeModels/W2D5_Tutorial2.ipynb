{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D5_GenerativeModels/W2D5_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 2: Introduction to GANs and Density Ratio Estimation Perspective of GANs\n",
    "\n",
    "**Week 2, Day 5: Generative Models**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Kai Xu, Seungwook Han, Akash Srivastava\n",
    "\n",
    "__Content reviewers:__ Polina Turishcheva, Melvin Selim Atay, Hadi Vafaei, Deepak Raya, Charles J. Edelson\n",
    "\n",
    "__Content editors:__ Charles J. Edelson, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Arush Tagade, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "## Tutorial Objectives\n",
    "\n",
    "The goal of this tutorial is two-fold; first you will be introduced to GANs training, and you will be able to understand how GANs are connected to other generative models that we have been before. \n",
    "\n",
    "By the end of the first part of this tutorial you will be able to:\n",
    "- Understand, at a high level, how GANs are implemented.\n",
    "- Understand the training dynamics of GANs. \n",
    "- Know about a few failure modes of GAN training.\n",
    "- Understand density ratio estimation using a binary classifier\n",
    "- Understand the connection between GANs and other generative models.\n",
    "- Implement a GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in this tutorial\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/dftym/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "ld_true = [-7.0066e-01, -2.6368e-01, -2.4250e+00, -2.0247e+00, -1.1795e+00,\n",
    "        -4.5558e-01, -7.1316e-01, -1.0932e-01, -7.8608e-01, -4.5838e-01,\n",
    "        -1.0530e+00, -9.1201e-01, -3.8020e+00, -1.7787e+00, -1.2246e+00,\n",
    "        -6.5677e-01, -3.6001e-01, -2.2313e-01, -1.8262e+00, -1.2649e+00,\n",
    "        -3.8330e-01, -8.8619e-02, -9.2357e-01, -1.3450e-01, -8.6891e-01,\n",
    "        -5.9257e-01, -4.8415e-02, -3.3197e+00, -1.6862e+00, -9.8506e-01,\n",
    "        -1.1871e+00, -7.0422e-02, -1.7378e+00, -1.3099e+00, -1.8926e+00,\n",
    "        -3.4508e+00, -1.5696e+00, -7.2787e-02, -3.2420e-01, -2.9795e-01,\n",
    "        -6.4189e-01, -1.4120e+00, -5.3684e-01, -3.4066e+00, -1.9753e+00,\n",
    "        -1.4178e+00, -2.0399e-01, -2.3173e-01, -1.2792e+00, -7.2990e-01,\n",
    "        -1.9872e-01, -2.9378e-03, -3.5890e-01, -5.6643e-01, -1.8003e-01,\n",
    "        -1.5818e+00, -5.2227e-01, -2.1862e+00, -1.8743e+00, -1.4200e+00,\n",
    "        -3.1988e-01, -3.5513e-01, -1.5905e+00, -4.2916e-01, -2.5556e-01,\n",
    "        -8.2807e-01, -6.5568e-01, -4.8475e-01, -2.1049e-01, -2.0104e-02,\n",
    "        -2.1655e+00, -1.1496e+00, -3.6168e-01, -8.9624e-02, -6.7098e-02,\n",
    "        -6.0623e-02, -5.1165e-01, -2.7302e+00, -6.0514e-01, -1.6756e+00,\n",
    "        -3.3807e+00, -5.7368e-02, -1.2763e-01, -6.6959e+00, -5.2157e-01,\n",
    "        -8.7762e-01, -8.7295e-01, -1.3052e+00, -3.6777e-01, -1.5904e+00,\n",
    "        -3.8083e-01, -2.8388e-01, -1.5323e-01, -3.7549e-01, -5.2722e+00,\n",
    "        -1.7393e+00, -2.8814e-01, -5.0310e-01, -2.2077e+00, -1.5507e+00,\n",
    "        -6.8569e-01, -1.4620e+00, -9.2639e-02, -1.4160e-01, -3.6734e-01,\n",
    "        -1.0053e+00, -6.7353e-01, -2.2676e+00, -6.0812e-01, -1.0005e+00,\n",
    "        -4.2908e-01, -5.1369e-01, -2.2579e-02, -1.8496e-01, -3.4798e-01,\n",
    "        -7.3089e-01, -1.1962e+00, -1.6095e+00, -1.7558e-01, -3.3166e-01,\n",
    "        -1.1445e+00, -2.4674e+00, -5.0600e-01, -2.0727e+00, -5.4371e-01,\n",
    "        -8.0499e-01, -3.0521e+00, -3.6835e-02, -2.0485e-01, -4.6747e-01,\n",
    "        -3.6399e-01, -2.6883e+00, -1.9348e-01, -3.1448e-01, -1.6332e-01,\n",
    "        -3.2233e-02, -2.3336e-01, -2.6564e+00, -1.2841e+00, -1.3561e+00,\n",
    "        -7.4717e-01, -2.7926e-01, -8.7849e-01, -3.3715e-02, -1.4933e-01,\n",
    "        -2.7738e-01, -1.6899e+00, -1.5758e+00, -3.2608e-01, -6.5770e-01,\n",
    "        -1.7136e+00, -5.8316e+00, -1.1988e+00, -8.3828e-01, -1.8033e+00,\n",
    "        -2.3017e-01, -8.9936e-01, -1.1917e-01, -1.6659e-01, -2.7669e-01,\n",
    "        -1.2955e+00, -1.2076e+00, -2.2793e-01, -1.0528e+00, -1.4894e+00,\n",
    "        -5.7428e-01, -7.3208e-01, -9.5673e-01, -1.6617e+00, -3.9169e+00,\n",
    "        -1.2182e-01, -3.8092e-01, -1.1924e+00, -2.4566e+00, -2.7350e+00,\n",
    "        -2.8332e+00, -9.1506e-01, -6.7432e-02, -7.8965e-01, -2.0727e-01,\n",
    "        -3.4615e-02, -2.8868e+00, -2.1218e+00, -1.2368e-03, -9.0038e-01,\n",
    "        -5.3746e-01, -5.4080e-01, -3.1625e-01, -1.1786e+00, -2.2797e-01,\n",
    "        -1.1498e+00, -1.3978e+00, -1.9515e+00, -1.1614e+00, -5.1456e-03,\n",
    "        -1.9316e-01, -1.3849e+00, -9.2799e-01, -1.1649e-01, -2.3837e-01]\n",
    "\n",
    "\n",
    "def plotting_ld(ld, true=ld_true):\n",
    "  fig, ax = plt.subplots(figsize=(7, 7))\n",
    "  ax.plot([-6, 1], [-6, 1], label=\"Ground Truth\")\n",
    "  ax.scatter(true, ld, marker=\"x\",\n",
    "             label=\"Your implementation\")\n",
    "  ax.set_xlabel(\"Loss from oracle implementation\")\n",
    "  ax.set_ylabel(\"Loss from your implementation\")\n",
    "  ax.legend()\n",
    "  ax.set_title(\"Discriminator Loss\")\n",
    "\n",
    "\n",
    "lg_true = [-7.0066e-01, -2.6368e-01, -2.4250e+00, -2.0247e+00, -1.1795e+00,\n",
    "        -4.5558e-01, -7.1316e-01, -1.0932e-01, -7.8608e-01, -4.5838e-01,\n",
    "        -1.0530e+00, -9.1201e-01, -3.8020e+00, -1.7787e+00, -1.2246e+00,\n",
    "        -6.5677e-01, -3.6001e-01, -2.2313e-01, -1.8262e+00, -1.2649e+00,\n",
    "        -3.8330e-01, -8.8619e-02, -9.2357e-01, -1.3450e-01, -8.6891e-01,\n",
    "        -5.9257e-01, -4.8415e-02, -3.3197e+00, -1.6862e+00, -9.8506e-01,\n",
    "        -1.1871e+00, -7.0422e-02, -1.7378e+00, -1.3099e+00, -1.8926e+00,\n",
    "        -3.4508e+00, -1.5696e+00, -7.2787e-02, -3.2420e-01, -2.9795e-01,\n",
    "        -6.4189e-01, -1.4120e+00, -5.3684e-01, -3.4066e+00, -1.9753e+00,\n",
    "        -1.4178e+00, -2.0399e-01, -2.3173e-01, -1.2792e+00, -7.2990e-01,\n",
    "        -1.9872e-01, -2.9378e-03, -3.5890e-01, -5.6643e-01, -1.8003e-01,\n",
    "        -1.5818e+00, -5.2227e-01, -2.1862e+00, -1.8743e+00, -1.4200e+00,\n",
    "        -3.1988e-01, -3.5513e-01, -1.5905e+00, -4.2916e-01, -2.5556e-01,\n",
    "        -8.2807e-01, -6.5568e-01, -4.8475e-01, -2.1049e-01, -2.0104e-02,\n",
    "        -2.1655e+00, -1.1496e+00, -3.6168e-01, -8.9624e-02, -6.7098e-02,\n",
    "        -6.0623e-02, -5.1165e-01, -2.7302e+00, -6.0514e-01, -1.6756e+00,\n",
    "        -3.3807e+00, -5.7368e-02, -1.2763e-01, -6.6959e+00, -5.2157e-01,\n",
    "        -8.7762e-01, -8.7295e-01, -1.3052e+00, -3.6777e-01, -1.5904e+00,\n",
    "        -3.8083e-01, -2.8388e-01, -1.5323e-01, -3.7549e-01, -5.2722e+00,\n",
    "        -1.7393e+00, -2.8814e-01, -5.0310e-01, -2.2077e+00, -1.5507e+00]\n",
    "\n",
    "\n",
    "def plotting_lg(lg, true=lg_true):\n",
    "  fig, ax = plt.subplots(figsize=(7, 7))\n",
    "  ax.plot([-6, 1], [-6, 1], label=\"Ground Truth\")\n",
    "  ax.scatter(true, lg, marker=\"x\",\n",
    "             label=\"Your implementation\")\n",
    "  ax.set_xlabel(\"Loss from oracle implementation\")\n",
    "  ax.set_ylabel(\"Loss from your implementation\")\n",
    "  ax.legend()\n",
    "  ax.set_title(\"Generator loss\")\n",
    "\n",
    "\n",
    "def plotting_ratio_impl(ax, x_real, x_fake, ratio, yscale=\"linear\"):\n",
    "  dist_p = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "  dist_q = torch.distributions.normal.Normal(loc=-2, scale=1)\n",
    "  x = torch.linspace(-3, 5, 100)\n",
    "  prob_p = torch.exp(dist_p.log_prob(x))\n",
    "  prob_q = torch.exp(dist_q.log_prob(x))\n",
    "  trueRatio = prob_p / prob_q\n",
    "  ax.plot(x, trueRatio, label=\"True tatio\")\n",
    "\n",
    "  x = torch.cat([x_real, x_fake])\n",
    "  ax.scatter(x[:,0][::10], ratio[:,0][::10], marker=\"x\",\n",
    "             label=\"Ratio from discriminator\")\n",
    "  ax.hist(x_real[:,0], density=True, bins=50, histtype=\"step\", label=\"Real\")\n",
    "  ax.hist(x_fake[:,0], density=True, bins=50, histtype=\"step\", label=\"Fake\")\n",
    "  ax.set_yscale(yscale)\n",
    "  title = \"Densities and the ratio from discriminator\"\n",
    "  if yscale == \"log\":\n",
    "    title += \" in log scale\"\n",
    "  ax.set_title(title)\n",
    "  ax.legend()\n",
    "\n",
    "\n",
    "def plotting_ratio(x_real, x_fake, ratio):\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(2 * 7, 7))\n",
    "  plotting_ratio_impl(axes[0], x_real, x_fake, ratio, yscale=\"linear\")\n",
    "  plotting_ratio_impl(axes[1], x_real, x_fake, ratio, yscale=\"log\")\n",
    "\n",
    "\n",
    "class Interactive:\n",
    "  def display_widgets(self):\n",
    "    for widget in self.widgets:\n",
    "      display(widget)\n",
    "\n",
    "  def __init__(self, widgets, handler):\n",
    "    def handler_with_extra_steps(b):\n",
    "      clear_output(wait=True)\n",
    "      handler(*map(lambda w: w.value, widgets))\n",
    "      self.display_widgets()\n",
    "    self.widgets = widgets\n",
    "    for widget in self.widgets:\n",
    "      widget.observe(handler_with_extra_steps,\n",
    "                     names=['value'])\n",
    "    handler(*map(lambda w: w.value, widgets))\n",
    "    self.display_widgets()\n",
    "\n",
    "\n",
    "# Using Interactive\n",
    "# All widgets: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html\n",
    "\n",
    "\n",
    "def make_plot(xmax, title, ftype):\n",
    "  fig, ax = plt.subplots()\n",
    "  x = np.linspace(-2, xmax, 100)\n",
    "  if ftype == \"sin\":\n",
    "    y = np.sin(x)\n",
    "  if ftype == \"cos\":\n",
    "    y = np.cos(x)\n",
    "  if ftype == \"tanh\":\n",
    "    y = np.tanh(x)\n",
    "  ax.scatter(x, y)\n",
    "  ax.set_xlim(-2.1, 2.1)\n",
    "  ax.set_ylim(-2, 2)\n",
    "  if title:\n",
    "    ax.set_title(f\"Range from -1 to {xmax}\")\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: How to train GANs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Generative Adversarial Networks\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV19V411H72M\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"fCGnJPW2RKM\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "GANs consist two networks: A critic or discriminator (`disc`) and a generator (`gen`) that are trained by alternating between the following two steps:\n",
    "- In step 1, we update the parameters (`disc.params`) of the discriminator by backpropagating through the discriminator loss (BCE loss) `disc.loss`.\n",
    "- In step 2, we update the parameters (`gen.params`) of the generator by backpropagating through the generator loss, `gen.loss` (-1 * BCE loss).\n",
    "\n",
    "We will now implement a simple GAN training loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 1: The GAN training loop\n",
    "\n",
    "To get you started we have implemented a simple GAN in pseudocode. All you have to do is to implement the training loop.\n",
    "\n",
    "__Your goal__ is to arrange the functions given below in the correct order in the `train_gan_iter` function\n",
    "- `disc.loss(x_real, x_fake)`: Discriminator loss\n",
    "- `disc.classify(x)`: Classify `x` as real or fake\n",
    "- `gen.loss(x_fake, disc_fn)`: Generator loss\n",
    "- `disc_fn(x)` is a function to check `x` is real or fake.\n",
    "- `gen.sample(num_samples)`: Generate samples from the generator\n",
    "- `backprop(loss, model)`: Compute gradient of `loss` wrt `model`\n",
    "- `model` is either `disc` or `gen`\n",
    "\n",
    "We have already taken care of most of these functions. So you only have to figure out the placement of `disc.loss` and `gen.loss` functions.\n",
    "\n",
    "__We highly recommend studying `train_gan_iter` function to understand how the GAN training loop is structured.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown *Execute this cell to enable helper functions*\n",
    "\n",
    "def get_data():\n",
    "  return \"get_data\"\n",
    "\n",
    "\n",
    "class Disc:\n",
    "\n",
    "  def loss(self, x_real, x_fake):\n",
    "    assert x_real == \"get_data\" and x_fake == \"gen.sample\",\"Inputs to disc.loss is wrong\"\n",
    "\n",
    "  def classify(self, x):\n",
    "    return \"disc.classify\"\n",
    "\n",
    "\n",
    "class Gen:\n",
    "\n",
    "  def loss(self, x_fake, disc_fn):\n",
    "    assert x_fake == \"gen.sample\" and disc_fn(None) == \"disc.classify\", \"Inputs to gen.loss is wrong\"\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    return \"gen.sample\"\n",
    "\n",
    "\n",
    "def backprop(loss, model):\n",
    "  pass\n",
    "\n",
    "\n",
    "def update(model, grad):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def train_gan_iter(data, disc, gen):\n",
    "  \"\"\"Update the discriminator (`disc`) and the generator (`gen`) using `data`\n",
    "\n",
    "  Args:\n",
    "    data (ndarray): An array of shape (N,) that contains the data\n",
    "    disc (Disc): The discriminator\n",
    "    gen (Gen): The generator\n",
    "\n",
    "  Returns:\n",
    "  \"\"\"\n",
    "  #################################################\n",
    "  # Intructions for students:                        #\n",
    "  # Fill out ... in the function and remove below #\n",
    "  #################################################\n",
    "\n",
    "  # Number of samples in the data batch\n",
    "  num_samples = 200\n",
    "\n",
    "  # The data is the real samples\n",
    "  x_real = data\n",
    "\n",
    "  ## Discriminator training\n",
    "\n",
    "  # Ask the generator to generate some fake samples\n",
    "  x_fake = gen.sample(num_samples)\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: details of what they should do ##\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student exercise: Write code to compute disc_loss\")\n",
    "  #################################################\n",
    "  # Compute the discriminator loss\n",
    "  disc_loss = ...\n",
    "\n",
    "  # Compute the gradient for discriminator\n",
    "  disc_grad = backprop(disc_loss, disc)\n",
    "\n",
    "  # Update the discriminator\n",
    "  update(disc, disc_grad)\n",
    "\n",
    "  ## Generator training\n",
    "\n",
    "  # Ask the generator to generate some fake samples\n",
    "  x_fake = gen.sample(num_samples)\n",
    "\n",
    "  #################################################\n",
    "  ## TODO for students: details of what they should do ##\n",
    "  # Fill out function and remove\n",
    "  raise NotImplementedError(\"Student exercise: Write code to compute gen_loss\")\n",
    "  #################################################\n",
    "  # Compute the generator loss\n",
    "  gen_loss = ...\n",
    "\n",
    "  # Compute the gradient for generator\n",
    "  gen_grad = backprop(gen_loss, gen)\n",
    "\n",
    "  # Update the generator\n",
    "  update(gen, gen_grad)\n",
    "\n",
    "  print(\"Your implementation passes the check!\")\n",
    "\n",
    "\n",
    "data = get_data()\n",
    "disc = Disc()\n",
    "gen = Gen()\n",
    "## Uncomment below to check your function\n",
    "# train_gan_iter(data, disc, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def train_gan_iter(data, disc, gen):\n",
    "  \"\"\"Update the discriminator (`disc`) and the generator (`gen`) using `data`\n",
    "\n",
    "  Args:\n",
    "    data (ndarray): An array of shape (N,) that contains the data\n",
    "    disc (Disc): The discriminator\n",
    "    gen (Gen): The generator\n",
    "\n",
    "  Returns:\n",
    "  \"\"\"\n",
    "\n",
    "  # Number of samples in the data batch\n",
    "  num_samples = 200\n",
    "\n",
    "  # The data is the real samples\n",
    "  x_real = data\n",
    "\n",
    "  ## Discriminator training\n",
    "\n",
    "  # Ask the generator to generate some fake samples\n",
    "  x_fake = gen.sample(num_samples)\n",
    "\n",
    "  # Compute the discriminator loss\n",
    "  disc_loss = disc.loss(x_real, x_fake)\n",
    "\n",
    "  # Compute the gradient for discriminator\n",
    "  disc_grad = backprop(disc_loss, disc)\n",
    "\n",
    "  # Update the discriminator\n",
    "  update(disc, disc_grad)\n",
    "\n",
    "  ## Generator training\n",
    "\n",
    "  # Ask the generator to generate some fake samples\n",
    "  x_fake = gen.sample(num_samples)\n",
    "\n",
    "  # Compute the generator loss\n",
    "  gen_loss = gen.loss(x_fake, disc.classify)\n",
    "\n",
    "  # Compute the gradient for generator\n",
    "  gen_grad = backprop(gen_loss, gen)\n",
    "\n",
    "  # Update the generator\n",
    "  update(gen, gen_grad)\n",
    "\n",
    "  print(\"Your implementation passes the check!\")\n",
    "\n",
    "\n",
    "data = get_data()\n",
    "disc = Disc()\n",
    "gen = Gen()\n",
    "## Uncomment below to check your function\n",
    "train_gan_iter(data, disc, gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: The difficulty of GAN training.\n",
    "\n",
    "In this section we will develop an intuition for the training dynamics of GANs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interactive Demo 2: Failure modes of GAN training\n",
    "\n",
    "GAN training is notoriously difficult because \n",
    "it is very sensitive to hyper-parameters such as learning rate and model architecture. To help you develop a sense of this here is a very simple GAN training demo that we have borrowed from [Andrej Karpathy's website](https://cs.stanford.edu/people/karpathy/gan/).  \n",
    "\n",
    "The generator $G$, pictured in red, takes inputs sampled from a uniform distribution, $z$. It attempts to transform these to match a data distribution, shown below in blue. Meanwhile, the discriminator $D$ attempts to determine whether a sample is from the data distribution or the generating distribution. In the demo, the green curve represents the output of the discriminator. Its value is high where the discriminator is more confident that a sample with that value is drawn from the data distribution.\n",
    "\n",
    "Even though the GAN in this demo is very simple and operates in either 1D or 2D spaces, it is however very sensitive to the learning rate. Try it for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title GAN training demo\n",
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://xukai92.github.io/gan_demo/index.html', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Exercise 2: What makes GANs hard to train?\n",
    "\n",
    "You have played with the demo and it's time to think about a few questions\n",
    "\n",
    "1. Which target is more stable to train, 1D or 2D?\n",
    "2. If you keep increasing the learning rate, what happens? Does it happen in both the cases, i.e., 1D/2D targets?\n",
    "3. Can you think of some drawbacks of using small learning rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "  1. 2D as it is simply a more difficult distribution to model compared to the\n",
    "     unimodal 1D example.\n",
    "    2. The training becomes unstable and eventually diverges. Yes.\n",
    "  3. Training is too slow and takes longer to convege.\n",
    "\n",
    "In general, when training a GAN we need to ensure a certain balance between\n",
    "the critic and generator training. As such, tuninig the learning rate\n",
    "is crucial for succesfully training GANs.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: GAN Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The training objective of GANs consists of the losses for generators and discriminators respectively. In this section we will be implementing these objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Principles of GANs\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Fq4y1s7pf\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"bTJfZro6A9Q\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.1:  Discriminator Loss\n",
    "\n",
    "The critic or the discriminator in a vanilla GAN is trained as a binary classifier using the BCE criteria. In this section, we will implement the training objective for the discriminator. \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{BCE}_\\omega = \\mathbb{E}_{x \\sim p}[\\log(\\sigma(D_\\omega(x)))] + \\mathbb{E}_{x \\sim q}[\\log(1 - \\sigma(D_\\omega(x)))]\n",
    "\\end{equation}\n",
    "\n",
    "Here, $p$ is the data distribution and $q$ is the generator distribution. $D_\\omega$ is the logit, which represents $\\log \\frac{p}{q}$. $\\sigma$ is the sigmoid function and therfore, $\\sigma(D_\\omega)$ represents $\\frac{p}{p+q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.1: Implement Discriminator Loss\n",
    "\n",
    "To get you started we have implemented a simple GAN in pseudocode and partially implemented the discriminator training objective.\n",
    "\n",
    "**Your goal** is to complete the missing part in the training objective of the discriminator in the function `loss_disc`.\n",
    "\n",
    "`loss_disc` also allows you evaluate the loss function on some random samples.\n",
    "If your implementation is correct, you will see a plot where the loss values from your implementation will match the ground truth loss values.\n",
    "\n",
    "In practice, given $N$ samples, we estimate BCE as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{BCE}_\\omega = -\\frac{1}{N} \\sum_{i=1}^N y_i \\log(\\sigma(D_\\omega(x_i)) + (1-y_i) \\log(1-\\sigma(D_\\omega(x_i))).\n",
    "\\end{equation}\n",
    "\n",
    "Here, $y$ is the label. $y=1$ when $x \\sim p$ (real data) and $y=0$ when $x \\sim q$ (i.e., fake data).\n",
    "\n",
    "Please note, `disc.classify` = $\\sigma(D_\\omega)$ in `loss_disc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown *Execute this cell to enable helper functions*\n",
    "\n",
    "def get_data(num_samples=100, seed=0):\n",
    "  set_seed(seed)\n",
    "  return torch.randn([num_samples, 1])\n",
    "\n",
    "\n",
    "class DummyGen:\n",
    "  def sample(self, num_samples=100, seed=1):\n",
    "    set_seed(seed)\n",
    "    return torch.randn([num_samples, 1]) + 2\n",
    "\n",
    "\n",
    "class DummyDisc:\n",
    "  def classify(self, x, seed=0):\n",
    "    set_seed(seed)\n",
    "    return torch.rand([x.shape[0], ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def loss_disc(disc, x_real, x_fake):\n",
    "  \"\"\"Compute the discriminator loss for `x_real` and `x_fake` given `disc`\n",
    "\n",
    "  Args:\n",
    "    disc: The discriminator\n",
    "    x_real (ndarray): An array of shape (N,) that contains the real samples\n",
    "    x_fake (ndarray): An array of shape (N,) that contains the fake samples\n",
    "\n",
    "  Returns:\n",
    "    ndarray: The discriminator loss\n",
    "  \"\"\"\n",
    "\n",
    "  label_real = 1\n",
    "  #################################################\n",
    "  # TODO for students: Loss for real data\n",
    "  raise NotImplementedError(\"Student exercise: Implement loss for real samples\")\n",
    "  #################################################\n",
    "  loss_real = label_real * ...\n",
    "\n",
    "  label_fake = 0\n",
    "  #################################################\n",
    "  # TODO for students: Loss for fake data\n",
    "  raise NotImplementedError(\"Student exercise: Implement loss for fake samples\")\n",
    "  #################################################\n",
    "  loss_fake = ... * torch.log(1 - disc.classify(x_fake))\n",
    "\n",
    "\n",
    "  return torch.cat([loss_real, loss_fake])\n",
    "\n",
    "\n",
    "disc = DummyDisc()\n",
    "gen = DummyGen()\n",
    "\n",
    "x_real = get_data()\n",
    "x_fake = gen.sample()\n",
    "\n",
    "## Uncomment to check your function\n",
    "# ld = loss_disc(disc, x_real, x_fake)\n",
    "# plotting_ld(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def loss_disc(disc, x_real, x_fake):\n",
    "  \"\"\"Compute the discriminator loss for `x_real` and `x_fake` given `disc`\n",
    "\n",
    "  Args:\n",
    "    disc: The discriminator\n",
    "    x_real (ndarray): An array of shape (N,) that contains the real samples\n",
    "    x_fake (ndarray): An array of shape (N,) that contains the fake samples\n",
    "\n",
    "  Returns:\n",
    "    ndarray: The discriminator loss\n",
    "  \"\"\"\n",
    "\n",
    "  # Loss for real data\n",
    "  label_real = 1\n",
    "  loss_real = label_real * torch.log(disc.classify(x_real))\n",
    "\n",
    "  # Loss for fake data\n",
    "  label_fake = 0\n",
    "  loss_fake = (1 - label_fake) * torch.log(1 - disc.classify(x_fake))\n",
    "\n",
    "  return torch.cat([loss_real, loss_fake])\n",
    "\n",
    "\n",
    "disc = DummyDisc()\n",
    "gen = DummyGen()\n",
    "\n",
    "x_real = get_data()\n",
    "x_fake = gen.sample()\n",
    "\n",
    "## Uncomment to check your function\n",
    "ld = loss_disc(disc, x_real, x_fake)\n",
    "with plt.xkcd():\n",
    "  plotting_ld(ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**A note on numerical stability**\n",
    "\n",
    "It is common that functions like $\\log$ throw a numerical error.\n",
    "For $\\log$, it happens when $x$ in $\\log(x)$ is very close to 0.\n",
    "The most common practice is to always add some very small value $\\epsilon$ to $x$, i.e. use $\\log(x + \\epsilon)$ instead.\n",
    "Most build-in functions in modern DL frameworks like TensorFlow or PyTorch handle such things in their build-in loss already, e.g., `torch.nn.BCE`, which is equivalent to the loss you implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.2:  Density ratio estimation and the optimal discriminator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As explained in the lecture, the critic in GAN that trains as a binary classifier, upon training becomes a *density ratio estimator* of $\\frac{p}{q}$. \n",
    "\n",
    "The estimated density ratio allows for quantifying how far the real and generator distributions are from each other using $f$-divergence. The generator minimizes this estimated $f$-divergence during training.\n",
    "\n",
    "We will now train a discriminator to see how it estimates the density ratio between two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.2: Estimating the density ratio by the discriminator\n",
    "\n",
    "We have provided an implementation of a binary critic in the class `OptimalDisc`.\n",
    "\n",
    "__Your goal__ is to complete the implementation of the function `ratio_disc` below using the function `classify` from `OptimalCritic` class. \n",
    "\n",
    "Upon correct implementation, you should see that the plot of the ratios from the optimal discriminator align to the true density ratio.\n",
    "\n",
    "__Remember, $\\frac{p}{p + q} = \\sigma(D_\\omega(x))$__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown *Execute this cell to enable the optimal discriminator: `OptimalDisc`*\n",
    "\n",
    "class OptimalDisc:\n",
    "  def classify(self, x):\n",
    "    dist_p = torch.distributions.normal.Normal(loc=0, scale=1)\n",
    "    dist_q = torch.distributions.normal.Normal(loc=-2, scale=1)\n",
    "    prob_p = torch.exp(dist_p.log_prob(x))\n",
    "    prob_q = torch.exp(dist_q.log_prob(x))\n",
    "\n",
    "    return prob_p / (prob_p + prob_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def ratio_disc(disc, x_real, x_fake):\n",
    "  \"\"\"Compute the density ratio between real distribution and fake distribution for `x`\n",
    "\n",
    "  Args:\n",
    "    disc: The discriminator\n",
    "    x (ndarray): An array of shape (N,) that contains the samples to evaluate\n",
    "\n",
    "  Returns:\n",
    "    ndarray: The density ratios\n",
    "  \"\"\"\n",
    "\n",
    "  # Put samples together\n",
    "  x = torch.cat([x_real, x_fake])\n",
    "\n",
    "  #################################################\n",
    "  # TODO for students: Compute p / (p + q) i.e. p(D=1|x)\n",
    "  raise NotImplementedError(\"Student exercise: Implement p_over_pplusq\")\n",
    "  #################################################\n",
    "  p_over_pplusq = ...\n",
    "\n",
    "  #################################################\n",
    "  # TODO for students: Compute q / (p + q) i.e. 1 - p(D=1|x)\n",
    "  raise NotImplementedError(\"Student exercise: Implement q_over_pplusq\")\n",
    "  #################################################\n",
    "  q_over_pplusq = ...\n",
    "\n",
    "  # Compute p / q\n",
    "  p_over_q = p_over_pplusq / q_over_pplusq\n",
    "\n",
    "  return p_over_q\n",
    "\n",
    "\n",
    "disc = OptimalDisc()\n",
    "gen = DummyGen()\n",
    "\n",
    "x_real = get_data(1_000)\n",
    "x_fake = gen.sample(1_000)\n",
    "\n",
    "## Uncomment below to check your function\n",
    "# ratio = ratio_disc(disc, x_real, x_fake)\n",
    "# plotting_ratio(x_real, x_fake, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def ratio_disc(disc, x_real, x_fake):\n",
    "  \"\"\"Compute the density ratio between real distribution and fake distribution for `x`\n",
    "\n",
    "  Args:\n",
    "    disc: The discriminator\n",
    "    x (ndarray): An array of shape (N,) that contains the samples to evaluate\n",
    "\n",
    "  Returns:\n",
    "    ndarray: The density ratios\n",
    "  \"\"\"\n",
    "\n",
    "  # Put samples together\n",
    "  x = torch.cat([x_real, x_fake])\n",
    "\n",
    "  # Compute p / (p + q)\n",
    "  p_over_pplusq = disc.classify(x)\n",
    "\n",
    "  # Compute q / (p + q)\n",
    "  q_over_pplusq = 1 - p_over_pplusq\n",
    "\n",
    "  # Compute p / q\n",
    "  p_over_q = p_over_pplusq / q_over_pplusq\n",
    "\n",
    "  return p_over_q\n",
    "\n",
    "\n",
    "disc = OptimalDisc()\n",
    "gen = DummyGen()\n",
    "\n",
    "x_real = get_data(1_000)\n",
    "x_fake = gen.sample(1_000)\n",
    "\n",
    "## Uncomment below to check your function\n",
    "ratio = ratio_disc(disc, x_real, x_fake)\n",
    "with plt.xkcd():\n",
    "  plotting_ratio(x_real, x_fake, ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.3:  The generator loss\n",
    "\n",
    "Now that we have a trained critic, lets see how to train the generator using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.3: The generator loss\n",
    "\n",
    "We will now implement the generator loss function and evaluate it on some fixed points.\n",
    "\n",
    "**Your goal** is to complete the implementation of the function `loss_gen` using the optimal critic from above.\n",
    "\n",
    "Upon correct implementation, you shall see a plot where the loss values from generator samples align with the \"Correct\" values.\n",
    "\n",
    "**HINT:** You simply need to change the labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def loss_gen(disc, x_fake):\n",
    "  \"\"\"Compute the generator loss for `x_fake` given `disc`\n",
    "\n",
    "  Args:\n",
    "    disc: The generator\n",
    "    x_fake (ndarray): An array of shape (N,) that contains the fake samples\n",
    "\n",
    "  Returns:\n",
    "    ndarray: The generator loss\n",
    "  \"\"\"\n",
    "\n",
    "  #################################################\n",
    "  # TODO for students: Loss for fake data\n",
    "  raise NotImplementedError(\"Student exercise: Implement loss for fake data\")\n",
    "  #################################################\n",
    "  label_fake = ...\n",
    "  loss_fake = label_fake * ...\n",
    "\n",
    "  return loss_fake\n",
    "\n",
    "\n",
    "disc = DummyDisc()\n",
    "gen = DummyGen()\n",
    "\n",
    "x_fake = gen.sample()\n",
    "## Uncomment below to check your function\n",
    "# lg = loss_gen(disc, x_fake)\n",
    "# plotting_lg(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def loss_gen(disc, x_fake):\n",
    "  \"\"\"Compute the generator loss for `x_fake` given `disc`\n",
    "\n",
    "  Args:\n",
    "    disc: The generator\n",
    "    x_fake (ndarray): An array of shape (N,) that contains the fake samples\n",
    "\n",
    "  Returns:\n",
    "    ndarray: The generator loss\n",
    "  \"\"\"\n",
    "\n",
    "  # Loss for fake data\n",
    "  label_fake = 1\n",
    "  loss_fake = label_fake * torch.log(disc.classify(x_fake))\n",
    "\n",
    "  return loss_fake\n",
    "\n",
    "\n",
    "disc = DummyDisc()\n",
    "gen = DummyGen()\n",
    "\n",
    "x_fake = gen.sample()\n",
    "## Uncomment below to check your function\n",
    "lg = loss_gen(disc, x_fake)\n",
    "with plt.xkcd():\n",
    "  plotting_lg(lg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Did you notice?**\n",
    "\n",
    "The loss you implemented for generator is essentially the part for real data in `loss_disc`, i.e., it is saying, \"*the data I am feeding to you is real and not fake*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: GAN training in action!\n",
    "\n",
    "In this section we will be playing with a complete implementation of GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interactive Demo 4: GAN training in action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown *Execute this cell to enable the implemented GAN*\n",
    "# @markdown\n",
    "# @markdown You are encouraged to take a look at the implementation as well.\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Generator(nn.Module):\n",
    "  def __init__(self, latent_dim, layers, output_activation=None):\n",
    "\n",
    "    super(Generator, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.output_activation = output_activation\n",
    "    self._init_layers(layers)\n",
    "\n",
    "  def _init_layers(self, layers):\n",
    "    \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
    "    self.module_list = nn.ModuleList()\n",
    "    last_layer = self.latent_dim\n",
    "    for index, width in enumerate(layers):\n",
    "      self.module_list.append(nn.Linear(last_layer, width))\n",
    "      last_layer = width\n",
    "      if index + 1 != len(layers):\n",
    "        self.module_list.append(nn.LeakyReLU())\n",
    "    else:\n",
    "      if self.output_activation is not None:\n",
    "        self.module_list.append(self.output_activation())\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward pass; map latent vectors to samples.\"\"\"\n",
    "    intermediate = input_tensor\n",
    "    for layer in self.module_list:\n",
    "      intermediate = layer(intermediate)\n",
    "    return intermediate\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, input_dim, layers):\n",
    "\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self._init_layers(layers)\n",
    "\n",
    "  def _init_layers(self, layers):\n",
    "    \"\"\"Initialize the layers and store as self.module_list.\"\"\"\n",
    "    self.module_list = nn.ModuleList()\n",
    "    last_layer = self.input_dim\n",
    "    for index, width in enumerate(layers):\n",
    "      self.module_list.append(nn.Linear(last_layer, width))\n",
    "      last_layer = width\n",
    "      if index + 1 != len(layers):\n",
    "        self.module_list.append(nn.LeakyReLU())\n",
    "    else:\n",
    "      self.module_list.append(nn.Sigmoid())\n",
    "\n",
    "  def forward(self, input_tensor):\n",
    "    \"\"\"Forward pass; map samples to confidence they are real [0, 1].\"\"\"\n",
    "    intermediate = input_tensor\n",
    "    for layer in self.module_list:\n",
    "      intermediate = layer(intermediate)\n",
    "    return intermediate\n",
    "\n",
    "\n",
    "class VanillaGAN():\n",
    "  def __init__(self, generator, discriminator, noise_fn, data_fn,\n",
    "                batch_size=100, device='cpu', lr_d=1e-3, lr_g=2e-4):\n",
    "\n",
    "    self.generator = generator\n",
    "    self.generator = self.generator.to(device)\n",
    "    self.discriminator = discriminator\n",
    "    self.discriminator = self.discriminator.to(device)\n",
    "    self.noise_fn = noise_fn\n",
    "    self.data_fn = data_fn\n",
    "    self.batch_size = batch_size\n",
    "    self.device = device\n",
    "    self.criterion = nn.BCELoss()\n",
    "    self.optim_d = optim.Adam(discriminator.parameters(),\n",
    "                              lr=lr_d, betas=(0.5, 0.999))\n",
    "    self.optim_g = optim.Adam(generator.parameters(),\n",
    "                              lr=lr_g, betas=(0.5, 0.999))\n",
    "    self.target_ones = torch.ones((batch_size, 1)).to(device)\n",
    "    self.target_zeros = torch.zeros((batch_size, 1)).to(device)\n",
    "\n",
    "  def generate_samples(self, latent_vec=None, num=None):\n",
    "    \"\"\"Sample from the generator.\n",
    "    \"\"\"\n",
    "    num = self.batch_size if num is None else num\n",
    "    latent_vec = self.noise_fn(num) if latent_vec is None else latent_vec\n",
    "    with torch.no_grad():\n",
    "      samples = self.generator(latent_vec)\n",
    "    return samples\n",
    "\n",
    "  def real_data(self, num=None):\n",
    "    \"\"\"Real Data\n",
    "    \"\"\"\n",
    "    num = self.batch_size if num is None else num\n",
    "    with torch.no_grad():\n",
    "      samples = self.data_fn(num)\n",
    "    return samples\n",
    "\n",
    "  def train_step_generator(self):\n",
    "    \"\"\"Train the generator one step and return the loss.\"\"\"\n",
    "    self.generator.zero_grad()\n",
    "\n",
    "    latent_vec = self.noise_fn(self.batch_size)\n",
    "    generated = self.generator(latent_vec)\n",
    "    classifications = self.discriminator(generated)\n",
    "    loss = self.criterion(classifications,\n",
    "                          self.target_ones)\n",
    "    loss.backward()\n",
    "    self.optim_g.step()\n",
    "    return loss.item()\n",
    "\n",
    "  def train_step_discriminator(self):\n",
    "    \"\"\"Train the discriminator one step and return the losses.\"\"\"\n",
    "    self.discriminator.zero_grad()\n",
    "\n",
    "    # real samples\n",
    "    real_samples = self.data_fn(self.batch_size)\n",
    "    pred_real = self.discriminator(real_samples)\n",
    "    loss_real = self.criterion(pred_real,\n",
    "                               self.target_ones)\n",
    "\n",
    "    # generated samples\n",
    "    latent_vec = self.noise_fn(self.batch_size)\n",
    "    with torch.no_grad():\n",
    "        fake_samples = self.generator(latent_vec)\n",
    "    pred_fake = self.discriminator(fake_samples)\n",
    "    loss_fake = self.criterion(pred_fake,\n",
    "                               self.target_zeros)\n",
    "\n",
    "    # combine\n",
    "    loss = (loss_real + loss_fake) / 2\n",
    "    loss.backward()\n",
    "    self.optim_d.step()\n",
    "    return loss_real.item(), loss_fake.item()\n",
    "\n",
    "  def train_step(self):\n",
    "    \"\"\"Train both networks and return the losses.\"\"\"\n",
    "    loss_d = self.train_step_discriminator()\n",
    "    loss_g = self.train_step_generator()\n",
    "    return loss_g, loss_d\n",
    "\n",
    "\n",
    "def train(mean=0., sigma=.2, device='cpu'):\n",
    "  epochs = 30\n",
    "  batches = 100\n",
    "  generator = Generator(2, [64, 32, 2])\n",
    "  discriminator = Discriminator(2, [64, 32, 1])\n",
    "  noise_fn = lambda x: torch.rand((x, 2), device=device)\n",
    "  data_fn = lambda x: mean + sigma*torch.randn((x, 2), device=device)\n",
    "  gan = VanillaGAN(generator,\n",
    "                   discriminator,\n",
    "                   noise_fn,\n",
    "                   data_fn,\n",
    "                   lr_d=1e-3,\n",
    "                   lr_g=2e-4,\n",
    "                   device=device)\n",
    "  loss_g, loss_d_real, loss_d_fake = [], [], []\n",
    "  start = time()\n",
    "\n",
    "  fig, ax = plt.subplots(1, 1,figsize=(5,5))\n",
    "  x, y = np.random.random((2, 200))\n",
    "  data_scat = ax.scatter(x, y,\n",
    "                         label='Data',\n",
    "                         alpha=0.9,\n",
    "                         s=10.,\n",
    "                         c='r')\n",
    "  gan_scat = ax.scatter(x,y,\n",
    "                        label='GAN',\n",
    "                        alpha=0.9,\n",
    "                        s=10.,\n",
    "                        c='b')\n",
    "  ax.legend()\n",
    "  ax.set_xlim(-2, 2)\n",
    "  ax.set_ylim(-2, 2)\n",
    "  for epoch in range(epochs):\n",
    "    loss_g_running = 0\n",
    "    loss_d_real_running = 0\n",
    "    loss_d_fake_running = 0\n",
    "    for batch in range(batches):\n",
    "      lg_, (ldr_, ldf_) = gan.train_step()\n",
    "      loss_g_running += lg_\n",
    "      loss_d_real_running += ldr_\n",
    "      loss_d_fake_running += ldf_\n",
    "    loss_g.append(loss_g_running / batches)\n",
    "    loss_d_real.append(loss_d_real_running / batches)\n",
    "    loss_d_fake.append(loss_d_fake_running / batches)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} ({int(time() - start)}s):\"\n",
    "          f\" G={loss_g[-1]:.3f},\"\n",
    "          f\" Dr={loss_d_real[-1]:.3f},\"\n",
    "          f\" Df={loss_d_fake[-1]:.3f}\")\n",
    "\n",
    "    gan_scat.set_offsets(gan.generate_samples())\n",
    "    data_scat.set_offsets(gan.real_data())\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "  plt.close(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Notice in the implementaiton above, we use `torch.nn.BCELoss` to implement the discriminator and generator loss with proper \"labels\".\n",
    "They are actually equivalent to your implementation - do you want to varify this fact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title GAN demo\n",
    "# @markdown Make sure you execute this cell to enable the widget!\n",
    "import ipywidgets\n",
    "\n",
    "# https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20List.html\n",
    "slider_mean = ipywidgets.FloatSlider(value=0., min=-1, max=1,\n",
    "                                     step=.1, readout_format='.5f',\n",
    "                                     description=\"Set Target Mean\",\n",
    "                                     style={'description_width': 'initial'})\n",
    "slider_sigma = ipywidgets.FloatSlider(value=.2, min=.2, max=1.,\n",
    "                                      step=.1, readout_format='.5f',\n",
    "                                      description=\"Set Target Sigma\",\n",
    "                                      style={'description_width': 'initial'})\n",
    "button = ipywidgets.Button(description=\"Start Training!\")\n",
    "output = ipywidgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "  # Display the message within the output widget.\n",
    "  with output:\n",
    "    train(slider_mean.value, slider_sigma.value)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "display(slider_mean, slider_sigma, button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "Through this tutorial, we have learned\n",
    "\n",
    "- How to implement the training loop of GANs.\n",
    "- Developed an intuition about the training dynamics of GANs.\n",
    "- How to implement the training objectives for the generator and discriminator of GANs.\n",
    "- How are GANs connected to density ratio estimation.\n",
    "\n",
    "Next tutorial will cover conditional GANs and ethical issues of DL."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D5_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
