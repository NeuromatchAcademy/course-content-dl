{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D5_GenerativeModels/student/W2D5_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Variational Autoencoders (VAEs)\n",
    "\n",
    "**Week 2, Day 5: Generative Models**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Saeed Salehi, Spiros Chavlis, Vikash Gilja\n",
    "\n",
    "__Content reviewers:__ Diptodip Deb\n",
    "\n",
    "__Production editors:__ Saeed Salehi, Spiros Chavlis \n",
    "\n",
    "*Inspired from UPenn course*:\n",
    "__Instructor:__ Konrad Kording, __Original Content creators:__ Richard Lange, Arash Ash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "## Tutorial Objectives\n",
    "In the first tutorial of the *Generative Models* day, we are going to\n",
    "\n",
    "- Think about unsupervised learning and get a bird's eye view of why it is useful\n",
    "- See the connection between AutoEncoding and dimensionality reduction\n",
    "- Start thinking about neural networks as generative models\n",
    "- Put on our Bayesian hats and turn AEs into VAEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "# @markdown These are the slides for the videos in this tutorial\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/rd7ng/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown we need to first upgrade the Colab's TorchVision\n",
    "!pip install --upgrade torchvision --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import matplotlib.pylab as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "\n",
    "def image_moments(image_batches, n_batches=None):\n",
    "  \"\"\"\n",
    "  Compute mean an covariance of all pixels from batches of images\n",
    "  \"\"\"\n",
    "  m1, m2 = torch.zeros((), device=DEVICE), torch.zeros((), device=DEVICE)\n",
    "  n = 0\n",
    "  for im in tqdm(image_batches, total=n_batches, leave=False,\n",
    "                 desc='Computing pixel mean and covariance...'):\n",
    "    im = im.to(DEVICE)\n",
    "    b = im.size()[0]\n",
    "    im = im.view(b, -1)\n",
    "    m1 = m1 + im.sum(dim=0)\n",
    "    m2 = m2 + (im.view(b,-1,1) * im.view(b,1,-1)).sum(dim=0)\n",
    "    n += b\n",
    "  m1, m2 = m1/n, m2/n\n",
    "  cov = m2 - m1.view(-1,1)*m1.view(1,-1)\n",
    "  return m1.cpu(), cov.cpu()\n",
    "\n",
    "\n",
    "def pca_encoder_decoder(mu, cov, k):\n",
    "  \"\"\"\n",
    "  Compute encoder and decoder matrices for PCA dimensionality reduction\n",
    "  \"\"\"\n",
    "  mu = mu.view(1,-1)\n",
    "  u, s, v = torch.svd_lowrank(cov, q=k)\n",
    "  W_encode = v / torch.sqrt(s)\n",
    "  W_decode = u * torch.sqrt(s)\n",
    "\n",
    "  def pca_encode(x):\n",
    "    # Encoder: subtract mean image and project onto top K eigenvectors of\n",
    "    # the data covariance\n",
    "    return (x.view(-1,mu.numel()) - mu) @ W_encode\n",
    "\n",
    "  def pca_decode(h):\n",
    "    # Decoder: un-project then add back in the mean\n",
    "    return (h @ W_decode.T) + mu\n",
    "\n",
    "  return pca_encode, pca_decode\n",
    "\n",
    "\n",
    "def cout(x, layer):\n",
    "  \"\"\"Unnecessarily complicated but complete way to\n",
    "  calculate the output depth, height and width size for a Conv2D layer\n",
    "\n",
    "  Args:\n",
    "      x (tuple): input size (depth, height, width)\n",
    "      layer (nn.Conv2d): the Conv2D layer\n",
    "\n",
    "  returns:\n",
    "      (int): output shape as given in [Ref]\n",
    "\n",
    "  Ref:\n",
    "      https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "  \"\"\"\n",
    "  assert isinstance(layer, nn.Conv2d)\n",
    "  p = layer.padding if isinstance(layer.padding, tuple) else (layer.padding,)\n",
    "  k = layer.kernel_size if isinstance(layer.kernel_size, tuple) else (layer.kernel_size,)\n",
    "  d = layer.dilation if isinstance(layer.dilation, tuple) else (layer.dilation,)\n",
    "  s = layer.stride if isinstance(layer.stride, tuple) else (layer.stride,)\n",
    "  in_depth, in_height, in_width = x\n",
    "  out_depth = layer.out_channels\n",
    "  out_height = 1 + (in_height + 2 * p[0] - (k[0] - 1) * d[0] - 1) // s[0]\n",
    "  out_width = 1 + (in_width + 2 * p[-1] - (k[-1] - 1) * d[-1] - 1) // s[-1]\n",
    "  return (out_depth, out_height, out_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting functions\n",
    "\n",
    "def plot_linear_ae(lin_losses):\n",
    "  plt.figure()\n",
    "  plt.plot(lin_losses)\n",
    "  plt.ylim([0, 2*torch.as_tensor(lin_losses).median()])\n",
    "  plt.xlabel('Training batch')\n",
    "  plt.ylabel('MSE Loss')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_conv_ae(lin_losses, conv_losses):\n",
    "  plt.figure()\n",
    "  plt.plot(lin_losses)\n",
    "  plt.plot(conv_losses)\n",
    "  plt.legend(['Lin AE', 'Conv AE'])\n",
    "  plt.xlabel('Training batch')\n",
    "  plt.ylabel('MSE Loss')\n",
    "  plt.ylim([0,\n",
    "            2*max(torch.as_tensor(conv_losses).median(),\n",
    "                  torch.as_tensor(lin_losses).median())])\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_images(images, h=5, w=5):\n",
    "  plt.figure(figsize=(h*2, w*2))\n",
    "  for i in range(h*w):\n",
    "    plt.subplot(h, w, i + 1)\n",
    "    plot_torch_image(images[i])\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_phi(phi, num=4):\n",
    "  plt.figure(figsize=(12, 3))\n",
    "  for i in range(num):\n",
    "    plt.subplot(1, num, i + 1)\n",
    "    plt.scatter(zs[i, :, 0], zs[i, :, 1], marker='.')\n",
    "    th = torch.linspace(0, 6.28318, 100)\n",
    "    x, y = torch.cos(th), torch.sin(th)\n",
    "    # Draw 2-sigma contours\n",
    "    plt.plot(\n",
    "        2*x*phi[i, 2].exp().item() + phi[i, 0].item(),\n",
    "        2*y*phi[i, 2].exp().item() + phi[i, 1].item()\n",
    "        )\n",
    "    plt.xlim(-5, 5)\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.grid()\n",
    "    plt.axis('equal')\n",
    "  plt.suptitle('If rsample() is correct, then most but not all points should lie in the circles')\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_torch_image(image, ax=None):\n",
    "  ax = ax if ax is not None else plt.gca()\n",
    "  c, h, w = image.size()\n",
    "  if c==1:\n",
    "    cm = 'gray'\n",
    "  else:\n",
    "    cm = None\n",
    "\n",
    "  # Torch images have shape (channels, height, width) but matplotlib expects\n",
    "  # (height, width, channels) or just (height,width) when grayscale\n",
    "  im_plt = torch.clip(image.detach().cpu().permute(1,2,0).squeeze(), 0.0, 1.0)\n",
    "  ax.imshow(im_plt, cmap=cm)\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  ax.spines['right'].set_visible(False)\n",
    "  ax.spines['top'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Generative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note:** Please run the cell after the video to download MNIST and CIFAR10 image datasets while the video plays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Generative Models\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"5EEx0sdyR_U\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Download MNIST and CIFAR10 image datasets while the above video plays\n",
    "# See https://pytorch.org/docs/stable/torchvision/datasets.html\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# MNIST contains handwritten digets 0-9, in grayscale images of size (1,28,28)\n",
    "mnist = tv.datasets.MNIST('./mnist/',\n",
    "                          train=True,\n",
    "                          transform=tv.transforms.ToTensor(),\n",
    "                          download=True)\n",
    "mnist_val = tv.datasets.MNIST('./mnist/',\n",
    "                              train=False,\n",
    "                              transform=tv.transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "cifar10 = tv.datasets.CIFAR10('./cifar10/',\n",
    "                              train=True,\n",
    "                              transform=tv.transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "cifar10_val = tv.datasets.CIFAR10('./cifar10/',\n",
    "                                  train=False,\n",
    "                                  transform=tv.transforms.ToTensor(),\n",
    "                                  download=True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Select a dataset\n",
    "\n",
    "We've built today's tutorial to be flexible. It should work more-or-less out of the box with both MNIST and CIFAR (and other image datasets). MNIST is in many ways simpler, and the results will likely look better and run a bit faster if using MNIST. But we are leaving it up to you to pick which one you want to experiment with!\n",
    "\n",
    "We encourage pods to coordinate so that some members use MNIST and others use CIFAR10. Keep in mind that the CIFAR dataset may require more learning epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def get_data(name='mnist'):\n",
    "  if name == 'mnist':\n",
    "    my_dataset = mnist\n",
    "    my_dataset_name = \"MNIST\"\n",
    "    my_dataset_size = (1, 28, 28)\n",
    "    my_dataset_dim = 28 * 28\n",
    "    my_valset = mnist_val\n",
    "  elif name == 'cifar10':\n",
    "    my_dataset = cifar10\n",
    "    my_dataset_name = \"CIFAR10\"\n",
    "    my_dataset_size = (3, 32, 32)\n",
    "    my_dataset_dim = 3 * 32 * 32\n",
    "    my_valset = cifar10_val\n",
    "\n",
    "  return my_dataset, my_dataset_name, my_dataset_size, my_dataset_dim, my_valset\n",
    "\n",
    "\n",
    "my_dataset, my_dataset_name, my_dataset_size, my_dataset_dim, my_valset = get_data(name='mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 2: AutoEncoders - Conceptual introduction to AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Latent Variable Models\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"_e0nKUeBDFo\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.1: Build a linear AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we'll create our first autoencoder. It will reduce images down to $K$ dimensions. The architecture will be quite simple: the input will be linearly mapped to a single hidden layer with $K$ units, which will then be linearly mapped back to an output that is the same size as the input:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\longrightarrow \\mathbf{h} \\longrightarrow \\mathbf{x'}\n",
    "\\end{equation}\n",
    "\n",
    "The loss function we'll use will simply be mean squared error (MSE) quantifying how well the reconstruction ($\\mathbf{x'}$) matches the original image ($\\mathbf{x}$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MSE}_{Loss} = \\sum_{i=1}^{N} ||\\mathbf{x}_i - \\mathbf{x'}_i||^2_2\n",
    "\\end{equation}\n",
    "\n",
    "If all goes well, then the AutoEncoder will learn, **end to end**, a good \"encoding\" or \"compression\" of inputs ($\\mathbf{x \\longrightarrow h}$) as well as a good \"decoding\" ($\\mathbf{h \\longrightarrow x'}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The first choice to make is the dimensionality of $\\mathbf{h}$. We'll see more on this below, but For MNIST, 5 to 20 is plenty. For CIFAR, we need more like 50 to 100 dimensions.\n",
    "\n",
    "Coordinate with your pod to try a variety of values for $K$ in each dataset so you can compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.1\n",
    "\n",
    "Complete the missing parts of the `LinearAutoEncoder` class.\n",
    "\n",
    "The `LinearAutoEncoder` as two stages: an `encoder` which linearly maps from inputs of size `x_dim = my_dataset_dim` to a hidden layer of size `h_dim = K` (with no nonlinearity), and a `decoder` which maps back from `K` up to the number of pixels in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown #### Run to define the `train_autoencoder` function.\n",
    "# @markdown Feel free to inspect the training function if the time allows.\n",
    "# @markdown `train_autoencoder(autoencoder, dataset, device, epochs=20, batch_size=250, seed=0)`\n",
    "\n",
    "\n",
    "def train_autoencoder(autoencoder, dataset, device, epochs=20, batch_size=250,\n",
    "                      seed=0):\n",
    "  autoencoder.to(DEVICE)\n",
    "  optim = torch.optim.Adam(autoencoder.parameters(),\n",
    "                           lr=1e-3,\n",
    "                           weight_decay=1e-5)\n",
    "  loss_fn = nn.MSELoss()\n",
    "  g_seed = torch.Generator()\n",
    "  g_seed.manual_seed(seed)\n",
    "  loader = DataLoader(dataset,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      pin_memory=True,\n",
    "                      num_workers=2,\n",
    "                      worker_init_fn=seed_worker,\n",
    "                      generator=g_seed)\n",
    "\n",
    "  mse_loss = torch.zeros(epochs * len(dataset) // batch_size, device=device)\n",
    "  i = 0\n",
    "  for epoch in trange(epochs, desc='Epoch'):\n",
    "    for im_batch, _ in loader:\n",
    "      im_batch = im_batch.to(device)\n",
    "      optim.zero_grad()\n",
    "      reconstruction = autoencoder(im_batch)\n",
    "      # write the loss calculation\n",
    "      loss = loss_fn(reconstruction.view(batch_size, -1),\n",
    "                     target=im_batch.view(batch_size, -1))\n",
    "      loss.backward()\n",
    "      optim.step()\n",
    "\n",
    "      mse_loss[i] = loss.detach()\n",
    "      i += 1\n",
    "  # After training completes, make sure the model is on CPU so we can easily\n",
    "  # do more visualizations and demos.\n",
    "  autoencoder.to('cpu')\n",
    "  return mse_loss.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class LinearAutoEncoder(nn.Module):\n",
    "  def __init__(self, x_dim, h_dim):\n",
    "    \"\"\"A Linear AutoEncoder\n",
    "\n",
    "    Args:\n",
    "      x_dim (int): input dimension\n",
    "      h_dim (int): hidden dimension, bottleneck dimension, K\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...),\n",
    "    # then remove or comment the line below to test your class\n",
    "    raise NotImplementedError(\"Please complete the LinearAutoEncoder class!\")\n",
    "    ####################################################################\n",
    "    # encoder layer\n",
    "    self.enc_lin = ...\n",
    "    # decoder layer\n",
    "    self.dec_lin = ...\n",
    "\n",
    "  def encode(self, x):\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...),\n",
    "    raise NotImplementedError(\"Please complete the `encode` function!\")\n",
    "    ####################################################################\n",
    "    h = self.enc_lin(x)\n",
    "    return h\n",
    "\n",
    "  def decode(self, h):\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...),\n",
    "    raise NotImplementedError(\"Please complete the `decode` function!\")\n",
    "    ####################################################################\n",
    "    x_prime = self.dec_lin(h)\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    flat_x = x.view(x.size(0), -1)\n",
    "    h = self.encode(flat_x)\n",
    "    return self.decode(h).view(x.size())\n",
    "\n",
    "\n",
    "# Pick your own K\n",
    "K = 20\n",
    "set_seed(seed=SEED)\n",
    "## Uncomment to test your code\n",
    "# lin_ae = LinearAutoEncoder(my_dataset_dim, K)\n",
    "# lin_losses = train_autoencoder(lin_ae, my_dataset, device=DEVICE, seed=SEED)\n",
    "# plot_linear_ae(lin_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_GenerativeModels/solutions/W2D5_Tutorial1_Solution_9e2bcbea.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1119.0 height=832.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D5_GenerativeModels/static/W2D5_Tutorial1_Solution_9e2bcbea_3.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "One way to think about AutoEncoders is that they automatically discover good dimensionality-reduction of the data. Another easy and common technique for dimensionality reduction is to project data onto the top $K$ **principal components** (Principal Component Analysis or PCA). For comparison, let's also do PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# PCA requires finding the top K eigenvectors of the data covariance. Start by\n",
    "# finding the mean and covariance of the pixels in our dataset\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "loader = DataLoader(my_dataset,\n",
    "                    batch_size=32,\n",
    "                    pin_memory=True,\n",
    "                    worker_init_fn=seed_worker,\n",
    "                    generator=g_seed)\n",
    "\n",
    "mu, cov = image_moments((im for im, _ in loader),\n",
    "                        n_batches=len(my_dataset) // 32)\n",
    "\n",
    "pca_encode, pca_decode = pca_encoder_decoder(mu, cov, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's visualize some of the reconstructions ($\\mathbf{x'}$) side-by-side with the input images ($\\mathbf{x}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Visualize the reconstructions `x'`\n",
    "\n",
    "n_plot = 7\n",
    "plt.figure(figsize=(10, 4.5))\n",
    "for i in range(n_plot):\n",
    "  idx = torch.randint(len(my_dataset), size=())\n",
    "  image, _ = my_dataset[idx]\n",
    "  # Get reconstructed image from autoencoder\n",
    "  with torch.no_grad():\n",
    "    reconstruction = lin_ae(image.unsqueeze(0)).reshape(image.size())\n",
    "\n",
    "  # Get reconstruction from PCA dimensionality reduction\n",
    "  h_pca = pca_encode(image)\n",
    "  recon_pca = pca_decode(h_pca).reshape(image.size())\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1)\n",
    "  plot_torch_image(image)\n",
    "  if i == 0:\n",
    "    plt.ylabel('Original\\nImage')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + n_plot)\n",
    "  plot_torch_image(reconstruction)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'Lin AE\\n(K={K})')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + 2*n_plot)\n",
    "  plot_torch_image(recon_pca)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'PCA\\n(K={K})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 2.1: PCA vs. Linear autoencoder\n",
    "\n",
    "Compare the PCA-based reconstructions to those from the linear autoencoder. Is one better than the other? Are they equally good? Equally bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.2: Building a nonlinear convolutional autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Autoencoders\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"MlyIL1PmDCA\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The `nn.Linear` layer by default has a \"bias\" term, which is a learnable offset parameter separate for each output unit. Just like the PCA encoder \"centered\" the data by subtracting off the average image (`mu`) before encoding and added it back in during decoding, a bias term in the decoder can effectively account for the first moment of the data (AKA the average of all images in the training set). Convolution layers do have bias parameters, but the bias is applied per filter rather than per pixel location. If we're generating RGB images, then `Conv2d` will learn only 3 biases: one for each of R, G, and B.\n",
    "\n",
    "For some conceptual continuity with both PCA and the `nn.Linear` layers above, the next block defines a custom layer for adding a learnable per-pixel offset. This custom layer will be used twice: as the first stage of the encoder and as the final stage of the decoder. Ideally, this means that the rest of the neural net can focus on fitting more interesting fine-grained structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class BiasLayer(nn.Module):\n",
    "  def __init__(self, shape):\n",
    "    super(BiasLayer, self).__init__()\n",
    "    init_bias = torch.zeros(shape)\n",
    "    self.bias = nn.Parameter(init_bias, requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "With that out of the way, we will next define a **nonlinear** and **convolutional** autoencoder. Here's a quick tour of the architecture:\n",
    "\n",
    "1. The **encoder** once again maps from images to $\\mathbf{h}\\in\\mathbb{R}^K$. This will use a `BiasLayer` followed by two convolutional layers (`nn.Conv2D`), followed by flattening and linearly projecting down to $K$ dimensions. The convolutional layers will have `ReLU` nonlinearities on their outputs. \n",
    "1. The **decoder** inverts this process, taking in vectors of length $K$ and outputting images. Roughly speaking, its architecture is a \"mirror image\" of the encoder: the first decoder layer is linear, followed by two **deconvolution** layers ([`ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)). The `ConvTranspose2d`layers will have `ReLU` nonlinearities on their _inputs_. This \"mirror image\" between the encoder and decoder is a useful and near-ubiquitous convention. The idea is that the decoder can then learn to approximately invert the encoder, but it is not a strict requirement (and it does not guarantee the decoder will be an exact inverse of the encoder!).\n",
    "\n",
    "Below is a schematic of the architecture for MNIST. Notice that the width and height dimensions of the image planes reduce after each `nn.Conv2d` and increase after each `nn.ConvTranspose2d`. With CIFAR10, the architecture is the same but the exact sizes will differ a bit.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/CIS-522/course-content/main/tutorials/W08_AutoEncoders_GANs/static/conv_sizes.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "[`torch.nn.ConvTranspose2d`](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) module can be seen as the gradient of `Conv2d` with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation). The following code demonstrates this change in sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "dummy_image = torch.rand(my_dataset_size).unsqueeze(0)\n",
    "in_channels = my_dataset_size[0]\n",
    "out_channels = 7\n",
    "\n",
    "dummy_conv = nn.Conv2d(in_channels=in_channels,\n",
    "                       out_channels=out_channels,\n",
    "                       kernel_size=5)\n",
    "\n",
    "dummy_deconv = nn.ConvTranspose2d(in_channels=out_channels,\n",
    "                                  out_channels=in_channels,\n",
    "                                  kernel_size=5)\n",
    "\n",
    "print(f'Size of image is {dummy_image.shape}')\n",
    "print(f'Size of Conv2D(image) {dummy_conv(dummy_image).shape}')\n",
    "print(f'Size of ConvTranspose2D(Conv2D(image)) {dummy_deconv(dummy_conv(dummy_image)).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 2.2: Fill in code for the `ConvAutoEncoder` module\n",
    "\n",
    "Complete the `ConvAutoEncoder` class. We use the helper function `cout(torch.Tensor, nn.Conv2D)` to calculate the output shape of a [`nn.Conv2D`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) layer given a tensor with shape (channels, height, width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(nn.Module):\n",
    "  def __init__(self, x_dim, h_dim, n_filters=32, filter_size=5):\n",
    "    \"\"\"A Convolutional AutoEncoder\n",
    "\n",
    "    Args:\n",
    "      x_dim (tuple): input dimensions (channels, height, widths)\n",
    "      h_dim (int): hidden dimension, bottleneck dimension, K\n",
    "      n_filters (int): number of filters (number of output channels)\n",
    "      filter_size (int): kernel size\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    channels, height, widths = x_dim\n",
    "\n",
    "    # encoder input bias layer\n",
    "    self.enc_bias = BiasLayer(x_dim)\n",
    "\n",
    "    # first encoder conv2d layer\n",
    "    self.enc_conv_1 = nn.Conv2d(channels, n_filters, filter_size)\n",
    "\n",
    "    # output shape of the first encoder conv2d layer given x_dim input\n",
    "    conv_1_shape = cout(x_dim, self.enc_conv_1)\n",
    "\n",
    "    # second encoder conv2d layer\n",
    "    self.enc_conv_2 = nn.Conv2d(n_filters, n_filters, filter_size)\n",
    "\n",
    "    # output shape of the second encoder conv2d layer given conv_1_shape input\n",
    "    conv_2_shape = cout(conv_1_shape, self.enc_conv_2)\n",
    "\n",
    "    # The bottleneck is a dense layer, therefore we need a flattenning layer\n",
    "    self.enc_flatten = nn.Flatten()\n",
    "\n",
    "    # conv output shape is (depth, height, width), so the flatten size is:\n",
    "    flat_after_conv = conv_2_shape[0] * conv_2_shape[1] * conv_2_shape[2]\n",
    "\n",
    "    # encoder Linear layer\n",
    "    self.enc_lin = nn.Linear(flat_after_conv, h_dim)\n",
    "\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...),\n",
    "    # then remove or comment the line below to test your class\n",
    "    # Remember that decoder is \"undo\"-ing what the encoder has done!\n",
    "    raise NotImplementedError(\"Please complete the `ConvAutoEncoder` class!\")\n",
    "    ####################################################################\n",
    "    # decoder Linear layer\n",
    "    self.dec_lin = ...\n",
    "\n",
    "    # unflatten data to (depth, height, width) shape\n",
    "    self.dec_unflatten = nn.Unflatten(dim=-1, unflattened_size=conv_2_shape)\n",
    "\n",
    "    # first \"deconvolution\" layer\n",
    "    self.dec_deconv_1 = nn.ConvTranspose2d(n_filters, n_filters, filter_size)\n",
    "\n",
    "    # second \"deconvolution\" layer\n",
    "    self.dec_deconv_2 = ...\n",
    "\n",
    "    # decoder output bias layer\n",
    "    self.dec_bias = BiasLayer(x_dim)\n",
    "\n",
    "  def encode(self, x):\n",
    "    s = self.enc_bias(x)\n",
    "    s = F.relu(self.enc_conv_1(s))\n",
    "    s = F.relu(self.enc_conv_2(s))\n",
    "    s = self.enc_flatten(s)\n",
    "    h = self.enc_lin(s)\n",
    "    return h\n",
    "\n",
    "  def decode(self, h):\n",
    "    s = F.relu(self.dec_lin(h))\n",
    "    s = self.dec_unflatten(s)\n",
    "    s = F.relu(self.dec_deconv_1(s))\n",
    "    s = self.dec_deconv_2(s)\n",
    "    x_prime = self.dec_bias(s)\n",
    "    return x_prime\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.decode(self.encode(x))\n",
    "\n",
    "\n",
    "K = 20\n",
    "set_seed(seed=SEED)\n",
    "## Uncomment to test your solution\n",
    "# conv_ae = ConvAutoEncoder(my_dataset_size, K)\n",
    "# assert conv_ae.encode(my_dataset[0][0].unsqueeze(0)).numel() == K, \"Encoder output size should be K!\"\n",
    "# conv_losses = train_autoencoder(conv_ae, my_dataset, device=DEVICE, seed=SEED)\n",
    "# plot_conv_ae(lin_losses, conv_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_GenerativeModels/solutions/W2D5_Tutorial1_Solution_699f4d8b.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1119.0 height=832.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D5_GenerativeModels/static/W2D5_Tutorial1_Solution_699f4d8b_3.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "You should see that the `ConvAutoEncoder` achieved lower MSE loss than the linear one. If not, you may need to retrain it (or run another few training epochs from where it left off). We make fewer guarantees on this working with CIFAR10, but it should definitely work with MNIST.\n",
    "\n",
    "Now let's visually compare the reconstructed images from the linear and nonlinear autoencoders. Keep in mind that both have the same dimensionality for $\\mathbf{h}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @markdown Visualize the linear and nonlinear AE outputs\n",
    "n_plot = 7\n",
    "plt.figure(figsize=(10, 4.5))\n",
    "for i in range(n_plot):\n",
    "  idx = torch.randint(len(my_dataset), size=())\n",
    "  image, _ = my_dataset[idx]\n",
    "  with torch.no_grad():\n",
    "    # Get reconstructed image from linear autoencoder\n",
    "    lin_recon = lin_ae(image.unsqueeze(0))[0]\n",
    "\n",
    "    # Get reconstruction from deep (nonlinear) autoencoder\n",
    "    nonlin_recon = conv_ae(image.unsqueeze(0))[0]\n",
    "\n",
    "  plt.subplot(3, n_plot, i+1)\n",
    "  plot_torch_image(image)\n",
    "  if i == 0:\n",
    "    plt.ylabel('Original\\nImage')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + n_plot)\n",
    "  plot_torch_image(lin_recon)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'Lin AE\\n(K={K})')\n",
    "\n",
    "  plt.subplot(3, n_plot, i + 1 + 2*n_plot)\n",
    "  plot_torch_image(nonlin_recon)\n",
    "  if i == 0:\n",
    "    plt.ylabel(f'NonLin AE\\n(K={K})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 2.3: Inspecting the hidden representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Let's start by plotting points in the hidden space ($\\mathbf{h}$), colored by class of the image (which, of course, the autoencoder didn't know about during training!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "h_vectors = torch.zeros(len(my_valset), K, device=DEVICE)\n",
    "labels = torch.zeros(len(my_valset), dtype=torch.int32)\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "loader = DataLoader(my_valset, batch_size=200,\n",
    "                    pin_memory=True,\n",
    "                    worker_init_fn=seed_worker,\n",
    "                    generator=g_seed)\n",
    "conv_ae.to(DEVICE)\n",
    "i = 0\n",
    "for im, la in loader:\n",
    "  b = im.size()[0]\n",
    "  h_vectors[i:i+b, :] = conv_ae.encode(im.to(DEVICE))\n",
    "  labels[i:i+b] = la\n",
    "  i += b\n",
    "conv_ae.to('cpu')\n",
    "h_vectors = h_vectors.detach().cpu()\n",
    "_, _, h_pcs = torch.pca_lowrank(h_vectors, q=2)\n",
    "h_xy = h_vectors @ h_pcs\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(h_xy[:, 0], h_xy[:, 1], c=labels, cmap='hsv')\n",
    "plt.title('2D projection of h, colored by class')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "To explore the hidden representations, $\\mathbf{h}$, we're going to pick two random images from the dataset and interpolate them 3 different ways. Let's introduce some notation for this: we'll use a variable $t \\in [0,1]$ to gradually transition from image $\\mathbf{x}_1$ at $t=0$ to image $\\mathbf{x}_2$ at $t=1$. Using $\\mathbf{x}(t)$ to denote the interpolated output, the three methods will be\n",
    "\n",
    "1. interpolate the raw pixels, thus:\n",
    "  \\begin{equation}\n",
    "  \\mathbf{x}(t) = (1-t) \\cdot \\mathbf{x}_1 + t \\cdot \\mathbf{x}_2\n",
    "  \\end{equation}\n",
    "\n",
    "2. interpolate their encodings from the **linear** AE, thus:\n",
    "  \\begin{equation}\n",
    "\\mathbf{x}(t) = \\text{linear_decoder}((1-t) \\cdot \\text{linear_encoder}(\\mathbf{x}_1) + t \\cdot  \\text{linear_encoder}(\\mathbf{x}_2))\n",
    "  \\end{equation}\n",
    "\n",
    "3. interpolate their encodings from the **nonlinear** AE, thus:\n",
    "  \\begin{equation}\n",
    "  \\mathbf{x}(t) = \\text{conv_decoder}((1-t) \\cdot \\text{conv_encoder}(\\mathbf{x}_1) + t \\cdot  \\text{conv_encoder}(\\mathbf{x}_2))\n",
    "  \\end{equation}\n",
    "\n",
    "**Note:** this demo will likely look better using MNIST than using CIFAR10. Check with other members of your pod. If you're using CIFAR10 for this notebook, consider having someone using MNIST share their screen. \n",
    "\n",
    "What do you notice about the \"interpolated\" images, especially around $t \\approx 1/2$? How many distinct classes do you see in the bottom row?\n",
    "Re-run the below cell a few times to look at multiple examples.\n",
    "\n",
    "**Discuss with your pod and describe what is happening here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "idx1 = torch.randint(len(my_dataset), size=())\n",
    "idx2 = torch.randint(len(my_dataset), size=())\n",
    "x1, _ = my_dataset[idx1]\n",
    "x2, _ = my_dataset[idx2]\n",
    "n_interp = 11\n",
    "\n",
    "with torch.no_grad():\n",
    "  h1_lin = lin_ae.encode(x1.reshape(1, -1))\n",
    "  h2_lin = lin_ae.encode(x2.reshape(1, -1))\n",
    "  h1_conv = conv_ae.encode(x1.unsqueeze(0))\n",
    "  h2_conv = conv_ae.encode(x2.unsqueeze(0))\n",
    "\n",
    "plt.figure(figsize=(14, 4.5))\n",
    "for i in range(n_interp):\n",
    "  t = i / (n_interp - 1)\n",
    "  pixel_interp = (1 - t)*x1 + t*x2\n",
    "  plt.subplot(3, n_interp, i + 1)\n",
    "  plot_torch_image(pixel_interp)\n",
    "  if i == 0:\n",
    "    plt.ylabel('Raw\\nPixels')\n",
    "  plt.title(f't={i}/{n_interp-1}')\n",
    "\n",
    "  with torch.no_grad():\n",
    "    lin_ae_interp = lin_ae.decode((1-t)*h1_lin + t*h2_lin)\n",
    "  plt.subplot(3, n_interp, i + 1 + n_interp)\n",
    "  plot_torch_image(lin_ae_interp.reshape(my_dataset_size))\n",
    "  if i == 0:\n",
    "    plt.ylabel('Lin AE')\n",
    "\n",
    "  with torch.no_grad():\n",
    "    conv_ae_interp = conv_ae.decode((1-t)*h1_conv + t*h2_conv)[0]\n",
    "  plt.subplot(3, n_interp, i + 1 + 2*n_interp)\n",
    "  plot_torch_image(conv_ae_interp)\n",
    "  if i == 0:\n",
    "    plt.ylabel('NonLin AE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Generative models and density networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.1: Generating novel images from the decoder\n",
    "\n",
    "If we isolate the decoder part of the AutoEncoder, what we have is a neural network that takes as input a vector of size $K$ and produces as output an image that looks something like our training data. Recall that in our earlier notation, we had an input $\\mathbf{x}$ that was mapped to a low-dimensional hidden representation $\\mathbf{h}$ which was then decoded into a reconstruction of the input, $\\mathbf{x'}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\overset{\\text{encode}}{\\longrightarrow} \\mathbf{h} \\overset{\\text{decode}}{\\longrightarrow} \\mathbf{x'}\\, .\n",
    "\\end{equation}\n",
    "\n",
    "Partly as a matter of convention, and partly to distinguish where we are going next from the previous section, we're going to introduce a new variable, $\\mathbf{z} \\in \\mathbb{R}^K$, which will take the place of $\\mathbf{h}$. The key difference is that while $\\mathbf{h}$ is produced by the encoder for a particular $\\mathbf{x}$, $\\mathbf{z}$ will be drawn out of thin air from a prior of our choosing:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{z} &\\sim p(\\mathbf{z})\\\\\n",
    "\\mathbf{z} &\\overset{\\text{decode}}{\\longrightarrow} \\mathbf{x}\\, .\n",
    "\\end{align}\n",
    "\n",
    "(Note that it is also conventional to drop the \"prime\" on $\\mathbf{x}$ when it is no longer being thought of as a \"reconstruction\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 3.1: sample $\\mathbf{z}$ from a standard normal and visualize the images produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def generate_images(autoencoder, K, n_images=1, seed=0):\n",
    "  \"\"\"Generate n_images 'new' images from the decoder part of the given\n",
    "  autoencoder.\n",
    "\n",
    "  returns (n_images, channels, height, width) tensor of images\n",
    "  \"\"\"\n",
    "\n",
    "  set_seed(seed=seed)\n",
    "  # Concatenate tuples to get (n_images, channels, height, width)\n",
    "  output_shape = (n_images,) + my_dataset_size\n",
    "\n",
    "  with torch.no_grad():\n",
    "    ####################################################################\n",
    "    # Fill in all missing code below (...)\n",
    "    # sample z from a standard normal distribution with shape (n_images, K)\n",
    "    # then pass z through autoencoder.decode()\n",
    "    # then remove or comment the line below to test your function\n",
    "    raise NotImplementedError(\"Please complete the generate_images function!\")\n",
    "    ####################################################################\n",
    "    # sample z, pass through autoencoder.decode()\n",
    "    z = ...\n",
    "    x = ...\n",
    "\n",
    "    return x.reshape(output_shape)\n",
    "\n",
    "\n",
    "K = 20\n",
    "## Uncomment to run it\n",
    "# images = generate_images(conv_ae, K, n_images=25, seed=SEED)\n",
    "# plot_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_GenerativeModels/solutions/W2D5_Tutorial1_Solution_14da446b.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1406.0 height=1408.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D5_GenerativeModels/static/W2D5_Tutorial1_Solution_14da446b_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.2: Formalizing the problem: density estimation with maximum likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Note: we've moved the technical details of \"formalizing the problem\" to Appendix A.1 at the end of this notebook. Those who want more of the theoretical/mathematical backstory are encouraged to read it. Those who just want to build a VAE, carry on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: Variational Auto-Encoders (VAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Variational Auto-Encoders (VAEs)\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"srWb_Gp6OGA\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 4.1: Components of a VAE\n",
    "### Recognition models and density networks\n",
    "\n",
    "Variational AutoEncoders (VAEs) are a lot like the classic AutoEncoders (AEs) you just saw, but where we explicitly think about probability distributions. In the language of VAEs, the __encoder__ is replaced with a __recognition model__, and the __decoder__ is replaced with a __density network__.\n",
    "\n",
    "Where in a classic autoencoder the encoder maps from images to a single hidden vector,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\overset{\\text{AE}}{\\longrightarrow} \\mathbf{h} \\, ,\n",
    "\\end{equation}\n",
    "\n",
    "in a VAE we would say that a recognition model maps from inputs to entire __distributions__ over hidden vectors,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\overset{\\text{VAE}}{\\longrightarrow} q(\\mathbf{z}) \\, ,\n",
    "\\end{equation}\n",
    "\n",
    "which we will then sample from.\n",
    "We'll say more in a moment about what kind of distribution $q(\\mathbf{z})$ is.\n",
    "Part of what makes VAEs work is that the loss function will require good reconstructions of the input not just for a single $\\mathbf{z}$, but _on average_ from samples of $\\mathbf{z} \\sim q(\\mathbf{z})$.\n",
    "\n",
    "In the classic autoencoder, we had a decodebeginr which maps from hidden vectors to reconstructions of the input:\n",
    "\\begin{equation}\n",
    "\\mathbf{h} \\overset{\\text{AE}}{\\longrightarrow} \\mathbf{x'} \\, .\n",
    "\\end{equation}\n",
    "\n",
    "In a density network, reconstructions are expressed in terms of a distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z} \\overset{\\text{VAE}}{\\longrightarrow} p(\\mathbf{x}|\\mathbf{z};\\mathbf{w})\n",
    "\\end{equation}\n",
    "\n",
    "where, as above, $p(\\mathbf{x}|\\mathbf{z};\\mathbf{w})$ is defined by mapping $\\mathbf{z}$ through a density network then treating the resulting $f(\\mathbf{z};\\mathbf{w})$ as the mean of a (Gaussian) distribution over $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 4.1: sampling from $q(\\mathbf{z})$\n",
    "\n",
    "How can a neural network (the __recognition model__) output an entire probability distribution $$\\mathbf{x} \\longrightarrow q(\\mathbf{z}) \\, ?$$\n",
    "One idea would be to make the weights of the neural network stochastic, so that every time the network is run, a different $\\mathbf{z}$ is produced. (In fact, this is quite common in [Bayesian Neural Networks](https://medium.com/neuralspace/bayesian-neural-network-series-post-1-need-for-bayesian-networks-e209e66b70b2), but this isn't what people use in VAEs.)\n",
    "\n",
    "Instead, we will start by committing to a particular _family_ of distributions. We'll then have the recognition model output the _parameters_ of $q$, which we'll call $\\phi$. A common choice, which we will use throughout, is the family of isotropic multivariate Gaussians (spherical or isotropic: $\\mathbf{\\Sigma} = \\sigma^2 ~ \\mathbf{I}$) $^\\dagger$:\n",
    "\n",
    "\\begin{equation}\n",
    "q(\\mathbf{z};\\phi) = \\mathcal{N}(\\mathbf{z};\\boldsymbol{\\mu},\\sigma^2\\mathbf{I}_K) = \\prod_{k=1}^K \\mathcal{N}(z_k; \\mu_k, \\sigma^2)\n",
    "\\end{equation}\n",
    "\n",
    "where the $K+1$ parameters are$^{\\dagger \\dagger}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi = \\lbrace{\\mu_1, \\mu_2, \\ldots, \\mu_K, \\log(\\sigma)}\\rbrace \\, .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "By defining the last entry of $\\phi$ as the _logarithm_ of $\\sigma$, the last entry can be any real number while enforcing the requirement that $\\sigma > 0$.\n",
    "\n",
    "A recognition model is a neural network that takes $\\mathbf{x}$ as input and produces $\\phi$ as output. The purpose of the following exercise is not to write a recognition model (that will come later), but to clarify the relationship between $\\phi$ and $q(\\mathbf{z})$. You will write a function, `rsample`, which takes as input a batch $\\phi$s and will output a set of samples of $\\mathbf{z}$ drawn from $q(\\mathbf{z};\\phi)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "\n",
    "$^\\dagger$ PyTorch has a `MultivariateNormal` class which handles multivariate Gaussian distributions with arbitrary covariance matrices. It is not very beginner-friendly, though, so we will write our own functions to work with $\\phi$, which will both teach you some implementation details and is not very hard especially if we use only an isotropic ($\\sigma$) or diagonal $\\left( \\lbrace{\\sigma_1, \\ldots, \\sigma_K}\\rbrace \\right)$ covariance\n",
    "\n",
    "$^{\\dagger \\dagger}$ Another common parameterization is to use a separate $\\sigma$ for each dimension of $\\mathbf{z}$, in which case $\\phi$ would instead contain $2K$ parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi = \\lbrace{\\mu_1, \\mu_2, \\ldots, \\mu_K, \\log(\\sigma_1), \\ldots, \\log(\\sigma_K)}\\rbrace \\, .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def rsample(phi, n_samples, seed=0):\n",
    "  \"\"\"Sample z ~ q(z;phi)\n",
    "  Ouput z is size [b, n_samples, K] given phi with shape [b,K+1]. The first K\n",
    "  entries of each row of phi are the mean of q, and phi[:,-1] is the log\n",
    "  standard deviation\n",
    "  \"\"\"\n",
    "\n",
    "  set_seed(seed=seed)\n",
    "  b, kplus1 = phi.size()\n",
    "  k = kplus1 - 1\n",
    "  mu, sig = phi[:, :-1], phi[:, -1].exp()\n",
    "  ####################################################################\n",
    "  # Fill in all missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"Please complete the rsample function!\")\n",
    "  ####################################################################\n",
    "  eps = ...\n",
    "  return eps * sig.view(b, 1, 1) + mu.view(b, 1, k)\n",
    "\n",
    "\n",
    "phi = torch.randn(4, 3, device=DEVICE)\n",
    "## Uncomment below to test your code\n",
    "# zs = rsample(phi=phi, n_samples=100, seed=SEED)\n",
    "# assert zs.size() == (4, 100, 2), \"rsample size is incorrect!\"\n",
    "# assert zs.device == phi.device, \"rsample device doesn't match phi device!\"\n",
    "# zs = zs.cpu()\n",
    "# plot_phi(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content-dl/tree/main//tutorials/W2D5_GenerativeModels/solutions/W2D5_Tutorial1_Solution_a4b2811e.py)\n",
    "\n",
    "*Example output:*\n",
    "\n",
    "<img alt='Solution hint' align='left' width=1696.0 height=421.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D5_GenerativeModels/static/W2D5_Tutorial1_Solution_a4b2811e_1.png>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 5: State of the art VAEs and Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 5: State-of-the-art VAEs\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"f2jSzq7lndo\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "display(out)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "W2D5_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
