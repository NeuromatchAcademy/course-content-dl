{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W3D1_AttentionAndTransformers/instructor/W3D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W3D1_AttentionAndTransformers/instructor/W3D1_Tutorial2.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Bonus Tutorial: Understanding Pre-training, Fine-tuning and robustness of Transformers\n",
    "\n",
    "**Week 3, Day 1: Attention and Transformers**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Bikram Khastgir, Rajaswa Patil, Egor Zverev, Kelson Shilling-Scrivo, Alish Dipani, He He\n",
    "\n",
    "__Content reviewers:__ Ezekiel Williams, Melvin Selim Atay, Khalid Almubarak, Lily Cheng, Hadi Vafaei, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Gagana B, Anoop Kulkarni, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Khalid Almubarak, Gagana B, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "On finishing the tutorial, you will be able to:\n",
    "- Write down the objective of language model pre-training\n",
    "- Understand the framework of pre-training then fine-tuning\n",
    "- Name three types of biases in pre-trained language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=\"https://mfr.ca-1.osf.io/render?url=https://osf.io/6eudb/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These are the slides for all videos in this tutorial. If you want to locally download the slides, click [here](https://osf.io/6eudb/download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown There may be *errors* and/or *warnings* reported during the installation. However, they are to be ignored.\n",
    "!pip install datasets --quiet\n",
    "!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set environment variables\n",
    "\n",
    "import os\n",
    "os.environ['TA_CACHE_DIR'] = 'data/'\n",
    "os.environ['NLTK_DATA'] = 'nltk_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# transformers library\n",
    "from transformers import Trainer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Handles variability by controlling sources of randomness\n",
    "  through set seed values\n",
    "\n",
    "  Args:\n",
    "    seed: Integer\n",
    "      Set the seed value to given integer.\n",
    "      If no seed, set seed value to random integer in the range 2^32\n",
    "    seed_torch: Bool\n",
    "      Seeds the random number generator for all devices to\n",
    "      offer some guarantees on reproducibility\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus 1: Language modeling as pre-training\n",
    "\n",
    "*Time estimate: ~20mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Pre-training\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')  \n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'dMpvzEEDOwI'), ('Bilibili', 'BV13q4y1X7Tt')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Bonus Interactive Demo 1: GPT-2 for sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this section, we will use the pre-trained language model GPT-2 for sentiment classification.\n",
    "\n",
    "Let's first load the Yelp review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download and load the dataset\n",
    "\n",
    "import requests, tarfile\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = 'data/'\n",
    "\n",
    "url = \"https://osf.io/kthjg/download\"\n",
    "fname = \"huggingface.tar.gz\"\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "  print('Dataset is being downloading...')\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fname, 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "  print('Download is finished.')\n",
    "\n",
    "  with tarfile.open(fname) as ft:\n",
    "    ft.extractall('data/')\n",
    "  print('Files have been extracted.')\n",
    "\n",
    "DATASET = datasets.load_dataset(\"yelp_review_full\",\n",
    "                                download_mode=\"reuse_dataset_if_exists\",\n",
    "                                cache_dir='data/')\n",
    "\n",
    "# If the above produces an error uncomment below:\n",
    "# DATASET = load_dataset(\"yelp_review_full\", ignore_verifications=True)\n",
    "print(type(DATASET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus 1.1: Load Yelp reviews dataset âŒ›ðŸ¤—\n",
    "train_dataset = DATASET['train']\n",
    "test_dataset = DATASET['test']\n",
    "\n",
    "# filter training data by sentiment value\n",
    "sentiment_dict = {}\n",
    "sentiment_dict[\"Sentiment = 0\"] = train_dataset.filter(lambda example: example['label']==0)\n",
    "sentiment_dict[\"Sentiment = 1\"] = train_dataset.filter(lambda example: example['label']==1)\n",
    "sentiment_dict[\"Sentiment = 2\"] = train_dataset.filter(lambda example: example['label']==2)\n",
    "sentiment_dict[\"Sentiment = 3\"] = train_dataset.filter(lambda example: example['label']==3)\n",
    "sentiment_dict[\"Sentiment = 4\"] = train_dataset.filter(lambda example: example['label']==4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Kaggle users:** If the cell above fails, please re-execute it several times!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, we'll set up a text context for the pre-trained language models. We can either sample a review from the Yelp reviews dataset or write our own custom review as the text context. We will perform text-generation and sentiment-classification with this text context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus 1.2: Setting up a text context âœï¸\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean up text\n",
    "\n",
    "    Args:\n",
    "      text: String\n",
    "        Input text sequence\n",
    "\n",
    "    Returns:\n",
    "      text: String\n",
    "        Returned clean string does not contain new-line characters,\n",
    "        backslashes etc.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\\\n\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    return text\n",
    "\n",
    "# @markdown ---\n",
    "sample_review_from_yelp = \"Sentiment = 4\"  # @param [\"Sentiment = 0\", \"Sentiment = 1\", \"Sentiment = 2\", \"Sentiment = 3\", \"Sentiment = 4\"]\n",
    "# @markdown **Randomly sample a response from the Yelp review dataset with the given sentiment value {0:ðŸ˜ , 1:ðŸ˜¦, 2:ðŸ˜, 3:ðŸ™‚, 4:ðŸ˜€}**\n",
    "\n",
    "# @markdown ---\n",
    "use_custom_review = False  # @param {type:\"boolean\"}\n",
    "custom_review = \"I liked this movie very much because ...\"  # @param {type:\"string\"}\n",
    "# @markdown ***Alternatively, write your own review (don't forget to enable custom review using the checkbox given above)***\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown **NOTE:** *Run the cell after setting all the You can adding different kinds of extension above fields appropriately!*\n",
    "\n",
    "print(\"\\n ****** The selected text context ****** \\n\")\n",
    "if use_custom_review:\n",
    "  context = clean_text(custom_review)\n",
    "else:\n",
    "  context = clean_text(sentiment_dict[sample_review_from_yelp][random.randint(0,len(sentiment_dict[sample_review_from_yelp])-1)][\"text\"])\n",
    "pprint(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Here, we'll ask the pre-trained language models to extend the selected text context further. You can try adding different kinds of extension prompts at the end of the text context, conditioning it for different kinds of text extensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus 1.3: Extending the review with pre-trained models ðŸ¤–\n",
    "\n",
    "# @markdown ---\n",
    "model = \"gpt2\"  # @param [\"gpt2\", \"gpt2-medium\", \"xlnet-base-cased\"]\n",
    "generator = pipeline('text-generation', model=model)\n",
    "set_seed(seed=SEED)\n",
    "# @markdown **Select a pre-trained language model to generate text ðŸ¤–**\n",
    "\n",
    "# @markdown *(might take some time to download the pre-trained weights for the first time)*\n",
    "\n",
    "# @markdown ---\n",
    "extension_prompt = \"Hence, overall I feel that ...\"  # @param {type:\"string\"}\n",
    "num_output_responses = 1  # @param {type:\"slider\", min:1, max:10, step:1}\n",
    "# @markdown **Provide a prompt to extend the review âœï¸**\n",
    "\n",
    "input_text = context + \" \" + extension_prompt\n",
    "# @markdown **NOTE:** *Run this cell after setting all the fields appropriately!*\n",
    "\n",
    "# @markdown **NOTE:** *Some pre-trained models might not work well with longer texts!*\n",
    "\n",
    "generated_responses = generator(input_text, max_length=512, num_return_sequences=num_output_responses)\n",
    "\n",
    "print(\"\\n *********** INPUT PROMPT TO THE MODEL ************ \\n\")\n",
    "pprint(input_text)\n",
    "\n",
    "print(\"\\n *********** EXTENDED RESPONSES BY THE MODEL ************ \\n\")\n",
    "for response in generated_responses:\n",
    "  pprint(response[\"generated_text\"][len(input_text):] + \" ...\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, we'll ask the pre-trained language models to calculate the likelihood of already existing text-extensions. We can define a positive text-extension as well as a negative text-extension. The sentiment of the given text context can then be determined by comparing the likelihoods of the given text extensions. \n",
    "\n",
    "(For a positive review, a positive text-extension should ideally be given more likelihood by the pre-trained language model as compared to a negative text-extension. Similarly, for a negative review, the negative text-extension should have more likelihood than the positive text-extension.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus 1.4: Sentiment binary-classification with likelihood of positive and negative extensions of the review ðŸ‘ðŸ‘Ž\n",
    "\n",
    "# @markdown ---\n",
    "model_name = \"gpt2\"  # @param [\"gpt2\", \"gpt2-medium\", \"xlnet-base-cased\"]\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# @markdown **Select a pre-trained language model to score the likelihood of extended review**\n",
    "\n",
    "# @markdown *(might take some time to download the pre-trained weights for the first time)*\n",
    "\n",
    "# @markdown ---\n",
    "custom_positive_extension = \"I would definitely recommend this!\"  # @param {type:\"string\"}\n",
    "custom_negative_extension = \"I would not recommend this!\"  # @param {type:\"string\"}\n",
    "# @markdown **Provide custom positive and negative extensions to the review âœï¸**\n",
    "\n",
    "texts = [context, custom_positive_extension, custom_negative_extension]\n",
    "encodings = tokenizer(texts)\n",
    "\n",
    "positive_input_ids = torch.tensor(encodings[\"input_ids\"][0] + encodings[\"input_ids\"][1])\n",
    "positive_attention_mask = torch.tensor(encodings[\"attention_mask\"][0] + encodings[\"attention_mask\"][1])\n",
    "positive_label_ids = torch.tensor([-100]*len(encodings[\"input_ids\"][0]) + encodings[\"input_ids\"][1])\n",
    "outputs = model(input_ids=positive_input_ids,\n",
    "                attention_mask=positive_attention_mask,\n",
    "                labels=positive_label_ids)\n",
    "positive_extension_likelihood = -1*outputs.loss\n",
    "print(\"\\nLog-likelihood of positive extension = \", positive_extension_likelihood.item())\n",
    "\n",
    "negative_input_ids = torch.tensor(encodings[\"input_ids\"][0] + encodings[\"input_ids\"][2])\n",
    "negative_attention_mask = torch.tensor(encodings[\"attention_mask\"][0] + encodings[\"attention_mask\"][2])\n",
    "negative_label_ids = torch.tensor([-100]*len(encodings[\"input_ids\"][0]) + encodings[\"input_ids\"][2])\n",
    "outputs = model(input_ids=negative_input_ids,\n",
    "                attention_mask=negative_attention_mask,\n",
    "                labels=negative_label_ids)\n",
    "negative_extension_likelihood = -1*outputs.loss\n",
    "print(\"\\nLog-likelihood of negative extension = \", negative_extension_likelihood.item())\n",
    "\n",
    "if (positive_extension_likelihood.item() > negative_extension_likelihood.item()):\n",
    "    print(\"\\nPositive text-extension has greater likelihood probabilities!\")\n",
    "    print(\"The given review can be predicted to be POSITIVE ðŸ‘\")\n",
    "else:\n",
    "    print(\"\\nNegative text-extension has greater likelihood probabilities!\")\n",
    "    print(\"The given review can be predicted to be NEGATIVE ðŸ‘Ž\")\n",
    "# @markdown **NOTE:** *Run this cell after setting all the fields appropriately!*\n",
    "\n",
    "# @markdown **NOTE:** *Some pre-trained models might not work well with longer texts!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus 2: Light-weight fine-tuning\n",
    "\n",
    "*Time estimate: ~10mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: Fine-tuning\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')  \n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'buZLOKdf7Qw'), ('Bilibili', 'BV1CU4y1n7bV')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Fine-tuning these large pre-trained models with billions of parameters tends to be very slow. In this section, we will explore the effect of fine-tuning a few layers (while fixing the others) to save training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The HuggingFace python library provides a simplified API for training and fine-tuning transformer language models. In this exercise we will fine-tune a pre-trained language model for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus 2.1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Pre-trained transformer models have a fixed vocabulary of words and sub-words. The input text to a transformer model has to be tokenized into these words and sub-words during the pre-processing stage. We'll use the HuggingFace `tokenizers` to perform the tokenization here.\n",
    "\n",
    "(By default we'll use the BERT base-cased pre-trained language model here. You can try using one of the other models available [here](https://huggingface.co/transformers/pretrained_models.html) by changing the model ID values at appropriate places in the code.)\n",
    "\n",
    "Most of the pre-trained language models have a fixed maximum sequence length. With the HuggingFace `tokenizer` library, we can either pad or truncate input text sequences to maximum length with a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Tokenize the input texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "  \"\"\"\n",
    "  Tokenises incoming sequences;\n",
    "\n",
    "  Args:\n",
    "    examples: Sequence of strings\n",
    "      Sequences to tokenise\n",
    "\n",
    "  Returns:\n",
    "    Returns transformer autotokenizer object with padded, truncated input sequences.\n",
    "  \"\"\"\n",
    "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Here we use the `DATASET` as defined above.\n",
    "# Recall that DATASET = load_dataset(\"yelp_review_full\", ignore_verifications=True)\n",
    "tokenized_datasets = DATASET.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We'll randomly sample a subset of the [Yelp reviews dataset](https://huggingface.co/datasets/yelp_review_full) (10k train samples, 5k samples for validation & testing each). You can include more samples here for better performance (at the cost of longer training times!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Select the data splits\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=SEED).select(range(10000))\n",
    "test_dataset = tokenized_datasets[\"test\"].select(range(0, 2500))\n",
    "validation_dataset = tokenized_datasets[\"test\"].select(range(2500, 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus 2.2: Model Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Next, we'll load a pre-trained checkpoint of the model and decide which layers are to be fine-tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Modify the `train_layers` variable below to pick which layers you would like to fine-tune (you can uncomment the print statements for this). Fine-tuning more layers might result in better performance (at the cost of longer training times). Due to computational limitations (limited GPU memory) we cannot fine-tune the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and freeze layers\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",\n",
    "                                                           cache_dir=\"data/\",\n",
    "                                                           num_labels=5)\n",
    "train_layers = [\"classifier\", \"bert.pooler\", \"bert.encoder.layer.11\"]  # add/remove layers here (use layer-name sub-strings)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "  if any(x in name for x in train_layers):\n",
    "    param.requires_grad = True\n",
    "    # print(\"FINE-TUNING -->\", name)\n",
    "  else:\n",
    "    param.requires_grad = False\n",
    "    # print(\"FROZEN -->\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus 2.3: Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Fine-tune the model! The HuggingFace `Trainer` class supports easy fine-tuning and logging. You can play around with various hyperparameters here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Setup huggingface trainer\n",
    "training_args = TrainingArguments(output_dir=\"yelp_bert\",\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=32,\n",
    "                                  per_device_eval_batch_size=32,\n",
    "                                  learning_rate=5e-5,\n",
    "                                  weight_decay=0.0,\n",
    "                                  num_train_epochs=1,  # students may use 5 to see a full training!\n",
    "                                  fp16=False if DEVICE=='cpu' else True,\n",
    "                                  save_steps=50,\n",
    "                                  logging_steps=10,\n",
    "                                  report_to=\"tensorboard\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We'll use `Accuracy` as the evaluation metric for the sentiment classification task. The HuggingFace `datasets` library supports various metrics. You can try experimenting with other classification metrics here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Setup evaluation metric\n",
    "def compute_metrics(eval_pred):\n",
    "  \"\"\"\n",
    "  Computes accuracy of the prediction\n",
    "\n",
    "  Args:\n",
    "    eval_pred: Tuple\n",
    "      Logits predicted by the model vs actual labels\n",
    "\n",
    "  Returns:\n",
    "    Dictionary containing accuracy of the prediction\n",
    "  \"\"\"\n",
    "  metric = datasets.load_metric(\"accuracy\")\n",
    "  logits, labels = eval_pred\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  accuracy = metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "  return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Start the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Instantiate a trainer with training and validation datasets\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "if DEVICE != 'cpu':\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "if DEVICE != 'cpu':\n",
    "  trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now visualize the `Tensorboard` logs to analyze the training process! The HuggingFace `Trainer` class will log various loss values and evaluation metrics automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Visualize the tensorboard logs\n",
    "if DEVICE != 'cpu':\n",
    "  %tensorboard --logdir yelp_bert/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus 3: Model robustness\n",
    "\n",
    "*Time estimate: ~22mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: Robustness\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import YouTubeVideo\n",
    "from IPython.display import IFrame\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "class PlayVideo(IFrame):\n",
    "  def __init__(self, id, source, page=1, width=400, height=300, **kwargs):\n",
    "    self.id = id\n",
    "    if source == 'Bilibili':\n",
    "      src = f'https://player.bilibili.com/player.html?bvid={id}&page={page}'\n",
    "    elif source == 'Osf':\n",
    "      src = f'https://mfr.ca-1.osf.io/render?url=https://osf.io/download/{id}/?direct%26mode=render'\n",
    "    super(PlayVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "\n",
    "def display_videos(video_ids, W=400, H=300, fs=1):\n",
    "  tab_contents = []\n",
    "  for i, video_id in enumerate(video_ids):\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "      if video_ids[i][0] == 'Youtube':\n",
    "        video = YouTubeVideo(id=video_ids[i][1], width=W,\n",
    "                             height=H, fs=fs, rel=0)\n",
    "        print(f'Video available at https://youtube.com/watch?v={video.id}')\n",
    "      else:\n",
    "        video = PlayVideo(id=video_ids[i][1], source=video_ids[i][0], width=W,\n",
    "                          height=H, fs=fs, autoplay=False)\n",
    "        if video_ids[i][0] == 'Bilibili':\n",
    "          print(f'Video available at https://www.bilibili.com/video/{video.id}')\n",
    "        elif video_ids[i][0] == 'Osf':\n",
    "          print(f'Video available at https://osf.io/{video.id}')  \n",
    "      display(video)\n",
    "    tab_contents.append(out)\n",
    "  return tab_contents\n",
    "\n",
    "\n",
    "video_ids = [('Youtube', 'hJdV2L2t4-c'), ('Bilibili', 'BV1Y54y1E77J')]\n",
    "tab_contents = display_videos(video_ids, W=854, H=480)\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = tab_contents\n",
    "for i in range(len(tab_contents)):\n",
    "  tabs.set_title(i, video_ids[i][0])\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Given the previously trained model for sentiment classification, it is possible to deceive it using various text perturbations. The text perturbations can act as previously unseen noise to the model, which might persuade it to impart wrong values of sentiment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Interactive Demo 3: Break the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus 3.1: Load an original review\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean up text\n",
    "\n",
    "    Args:\n",
    "      text: String\n",
    "        Input text sequence\n",
    "\n",
    "    Returns:\n",
    "      text: String\n",
    "        Returned string does not contain characters new-line characters, backslashes etc.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\\\n\", \" \")\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    return text\n",
    "\n",
    "# @markdown ---\n",
    "sample_review_from_yelp = \"Sentiment = 4\" #@param [\"Sentiment = 0\", \"Sentiment = 1\", \"Sentiment = 2\", \"Sentiment = 3\", \"Sentiment = 4\"]\n",
    "# @markdown **Randomly sample a response from the Yelp review dataset with the given sentiment value {0:ðŸ˜ , 1:ðŸ˜¦, 2:ðŸ˜, 3:ðŸ™‚, 4:ðŸ˜€}**\n",
    "\n",
    "# @markdown ---\n",
    "\n",
    "context = clean_text(sentiment_dict[sample_review_from_yelp][random.randint(0,len(sentiment_dict[sample_review_from_yelp])-1)][\"text\"])\n",
    "\n",
    "print(\"Review for \", sample_review_from_yelp, \":\\n\")\n",
    "pprint(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can apply various text perturbations to the selected review using the `textattack` python library. This will help us augment the original text to break the model!\n",
    "\n",
    "**Important:** Locally or on colab (with `!`) you can simple\n",
    "\n",
    "```bash\n",
    "pip install textattack --quiet\n",
    "```\n",
    "\n",
    "Then, import the packages:\n",
    "\n",
    "```python\n",
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapQWERTY\n",
    "from textattack.transformations import WordSwapExtend\n",
    "from textattack.transformations import WordSwapContract\n",
    "from textattack.transformations import WordSwapHomoglyphSwap\n",
    "from textattack.transformations import CompositeTransformation\n",
    "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "from textattack.transformations import WordSwapNeighboringCharacterSwap\n",
    "from textattack.transformations import WordSwapRandomCharacterInsertion\n",
    "from textattack.transformations import WordSwapRandomCharacterSubstitution\n",
    "```\n",
    "\n",
    "However, as we faced issues, you can run the cell below to load all necessary classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Helper functions to avoid `textattack` issue\n",
    "!pip install flair --quiet\n",
    "\n",
    "import flair\n",
    "from collections import OrderedDict\n",
    "from flair.data import Sentence\n",
    "\n",
    "\"\"\"\n",
    "Word Swap\n",
    "-------------------------------\n",
    "Word swap transformations act by\n",
    "replacing some words in the input.\n",
    "Subclasses can implement the abstract WordSwap class by\n",
    "overriding self._get_replacement_words\n",
    "\"\"\"\n",
    "\n",
    "def default_class_repr(self):\n",
    "    \"\"\"\n",
    "    Formats given input\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      Formatted string with additional parameters\n",
    "    \"\"\"\n",
    "    if hasattr(self, \"extra_repr_keys\"):\n",
    "        extra_params = []\n",
    "        for key in self.extra_repr_keys():\n",
    "            extra_params.append(\"  (\" + key + \")\" + \":  {\" + key + \"}\")\n",
    "        if len(extra_params):\n",
    "            extra_str = \"\\n\" + \"\\n\".join(extra_params) + \"\\n\"\n",
    "            extra_str = f\"({extra_str})\"\n",
    "        else:\n",
    "            extra_str = \"\"\n",
    "        extra_str = extra_str.format(**self.__dict__)\n",
    "    else:\n",
    "        extra_str = \"\"\n",
    "    return f\"{self.__class__.__name__}{extra_str}\"\n",
    "\n",
    "\n",
    "LABEL_COLORS = [\n",
    "    \"red\",\n",
    "    \"green\",\n",
    "    \"blue\",\n",
    "    \"purple\",\n",
    "    \"yellow\",\n",
    "    \"orange\",\n",
    "    \"pink\",\n",
    "    \"cyan\",\n",
    "    \"gray\",\n",
    "    \"brown\",\n",
    "]\n",
    "\n",
    "\n",
    "class Transformation(ABC):\n",
    "    \"\"\"\n",
    "    An abstract class for transforming a sequence of text to produce a\n",
    "    potential adversarial example.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        current_text,\n",
    "        pre_transformation_constraints=[],\n",
    "        indices_to_modify=None,\n",
    "        shifted_idxs=False,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Applies the pre_transformation_constraints then calls\n",
    "        _get_transformations.\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object to transform.\n",
    "          pre_transformation_constraints: List\n",
    "            The PreTransformationConstraint to apply for cross-checking transformation compatibility.\n",
    "          indices_to_modify: Integer\n",
    "            Word indices to be modified as dictated by the SearchMethod.\n",
    "          shifted_idxs: Boolean\n",
    "            Indicates whether indices could be shifted from their original position in the text.\n",
    "\n",
    "        Returns:\n",
    "          transformed_texts: List\n",
    "            Returns a list of all possible transformations for current_text.\n",
    "        \"\"\"\n",
    "        if indices_to_modify is None:\n",
    "            indices_to_modify = set(range(len(current_text.words)))\n",
    "            # If we are modifying all indices, we don't care if some of the indices might have been shifted.\n",
    "            shifted_idxs = False\n",
    "        else:\n",
    "            indices_to_modify = set(indices_to_modify)\n",
    "\n",
    "        if shifted_idxs:\n",
    "            indices_to_modify = set(\n",
    "                current_text.convert_from_original_idxs(indices_to_modify)\n",
    "            )\n",
    "\n",
    "        for constraint in pre_transformation_constraints:\n",
    "            indices_to_modify = indices_to_modify & constraint(current_text, self)\n",
    "        transformed_texts = self._get_transformations(current_text, indices_to_modify)\n",
    "        for text in transformed_texts:\n",
    "            text.attack_attrs[\"last_transformation\"] = self\n",
    "        return transformed_texts\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_transformations(self, current_text, indices_to_modify):\n",
    "        \"\"\"\n",
    "        Returns a list of all possible transformations for current_text,\n",
    "        only modifying indices_to_modify.\n",
    "        Must be overridden by specific transformations.\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object to transform.\n",
    "          indicies_to_modify: Integer\n",
    "            Specifies word indices which can be modified.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @property\n",
    "    def deterministic(self):\n",
    "        return True\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        return []\n",
    "\n",
    "    __repr__ = __str__ = default_class_repr\n",
    "\n",
    "\n",
    "class WordSwap(Transformation):\n",
    "    \"\"\"\n",
    "    An abstract class that takes a sentence and transforms it by replacing\n",
    "    some of its words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, letters_to_insert=None):\n",
    "        \"\"\"\n",
    "        Initializes following attributes\n",
    "\n",
    "        Args:\n",
    "          letters_to_insert: String\n",
    "            Letters allowed for insertion into words (used by some char-based transformations)\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        self.letters_to_insert = letters_to_insert\n",
    "        if not self.letters_to_insert:\n",
    "            self.letters_to_insert = string.ascii_letters\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Returns a set of replacements given an input word.\n",
    "        Must be overriden by specific word swap transformations.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            The input word for which replacements are to be found.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_random_letter(self):\n",
    "        \"\"\"\n",
    "        Helper function that returns a random single letter from the English\n",
    "        alphabet that could be lowercase or uppercase.\n",
    "\n",
    "        Args:\n",
    "          None\n",
    "\n",
    "        Returns:\n",
    "          Random Single Letter to simulate random-letter transformation\n",
    "        \"\"\"\n",
    "        return random.choice(self.letters_to_insert)\n",
    "\n",
    "    def _get_transformations(self, current_text, indices_to_modify):\n",
    "        \"\"\"\n",
    "        Returns a list of all possible transformations for current_text,\n",
    "        only modifying indices_to_modify.\n",
    "        Must be overridden by specific transformations.\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object to transform.\n",
    "          indicies_to_modify: Integer\n",
    "            Which word indices can be modified.\n",
    "\n",
    "        Returns:\n",
    "          transformed_texts: List\n",
    "            List of all transformed texts i.e., index at which transformation was applied\n",
    "        \"\"\"\n",
    "        words = current_text.words\n",
    "        transformed_texts = []\n",
    "\n",
    "        for i in indices_to_modify:\n",
    "            word_to_replace = words[i]\n",
    "            replacement_words = self._get_replacement_words(word_to_replace)\n",
    "            transformed_texts_idx = []\n",
    "            for r in replacement_words:\n",
    "                if r == word_to_replace:\n",
    "                    continue\n",
    "                transformed_texts_idx.append(current_text.replace_word_at_index(i, r))\n",
    "            transformed_texts.extend(transformed_texts_idx)\n",
    "\n",
    "        return transformed_texts\n",
    "\n",
    "\n",
    "class WordSwapQWERTY(WordSwap):\n",
    "    \"\"\"\n",
    "    A transformation that swaps characters with adjacent keys on a\n",
    "    QWERTY keyboard, replicating the kind of errors that come from typing\n",
    "    too quickly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, random_one=True, skip_first_char=False, skip_last_char=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes\n",
    "\n",
    "        Args:\n",
    "          random_one: Boolean\n",
    "            Specifies whether to return a single (random) swap, or all possible swaps.\n",
    "          skip_first_char: Boolean\n",
    "            When True, do not modify the first character of each word.\n",
    "          skip_last_char: Boolean\n",
    "            When True, do not modify the last character of each word.\n",
    "\n",
    "        Usage/Example:\n",
    "          >>> from textattack.transformations import WordSwapQWERTY\n",
    "          >>> from textattack.augmentation import Augmenter\n",
    "          >>> transformation = WordSwapQWERT()\n",
    "          >>> augmenter = Augmenter(transformation=transformation)\n",
    "          >>> s = 'I am fabulous.'\n",
    "          >>> augmenter.augment(s)\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.random_one = random_one\n",
    "        self.skip_first_char = skip_first_char\n",
    "        self.skip_last_char = skip_last_char\n",
    "\n",
    "        self._keyboard_adjacency = {\n",
    "            \"q\": [\n",
    "                \"w\",\n",
    "                \"a\",\n",
    "                \"s\",\n",
    "            ],\n",
    "            \"w\": [\"q\", \"e\", \"a\", \"s\", \"d\"],\n",
    "            \"e\": [\"w\", \"s\", \"d\", \"f\", \"r\"],\n",
    "            \"r\": [\"e\", \"d\", \"f\", \"g\", \"t\"],\n",
    "            \"t\": [\"r\", \"f\", \"g\", \"h\", \"y\"],\n",
    "            \"y\": [\"t\", \"g\", \"h\", \"j\", \"u\"],\n",
    "            \"u\": [\"y\", \"h\", \"j\", \"k\", \"i\"],\n",
    "            \"i\": [\"u\", \"j\", \"k\", \"l\", \"o\"],\n",
    "            \"o\": [\"i\", \"k\", \"l\", \"p\"],\n",
    "            \"p\": [\"o\", \"l\"],\n",
    "            \"a\": [\"q\", \"w\", \"s\", \"z\", \"x\"],\n",
    "            \"s\": [\"q\", \"w\", \"e\", \"a\", \"d\", \"z\", \"x\"],\n",
    "            \"d\": [\"w\", \"e\", \"r\", \"f\", \"c\", \"x\", \"s\"],\n",
    "            \"f\": [\"e\", \"r\", \"t\", \"g\", \"v\", \"c\", \"d\"],\n",
    "            \"g\": [\"r\", \"t\", \"y\", \"h\", \"b\", \"v\", \"d\"],\n",
    "            \"h\": [\"t\", \"y\", \"u\", \"g\", \"j\", \"b\", \"n\"],\n",
    "            \"j\": [\"y\", \"u\", \"i\", \"k\", \"m\", \"n\", \"h\"],\n",
    "            \"k\": [\"u\", \"i\", \"o\", \"l\", \"m\", \"j\"],\n",
    "            \"l\": [\"i\", \"o\", \"p\", \"k\"],\n",
    "            \"z\": [\"a\", \"s\", \"x\"],\n",
    "            \"x\": [\"s\", \"d\", \"z\", \"c\"],\n",
    "            \"c\": [\"x\", \"d\", \"f\", \"v\"],\n",
    "            \"v\": [\"c\", \"f\", \"g\", \"b\"],\n",
    "            \"b\": [\"v\", \"g\", \"h\", \"n\"],\n",
    "            \"n\": [\"b\", \"h\", \"j\", \"m\"],\n",
    "            \"m\": [\"n\", \"j\", \"k\"],\n",
    "        }\n",
    "\n",
    "    def _get_adjacent(self, s):\n",
    "        \"\"\"\n",
    "        Helper function to extract keys adjacent to given input key\n",
    "\n",
    "        Args:\n",
    "          s: String\n",
    "            Letter for which adjacent keys are to be queried\n",
    "\n",
    "        Returns:\n",
    "          adjacent_keys: List\n",
    "            List of co-occuring keys with respect to input\n",
    "        \"\"\"\n",
    "        s_lower = s.lower()\n",
    "        if s_lower in self._keyboard_adjacency:\n",
    "            adjacent_keys = self._keyboard_adjacency.get(s_lower, [])\n",
    "            if s.isupper():\n",
    "                return [key.upper() for key in adjacent_keys]\n",
    "            else:\n",
    "                return adjacent_keys\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Helper function to find candidate words with respect to given input key.\n",
    "        Candidate words are words selected based on nearest neighbors\n",
    "        with scope for subsequent swapping.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            Word for which candidate words are to be generated.\n",
    "\n",
    "        Returns:\n",
    "          candidate_words: List\n",
    "            List of candidate words with respect to input word.\n",
    "        \"\"\"\n",
    "        if len(word) <= 1:\n",
    "            return []\n",
    "\n",
    "        candidate_words = []\n",
    "\n",
    "        start_idx = 1 if self.skip_first_char else 0\n",
    "        end_idx = len(word) - (1 + self.skip_last_char)\n",
    "\n",
    "        if start_idx >= end_idx:\n",
    "            return []\n",
    "\n",
    "        if self.random_one:\n",
    "            i = random.randrange(start_idx, end_idx + 1)\n",
    "            candidate_word = (\n",
    "                word[:i] + random.choice(self._get_adjacent(word[i])) + word[i + 1 :]\n",
    "            )\n",
    "            candidate_words.append(candidate_word)\n",
    "        else:\n",
    "            for i in range(start_idx, end_idx + 1):\n",
    "                for swap_key in self._get_adjacent(word[i]):\n",
    "                    candidate_word = word[:i] + swap_key + word[i + 1 :]\n",
    "                    candidate_words.append(candidate_word)\n",
    "\n",
    "        return candidate_words\n",
    "\n",
    "    @property\n",
    "    def deterministic(self):\n",
    "        return not self.random_one\n",
    "\n",
    "\n",
    "EXTENSION_MAP = {\"ain't\": \"isn't\", \"aren't\": 'are not', \"can't\": 'cannot', \"can't've\": 'cannot have', \"could've\": 'could have', \"couldn't\": 'could not', \"didn't\": 'did not', \"doesn't\": 'does not', \"don't\": 'do not', \"hadn't\": 'had not', \"hasn't\": 'has not', \"haven't\": 'have not', \"he'd\": 'he would', \"he'd've\": 'he would have', \"he'll\": 'he will', \"he's\": 'he is', \"how'd\": 'how did', \"how'd'y\": 'how do you', \"how'll\": 'how will', \"how's\": 'how is', \"I'd\": 'I would', \"I'll\": 'I will', \"I'm\": 'I am', \"I've\": 'I have', \"i'd\": 'i would', \"i'll\": 'i will', \"i'm\": 'i am', \"i've\": 'i have', \"isn't\": 'is not', \"it'd\": 'it would', \"it'll\": 'it will', \"it's\": 'it is', \"ma'am\": 'madam', \"might've\": 'might have', \"mightn't\": 'might not', \"must've\": 'must have', \"mustn't\": 'must not', \"needn't\": 'need not', \"oughtn't\": 'ought not', \"shan't\": 'shall not', \"she'd\": 'she would', \"she'll\": 'she will', \"she's\": 'she is', \"should've\": 'should have', \"shouldn't\": 'should not', \"that'd\": 'that would', \"that's\": 'that is', \"there'd\": 'there would', \"there's\": 'there is', \"they'd\": 'they would', \"they'll\": 'they will', \"they're\": 'they are', \"they've\": 'they have', \"wasn't\": 'was not', \"we'd\": 'we would', \"we'll\": 'we will', \"we're\": 'we are', \"we've\": 'we have', \"weren't\": 'were not', \"what're\": 'what are', \"what's\": 'what is', \"when's\": 'when is', \"where'd\": 'where did', \"where's\": 'where is', \"where've\": 'where have', \"who'll\": 'who will', \"who's\": 'who is', \"who've\": 'who have', \"why's\": 'why is', \"won't\": 'will not', \"would've\": 'would have', \"wouldn't\": 'would not', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you're\": 'you are', \"you've\": 'you have'}\n",
    "\n",
    "\n",
    "class WordSwap(Transformation):\n",
    "    \"\"\"\n",
    "    An abstract class that takes a sentence and transforms it by replacing\n",
    "    some of its words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, letters_to_insert=None):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes\n",
    "\n",
    "        Args:\n",
    "          letters_to_insert: String\n",
    "            Letters allowed for insertion into words\n",
    "            (used by some char-based transformations)\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        self.letters_to_insert = letters_to_insert\n",
    "        if not self.letters_to_insert:\n",
    "            self.letters_to_insert = string.ascii_letters\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Returns a set of replacements given an input word.\n",
    "        Must be overridden by specific word swap transformations.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            The input word to find replacements for.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _get_random_letter(self):\n",
    "        \"\"\"\n",
    "        Helper function that returns a random single letter from the English\n",
    "        alphabet that could be lowercase or uppercase.\n",
    "\n",
    "        Args:\n",
    "          None\n",
    "\n",
    "        Returns:\n",
    "          Random single letter for random-letter transformation\n",
    "        \"\"\"\n",
    "        return random.choice(self.letters_to_insert)\n",
    "\n",
    "    def _get_transformations(self, current_text, indices_to_modify):\n",
    "        \"\"\"\n",
    "        Returns a list of all possible transformations for current_text,\n",
    "        only modifying indices_to_modify.\n",
    "        Must be overridden by specific transformations.\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object to transform.\n",
    "          indicies_to_modify: Integer\n",
    "            Which word indices can be modified.\n",
    "\n",
    "        Returns:\n",
    "          transformed_texts: List\n",
    "            List of all transformed texts with indexes at which transformation was applied\n",
    "        \"\"\"\n",
    "        words = current_text.words\n",
    "        transformed_texts = []\n",
    "\n",
    "        for i in indices_to_modify:\n",
    "            word_to_replace = words[i]\n",
    "            replacement_words = self._get_replacement_words(word_to_replace)\n",
    "            transformed_texts_idx = []\n",
    "            for r in replacement_words:\n",
    "                if r == word_to_replace:\n",
    "                    continue\n",
    "                transformed_texts_idx.append(current_text.replace_word_at_index(i, r))\n",
    "            transformed_texts.extend(transformed_texts_idx)\n",
    "\n",
    "        return transformed_texts\n",
    "\n",
    "\n",
    "class WordSwapExtend(WordSwap):\n",
    "    \"\"\"\n",
    "    Transforms an input by performing extension on recognized\n",
    "    combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    def _get_transformations(self, current_text, indices_to_modify):\n",
    "        \"\"\"\n",
    "        Return all possible transformed sentences, each with one extension.\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object to transform.\n",
    "          indicies_to_modify: Integer\n",
    "            Which word indices can be modified.\n",
    "\n",
    "        Returns:\n",
    "          transformed_texts: List\n",
    "            List of all transformed texts based on extension map\n",
    "\n",
    "        Usage/Examples:\n",
    "        >>> from textattack.transformations import WordSwapExtend\n",
    "        >>> from textattack.augmentation import Augmenter\n",
    "        >>> transformation = WordSwapExtend()\n",
    "        >>> augmenter = Augmenter(transformation=transformation)\n",
    "        >>> s = '''I'm fabulous'''\n",
    "        >>> augmenter.augment(s)\n",
    "        \"\"\"\n",
    "        transformed_texts = []\n",
    "        words = current_text.words\n",
    "        for idx in indices_to_modify:\n",
    "            word = words[idx]\n",
    "            # expend when word in map\n",
    "            if word in EXTENSION_MAP:\n",
    "                expanded = EXTENSION_MAP[word]\n",
    "                transformed_text = current_text.replace_word_at_index(idx, expanded)\n",
    "                transformed_texts.append(transformed_text)\n",
    "\n",
    "        return transformed_texts\n",
    "\n",
    "\n",
    "class WordSwapContract(WordSwap):\n",
    "    \"\"\"\n",
    "    Transforms an input by performing contraction on recognized\n",
    "    combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    reverse_contraction_map = {v: k for k, v in EXTENSION_MAP.items()}\n",
    "\n",
    "    def _get_transformations(self, current_text, indices_to_modify):\n",
    "        \"\"\"\n",
    "        Return all possible transformed sentences, each with one\n",
    "        contraction.\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object to transform.\n",
    "          indicies_to_modify: Integer\n",
    "            Which word indices can be modified.\n",
    "\n",
    "        Returns:\n",
    "          transformed_texts: List\n",
    "            List of all transformed texts based on reverse contraction map\n",
    "\n",
    "        Usage/Example:\n",
    "        >>> from textattack.transformations import WordSwapContract\n",
    "        >>> from textattack.augmentation import Augmenter\n",
    "        >>> transformation = WordSwapContract()\n",
    "        >>> augmenter = Augmenter(transformation=transformation)\n",
    "        >>> s = 'I am 12 years old.'\n",
    "        >>> augmenter.augment(s)\n",
    "        \"\"\"\n",
    "        transformed_texts = []\n",
    "\n",
    "        words = current_text.words\n",
    "        indices_to_modify = sorted(indices_to_modify)\n",
    "\n",
    "        # search for every 2-words combination in reverse_contraction_map\n",
    "        for idx, word_idx in enumerate(indices_to_modify[:-1]):\n",
    "            next_idx = indices_to_modify[idx + 1]\n",
    "            if (idx + 1) != next_idx:\n",
    "                continue\n",
    "            word = words[word_idx]\n",
    "            next_word = words[next_idx]\n",
    "\n",
    "            # generating the words to search for\n",
    "            key = \" \".join([word, next_word])\n",
    "\n",
    "            # when a possible contraction is found in map, contract the current text\n",
    "            if key in self.reverse_contraction_map:\n",
    "                transformed_text = current_text.replace_word_at_index(\n",
    "                    idx, self.reverse_contraction_map[key]\n",
    "                )\n",
    "                transformed_text = transformed_text.delete_word_at_index(next_idx)\n",
    "                transformed_texts.append(transformed_text)\n",
    "\n",
    "        return transformed_texts\n",
    "\n",
    "\n",
    "class WordSwapHomoglyphSwap(WordSwap):\n",
    "    \"\"\"\n",
    "    Transforms an input by replacing its words with visually similar words\n",
    "    using homoglyph swaps.\n",
    "    A homoglyph is one of two or more graphemes, characters, or glyphs\n",
    "    with shapes that appear identical or very similar.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_one=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes\n",
    "\n",
    "        Args:\n",
    "          random_one: Boolean\n",
    "            Choosing random substring for transformation\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "\n",
    "        Usage/Examples:\n",
    "          >>> from textattack.transformations import WordSwapHomoglyphSwap\n",
    "          >>> from textattack.augmentation import Augmenter\n",
    "          >>> transformation = WordSwapHomoglyphSwap()\n",
    "          >>> augmenter = Augmenter(transformation=transformation)\n",
    "          >>> s = 'I am fabulous.'\n",
    "          >>> augmenter.augment(s)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.homos = {\n",
    "            \"-\": \"Ë—\",\n",
    "            \"9\": \"à§­\",\n",
    "            \"8\": \"È¢\",\n",
    "            \"7\": \"ðŸ•\",\n",
    "            \"6\": \"Ð±\",\n",
    "            \"5\": \"Æ¼\",\n",
    "            \"4\": \"áŽ\",\n",
    "            \"3\": \"Æ·\",\n",
    "            \"2\": \"á’¿\",\n",
    "            \"1\": \"l\",\n",
    "            \"0\": \"O\",\n",
    "            \"'\": \"`\",\n",
    "            \"a\": \"É‘\",\n",
    "            \"b\": \"Ð¬\",\n",
    "            \"c\": \"Ï²\",\n",
    "            \"d\": \"Ô\",\n",
    "            \"e\": \"Ðµ\",\n",
    "            \"f\": \"ðš\",\n",
    "            \"g\": \"É¡\",\n",
    "            \"h\": \"Õ°\",\n",
    "            \"i\": \"Ñ–\",\n",
    "            \"j\": \"Ï³\",\n",
    "            \"k\": \"ð’Œ\",\n",
    "            \"l\": \"â…¼\",\n",
    "            \"m\": \"ï½\",\n",
    "            \"n\": \"Õ¸\",\n",
    "            \"o\": \"Ð¾\",\n",
    "            \"p\": \"Ñ€\",\n",
    "            \"q\": \"Ô›\",\n",
    "            \"r\": \"â²…\",\n",
    "            \"s\": \"Ñ•\",\n",
    "            \"t\": \"ðš\",\n",
    "            \"u\": \"Õ½\",\n",
    "            \"v\": \"Ñµ\",\n",
    "            \"w\": \"Ô\",\n",
    "            \"x\": \"Ã—\",\n",
    "            \"y\": \"Ñƒ\",\n",
    "            \"z\": \"á´¢\",\n",
    "        }\n",
    "        self.random_one = random_one\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list containing all possible words with 1 character\n",
    "        replaced by a homoglyph.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            Word for which homoglyphs are to be generated.\n",
    "\n",
    "        Returns:\n",
    "          candidate_words: List\n",
    "            List of homoglyphs with respect to input word.\n",
    "        \"\"\"\n",
    "        candidate_words = []\n",
    "\n",
    "        if self.random_one:\n",
    "            i = np.random.randint(0, len(word))\n",
    "            if word[i] in self.homos:\n",
    "                repl_letter = self.homos[word[i]]\n",
    "                candidate_word = word[:i] + repl_letter + word[i + 1 :]\n",
    "                candidate_words.append(candidate_word)\n",
    "        else:\n",
    "            for i in range(len(word)):\n",
    "                if word[i] in self.homos:\n",
    "                    repl_letter = self.homos[word[i]]\n",
    "                    candidate_word = word[:i] + repl_letter + word[i + 1 :]\n",
    "                    candidate_words.append(candidate_word)\n",
    "\n",
    "        return candidate_words\n",
    "\n",
    "    @property\n",
    "    def deterministic(self):\n",
    "        return not self.random_one\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        return super().extra_repr_keys()\n",
    "\n",
    "\n",
    "class WordSwapRandomCharacterDeletion(WordSwap):\n",
    "    \"\"\"\n",
    "    Transforms an input by deleting its characters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, random_one=True, skip_first_char=False, skip_last_char=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initiates the following parameters:\n",
    "\n",
    "        Args:\n",
    "          random_one: Boolean\n",
    "            Whether to return a single word with a random\n",
    "            character deleted. If not, returns all possible options.\n",
    "          skip_first_char: Boolean\n",
    "            Whether to disregard deleting the first character.\n",
    "          skip_last_char: Boolean\n",
    "            Whether to disregard deleting the last character.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "\n",
    "        Usage/Example:\n",
    "          >>> from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "          >>> from textattack.augmentation import Augmenter\n",
    "          >>> transformation = WordSwapRandomCharacterDeletion()\n",
    "          >>> augmenter = Augmenter(transformation=transformation)\n",
    "          >>> s = 'I am fabulous.'\n",
    "          >>> augmenter.augment(s)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.random_one = random_one\n",
    "        self.skip_first_char = skip_first_char\n",
    "        self.skip_last_char = skip_last_char\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list containing all possible words with 1 letter\n",
    "        deleted.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            The input word to find replacements for.\n",
    "\n",
    "        Returns:\n",
    "          candidate_words: List\n",
    "            List of candidate words with single letter deletion\n",
    "        \"\"\"\n",
    "        if len(word) <= 1:\n",
    "            return []\n",
    "\n",
    "        candidate_words = []\n",
    "\n",
    "        start_idx = 1 if self.skip_first_char else 0\n",
    "        end_idx = (len(word) - 1) if self.skip_last_char else len(word)\n",
    "\n",
    "        if start_idx >= end_idx:\n",
    "            return []\n",
    "\n",
    "        if self.random_one:\n",
    "            i = np.random.randint(start_idx, end_idx)\n",
    "            candidate_word = word[:i] + word[i + 1 :]\n",
    "            candidate_words.append(candidate_word)\n",
    "        else:\n",
    "            for i in range(start_idx, end_idx):\n",
    "                candidate_word = word[:i] + word[i + 1 :]\n",
    "                candidate_words.append(candidate_word)\n",
    "\n",
    "        return candidate_words\n",
    "\n",
    "    @property\n",
    "    def deterministic(self):\n",
    "        return not self.random_one\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        return super().extra_repr_keys() + [\"random_one\"]\n",
    "\n",
    "\n",
    "class WordSwapNeighboringCharacterSwap(WordSwap):\n",
    "    \"\"\"\n",
    "    Transforms an input by replacing its words with a neighboring character\n",
    "    swap.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, random_one=True, skip_first_char=False, skip_last_char=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes\n",
    "\n",
    "        Args:\n",
    "          random_one: Boolean\n",
    "            Whether to return a single word with two characters\n",
    "            swapped. If not, returns all possible options.\n",
    "          skip_first_char: Boolean\n",
    "            Whether to disregard perturbing the first\n",
    "            character.\n",
    "          skip_last_char: Boolean\n",
    "            Whether to disregard perturbing the last\n",
    "            character.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "\n",
    "        Usage/Examples:\n",
    "          >>> from textattack.transformations import WordSwapNeighboringCharacterSwap\n",
    "          >>> from textattack.augmentation import Augmenter\n",
    "          >>> transformation = WordSwapNeighboringCharacterSwap()\n",
    "          >>> augmenter = Augmenter(transformation=transformation)\n",
    "          >>> s = 'I am fabulous.'\n",
    "          >>> augmenter.augment(s)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.random_one = random_one\n",
    "        self.skip_first_char = skip_first_char\n",
    "        self.skip_last_char = skip_last_char\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list containing all possible words with a single pair of\n",
    "        neighboring characters swapped.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            The input word to find replacements for.\n",
    "\n",
    "        Returns:\n",
    "          candidate_words: List\n",
    "            List of candidate words\n",
    "        \"\"\"\n",
    "\n",
    "        if len(word) <= 1:\n",
    "            return []\n",
    "\n",
    "        candidate_words = []\n",
    "\n",
    "        start_idx = 1 if self.skip_first_char else 0\n",
    "        end_idx = (len(word) - 2) if self.skip_last_char else (len(word) - 1)\n",
    "\n",
    "        if start_idx >= end_idx:\n",
    "            return []\n",
    "\n",
    "        if self.random_one:\n",
    "            i = np.random.randint(start_idx, end_idx)\n",
    "            candidate_word = word[:i] + word[i + 1] + word[i] + word[i + 2 :]\n",
    "            candidate_words.append(candidate_word)\n",
    "        else:\n",
    "            for i in range(start_idx, end_idx):\n",
    "                candidate_word = word[:i] + word[i + 1] + word[i] + word[i + 2 :]\n",
    "                candidate_words.append(candidate_word)\n",
    "\n",
    "        return candidate_words\n",
    "\n",
    "    @property\n",
    "    def deterministic(self):\n",
    "        return not self.random_one\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        return super().extra_repr_keys() + [\"random_one\"]\n",
    "\n",
    "\n",
    "class WordSwapRandomCharacterInsertion(WordSwap):\n",
    "    \"\"\"\n",
    "    Transforms an input by inserting a random character.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, random_one=True, skip_first_char=False, skip_last_char=False, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes\n",
    "\n",
    "        Args:\n",
    "          random_one: Boolean\n",
    "            Whether to return a single word with a random\n",
    "            character deleted. If not, returns all possible options.\n",
    "          skip_first_char: Boolean\n",
    "            Whether to disregard inserting as the first character.\n",
    "          skip_last_char: Boolean\n",
    "            Whether to disregard inserting as the last character.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "\n",
    "        Usage/Example:\n",
    "          >>> from textattack.transformations import WordSwapRandomCharacterInsertion\n",
    "          >>> from textattack.augmentation import Augmenter\n",
    "          >>> transformation = WordSwapRandomCharacterInsertion()\n",
    "          >>> augmenter = Augmenter(transformation=transformation)\n",
    "          >>> s = 'I am fabulous.'\n",
    "          >>> augmenter.augment(s)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.random_one = random_one\n",
    "        self.skip_first_char = skip_first_char\n",
    "        self.skip_last_char = skip_last_char\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list containing all possible words with 1 random\n",
    "        character inserted.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            The input word to find replacements for.\n",
    "\n",
    "        Returns:\n",
    "          candidate_words: List\n",
    "            List of candidate words with all possible words with 1 random\n",
    "            character inserted.\n",
    "        \"\"\"\n",
    "        if len(word) <= 1:\n",
    "            return []\n",
    "\n",
    "        candidate_words = []\n",
    "\n",
    "        start_idx = 1 if self.skip_first_char else 0\n",
    "        end_idx = (len(word) - 1) if self.skip_last_char else len(word)\n",
    "\n",
    "        if start_idx >= end_idx:\n",
    "            return []\n",
    "\n",
    "        if self.random_one:\n",
    "            i = np.random.randint(start_idx, end_idx)\n",
    "            candidate_word = word[:i] + self._get_random_letter() + word[i:]\n",
    "            candidate_words.append(candidate_word)\n",
    "        else:\n",
    "            for i in range(start_idx, end_idx):\n",
    "                candidate_word = word[:i] + self._get_random_letter() + word[i:]\n",
    "                candidate_words.append(candidate_word)\n",
    "\n",
    "        return candidate_words\n",
    "\n",
    "    @property\n",
    "    def deterministic(self):\n",
    "        return not self.random_one\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        return super().extra_repr_keys() + [\"random_one\"]\n",
    "\n",
    "\n",
    "class WordSwapRandomCharacterSubstitution(WordSwap):\n",
    "    \"\"\"\n",
    "    Transforms an input by replacing one character in a word with a random\n",
    "    new character.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, random_one=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes\n",
    "\n",
    "        Args:\n",
    "          random_one: Boolean\n",
    "            Whether to return a single word with a random\n",
    "            character deleted. If not set, returns all possible options.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "\n",
    "        Usage/Example:\n",
    "          >>> from textattack.transformations import WordSwapRandomCharacterSubstitution\n",
    "          >>> from textattack.augmentation import Augmenter\n",
    "          >>> transformation = WordSwapRandomCharacterSubstitution()\n",
    "          >>> augmenter = Augmenter(transformation=transformation)\n",
    "          >>> s = 'I am fabulous.'\n",
    "          >>> augmenter.augment(s)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.random_one = random_one\n",
    "\n",
    "    def _get_replacement_words(self, word):\n",
    "        \"\"\"\n",
    "        Returns a list containing all possible words with 1 letter\n",
    "        substituted for a random letter.\n",
    "\n",
    "        Args:\n",
    "          word: String\n",
    "            The input word to find replacements for.\n",
    "\n",
    "        Returns:\n",
    "          candidate_words: List\n",
    "            List of candidate words with combinations involving random substitution\n",
    "        \"\"\"\n",
    "        if len(word) <= 1:\n",
    "            return []\n",
    "\n",
    "        candidate_words = []\n",
    "\n",
    "        if self.random_one:\n",
    "            i = np.random.randint(0, len(word))\n",
    "            candidate_word = word[:i] + self._get_random_letter() + word[i + 1 :]\n",
    "            candidate_words.append(candidate_word)\n",
    "        else:\n",
    "            for i in range(len(word)):\n",
    "                candidate_word = word[:i] + self._get_random_letter() + word[i + 1 :]\n",
    "                candidate_words.append(candidate_word)\n",
    "\n",
    "        return candidate_words\n",
    "\n",
    "    @property\n",
    "    def deterministic(self):\n",
    "        return not self.random_one\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        return super().extra_repr_keys() + [\"random_one\"]\n",
    "\n",
    "\n",
    "class CompositeTransformation(Transformation):\n",
    "    \"\"\"\n",
    "    A transformation which applies each of a list of transformations,\n",
    "    returning a set of all optoins.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transformations):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes\n",
    "\n",
    "        Args:\n",
    "          transformations: List\n",
    "            The list of Transformation to apply.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        if not (\n",
    "            isinstance(transformations, list) or isinstance(transformations, tuple)\n",
    "        ):\n",
    "            raise TypeError(\"transformations must be list or tuple\")\n",
    "        elif not len(transformations):\n",
    "            raise ValueError(\"transformations cannot be empty\")\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def _get_transformations(self, *_):\n",
    "        \"\"\"\n",
    "        Placeholder method that would throw an error if a user tried to\n",
    "        treat the CompositeTransformation as a 'normal' transformation.\n",
    "\n",
    "        Args:\n",
    "          None\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        raise RuntimeError(\n",
    "            \"CompositeTransformation does not support _get_transformations().\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Generates new attacked texts based on different possible transformations\n",
    "\n",
    "        Args:\n",
    "          None\n",
    "\n",
    "        Returns:\n",
    "          new_attacked_texts: List\n",
    "            List of new attacked texts based on different possible transformations\n",
    "\n",
    "        \"\"\"\n",
    "        new_attacked_texts = set()\n",
    "        for transformation in self.transformations:\n",
    "            new_attacked_texts.update(transformation(*args, **kwargs))\n",
    "        return list(new_attacked_texts)\n",
    "\n",
    "    def __repr__(self):\n",
    "        main_str = \"CompositeTransformation\" + \"(\"\n",
    "        transformation_lines = []\n",
    "        for i, transformation in enumerate(self.transformations):\n",
    "            transformation_lines.append(utils.add_indent(f\"({i}): {transformation}\", 2))\n",
    "        transformation_lines.append(\")\")\n",
    "        main_str += utils.add_indent(\"\\n\" + \"\\n\".join(transformation_lines), 2)\n",
    "        return main_str\n",
    "\n",
    "    __str__ = __repr__\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "===================\n",
    "Augmenter Class\n",
    "===================\n",
    "\"\"\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PreTransformationConstraint(ABC):\n",
    "    \"\"\"\n",
    "    An abstract class that represents constraints which are applied before\n",
    "    the transformation.\n",
    "    These restrict which words are allowed to be modified during the\n",
    "    transformation. For example, we might not allow stopwords to be\n",
    "    modified.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, current_text, transformation):\n",
    "        \"\"\"\n",
    "        Returns the word indices in current_text which are able to be\n",
    "        modified. First checks compatibility with transformation then calls\n",
    "        _get_modifiable_indices\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object input to consider.\n",
    "          transformation: Transformation Object\n",
    "            The Transformation which will be applied.\n",
    "\n",
    "        Returns:\n",
    "          Modifiable indices of input if transformation is compatible\n",
    "          Words of current text otherwise\n",
    "        \"\"\"\n",
    "        if not self.check_compatibility(transformation):\n",
    "            return set(range(len(current_text.words)))\n",
    "        return self._get_modifiable_indices(current_text)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _get_modifiable_indices(current_text):\n",
    "        \"\"\"\n",
    "        Returns the word indices in current_text which are able to be\n",
    "        modified. Must be overridden by specific pre-transformation\n",
    "        constraints.\n",
    "\n",
    "        Args:\n",
    "          current_text: String\n",
    "            The AttackedText Object input to consider.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def check_compatibility(self, transformation):\n",
    "        \"\"\"\n",
    "        Checks if this constraint is compatible with the given\n",
    "        transformation. For example, the WordEmbeddingDistance constraint\n",
    "        compares the embedding of the word inserted with that of the word\n",
    "        deleted. Therefore it can only be applied in the case of word swaps,\n",
    "        and not for transformations which involve only one of insertion or\n",
    "        deletion.\n",
    "\n",
    "        Args:\n",
    "          transformation: Transformation Object\n",
    "            The Transformation to check compatibility for.\n",
    "\n",
    "        Returns:\n",
    "          True\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def extra_repr_keys(self):\n",
    "        \"\"\"\n",
    "        Set the extra representation of the constraint using these keys.\n",
    "        To print customized extra information, you should reimplement\n",
    "        this method in your own constraint. Both single-line and multi-\n",
    "        line strings are acceptable.\n",
    "\n",
    "        Args:\n",
    "          None\n",
    "\n",
    "        Returns:\n",
    "          []\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    __str__ = __repr__ = default_class_repr\n",
    "\n",
    "\n",
    "flair.device = device\n",
    "\n",
    "def words_from_text(s, words_to_ignore=[]):\n",
    "    \"\"\"\n",
    "    Lowercases a string, removes all non-alphanumeric characters, and splits\n",
    "    into words.\n",
    "\n",
    "    Args:\n",
    "      s: String\n",
    "        Input String\n",
    "      words_to_ignore: List\n",
    "        List of words that explicitly need to be ignored\n",
    "\n",
    "    Returns:\n",
    "      words: List\n",
    "        Legitimate list of alpha-numeric words that aren't ignored\n",
    "    \"\"\"\n",
    "    homos = set(\n",
    "        [\n",
    "            \"Ë—\",\n",
    "            \"à§­\",\n",
    "            \"È¢\",\n",
    "            \"ðŸ•\",\n",
    "            \"Ð±\",\n",
    "            \"Æ¼\",\n",
    "            \"áŽ\",\n",
    "            \"Æ·\",\n",
    "            \"á’¿\",\n",
    "            \"l\",\n",
    "            \"O\",\n",
    "            \"`\",\n",
    "            \"É‘\",\n",
    "            \"Ð¬\",\n",
    "            \"Ï²\",\n",
    "            \"Ô\",\n",
    "            \"Ðµ\",\n",
    "            \"ðš\",\n",
    "            \"É¡\",\n",
    "            \"Õ°\",\n",
    "            \"Ñ–\",\n",
    "            \"Ï³\",\n",
    "            \"ð’Œ\",\n",
    "            \"â…¼\",\n",
    "            \"ï½\",\n",
    "            \"Õ¸\",\n",
    "            \"Ð¾\",\n",
    "            \"Ñ€\",\n",
    "            \"Ô›\",\n",
    "            \"â²…\",\n",
    "            \"Ñ•\",\n",
    "            \"ðš\",\n",
    "            \"Õ½\",\n",
    "            \"Ñµ\",\n",
    "            \"Ô\",\n",
    "            \"Ã—\",\n",
    "            \"Ñƒ\",\n",
    "            \"á´¢\",\n",
    "        ]\n",
    "    )\n",
    "    words = []\n",
    "    word = \"\"\n",
    "    for c in \" \".join(s.split()):\n",
    "        if c.isalnum() or c in homos:\n",
    "            word += c\n",
    "        elif c in \"'-_*@\" and len(word) > 0:\n",
    "            # Allow apostrophes, hyphens, underscores, asterisks and at signs as long as they don't begin the\n",
    "            # word.\n",
    "            word += c\n",
    "        elif word:\n",
    "            if word not in words_to_ignore:\n",
    "                words.append(word)\n",
    "            word = \"\"\n",
    "    if len(word) and (word not in words_to_ignore):\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "_flair_pos_tagger = None\n",
    "\n",
    "\n",
    "def flair_tag(sentence, tag_type=\"upos-fast\"):\n",
    "    \"\"\"\n",
    "    Tags a Sentence object using flair part-of-speech tagger.\n",
    "\n",
    "    Args:\n",
    "      sentence: Object\n",
    "        Input Sequence\n",
    "      tag_type: String\n",
    "        Type of flair tag that needs to be applied\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    global _flair_pos_tagger\n",
    "    if not _flair_pos_tagger:\n",
    "        from flair.models import SequenceTagger\n",
    "\n",
    "        _flair_pos_tagger = SequenceTagger.load(tag_type)\n",
    "    _flair_pos_tagger.predict(sentence)\n",
    "\n",
    "\n",
    "def zip_flair_result(pred, tag_type=\"upos-fast\"):\n",
    "    \"\"\"\n",
    "    Takes a sentence tagging from flair and returns two lists, of words\n",
    "    and their corresponding parts-of-speech.\n",
    "\n",
    "    Args:\n",
    "      pred: Object\n",
    "        Resulting Prediction on input sentence post tagging\n",
    "      tag_type: String\n",
    "        Type of flair tag that needs to be applied\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    from flair.data import Sentence\n",
    "\n",
    "\n",
    "class AttackedText:\n",
    "    \"\"\"\n",
    "    A helper class that represents a string that can be attacked.\n",
    "    Models that take multiple sentences as input separate them by SPLIT_TOKEN.\n",
    "    Attacks \"see\" the entire input, joined into one string, without the split\n",
    "    token.\n",
    "    AttackedText instances that were perturbed from other AttackedText\n",
    "    objects contain a pointer to the previous text\n",
    "    (attack_attrs[\"previous_attacked_text\"]), so that the full chain of\n",
    "    perturbations might be reconstructed by using this key to form a linked\n",
    "    list.\n",
    "    \"\"\"\n",
    "\n",
    "    SPLIT_TOKEN = \"<SPLIT>\"\n",
    "\n",
    "    def __init__(self, text_input, attack_attrs=None):\n",
    "        # Read in ``text_input`` as a string or OrderedDict.\n",
    "        \"\"\"\n",
    "        Initiates the following attributes:\n",
    "\n",
    "        Args:\n",
    "          text: String\n",
    "            The string that this AttackedText Object represents\n",
    "          attack_attrs: Dictionary\n",
    "            Dictionary of various attributes stored during the\n",
    "            course of an attack.\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        if isinstance(text_input, str):\n",
    "            self._text_input = OrderedDict([(\"text\", text_input)])\n",
    "        elif isinstance(text_input, OrderedDict):\n",
    "            self._text_input = text_input\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                f\"Invalid text_input type {type(text_input)} (required str or OrderedDict)\"\n",
    "            )\n",
    "        # Process input lazily.\n",
    "        self._words = None\n",
    "        self._words_per_input = None\n",
    "        self._pos_tags = None\n",
    "        self._ner_tags = None\n",
    "        # Format text inputs.\n",
    "        self._text_input = OrderedDict([(k, v) for k, v in self._text_input.items()])\n",
    "        if attack_attrs is None:\n",
    "            self.attack_attrs = dict()\n",
    "        elif isinstance(attack_attrs, dict):\n",
    "            self.attack_attrs = attack_attrs\n",
    "        else:\n",
    "            raise TypeError(f\"Invalid type for attack_attrs: {type(attack_attrs)}\")\n",
    "        # Indices of words from the *original* text. Allows us to map\n",
    "        # indices between original text and this text, and vice-versa.\n",
    "        self.attack_attrs.setdefault(\"original_index_map\", np.arange(self.num_words))\n",
    "        # A list of all indices in *this* text that have been modified.\n",
    "        self.attack_attrs.setdefault(\"modified_indices\", set())\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Compares two text instances to make sure they have the same attack\n",
    "        attributes.\n",
    "        Since some elements stored in self.attack_attrs may be numpy\n",
    "        arrays, we have to take special care when comparing them.\n",
    "\n",
    "        Args:\n",
    "          Other: String\n",
    "            Specifies second text instance to be compared for attack attributes\n",
    "\n",
    "        Returns:\n",
    "          True\n",
    "        \"\"\"\n",
    "        if not (self.text == other.text):\n",
    "            return False\n",
    "        if len(self.attack_attrs) != len(other.attack_attrs):\n",
    "            return False\n",
    "        for key in self.attack_attrs:\n",
    "            if key not in other.attack_attrs:\n",
    "                return False\n",
    "            elif isinstance(self.attack_attrs[key], np.ndarray):\n",
    "                if not (self.attack_attrs[key].shape == other.attack_attrs[key].shape):\n",
    "                    return False\n",
    "                elif not (self.attack_attrs[key] == other.attack_attrs[key]).all():\n",
    "                    return False\n",
    "            else:\n",
    "                if not self.attack_attrs[key] == other.attack_attrs[key]:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.text)\n",
    "\n",
    "    def free_memory(self):\n",
    "        \"\"\"\n",
    "        Delete items that take up memory.\n",
    "        Can be called once the AttackedText is only needed to display.\n",
    "\n",
    "        Args:\n",
    "          None\n",
    "\n",
    "        Returns:\n",
    "          Nothing\n",
    "        \"\"\"\n",
    "        if \"previous_attacked_text\" in self.attack_attrs:\n",
    "            self.attack_attrs[\"previous_attacked_text\"].free_memory()\n",
    "            self.attack_attrs.pop(\"previous_attacked_text\", None)\n",
    "\n",
    "        self.attack_attrs.pop(\"last_transformation\", None)\n",
    "\n",
    "        for key in self.attack_attrs:\n",
    "            if isinstance(self.attack_attrs[key], torch.Tensor):\n",
    "                self.attack_attrs.pop(key, None)\n",
    "\n",
    "    def text_window_around_index(self, index, window_size):\n",
    "        \"\"\"\n",
    "        The text window of window_size words centered around\n",
    "        index.\n",
    "\n",
    "        Args:\n",
    "          index: Integer\n",
    "            Index of transformation within input sequence\n",
    "          window_size: Integer\n",
    "            Specifies size of the window around index\n",
    "\n",
    "        Returns:\n",
    "          Substring of text with specified window_size\n",
    "        \"\"\"\n",
    "        length = self.num_words\n",
    "        half_size = (window_size - 1) / 2.0\n",
    "        if index - half_size < 0:\n",
    "            start = 0\n",
    "            end = min(window_size - 1, length - 1)\n",
    "        elif index + half_size >= length:\n",
    "            start = max(0, length - window_size)\n",
    "            end = length - 1\n",
    "        else:\n",
    "            start = index - math.ceil(half_size)\n",
    "            end = index + math.floor(half_size)\n",
    "        text_idx_start = self._text_index_of_word_index(start)\n",
    "        text_idx_end = self._text_index_of_word_index(end) + len(self.words[end])\n",
    "        return self.text[text_idx_start:text_idx_end]\n",
    "\n",
    "    def pos_of_word_index(self, desired_word_idx):\n",
    "        \"\"\"\n",
    "        Returns the part-of-speech of the word at index word_idx.\n",
    "        Uses FLAIR part-of-speech tagger.\n",
    "\n",
    "        Args:\n",
    "          desired_word_idx: Integer\n",
    "            Index where POS transformation is to be applied within input sequence\n",
    "\n",
    "        Returns:\n",
    "          Part-of-speech of the word at index word_idx\n",
    "        \"\"\"\n",
    "        if not self._pos_tags:\n",
    "            sentence = Sentence(\n",
    "                self.text, use_tokenizer=words_from_text\n",
    "            )\n",
    "            flair_tag(sentence)\n",
    "            self._pos_tags = sentence\n",
    "        flair_word_list, flair_pos_list = zip_flair_result(\n",
    "            self._pos_tags\n",
    "        )\n",
    "\n",
    "        for word_idx, word in enumerate(self.words):\n",
    "            assert (\n",
    "                word in flair_word_list\n",
    "            ), \"word absent in flair returned part-of-speech tags\"\n",
    "            word_idx_in_flair_tags = flair_word_list.index(word)\n",
    "            if word_idx == desired_word_idx:\n",
    "                return flair_pos_list[word_idx_in_flair_tags]\n",
    "            else:\n",
    "                flair_word_list = flair_word_list[word_idx_in_flair_tags + 1 :]\n",
    "                flair_pos_list = flair_pos_list[word_idx_in_flair_tags + 1 :]\n",
    "\n",
    "        raise ValueError(\n",
    "            f\"Did not find word from index {desired_word_idx} in flair POS tag\"\n",
    "        )\n",
    "\n",
    "    def ner_of_word_index(self, desired_word_idx, model_name=\"ner\"):\n",
    "        \"\"\"\n",
    "        Returns the ner tag of the word at index word_idx.\n",
    "        Uses FLAIR ner tagger.\n",
    "\n",
    "        Args:\n",
    "          desired_word_idx: Integer\n",
    "            Index where POS transformation is to be applied within input sequence\n",
    "          model_name: String\n",
    "            Name of the model tag that needs to be applied\n",
    "\n",
    "        Returns:\n",
    "          ner tag of the word at index word_idx.\n",
    "\n",
    "        \"\"\"\n",
    "        if not self._ner_tags:\n",
    "            sentence = Sentence(\n",
    "                self.text, use_tokenizer = words_from_text\n",
    "            )\n",
    "            flair_tag(sentence, model_name)\n",
    "            self._ner_tags = sentence\n",
    "        flair_word_list, flair_ner_list = zip_flair_result(\n",
    "            self._ner_tags, \"ner\"\n",
    "        )\n",
    "\n",
    "        for word_idx, word in enumerate(flair_word_list):\n",
    "            word_idx_in_flair_tags = flair_word_list.index(word)\n",
    "            if word_idx == desired_word_idx:\n",
    "                return flair_ner_list[word_idx_in_flair_tags]\n",
    "            else:\n",
    "                flair_word_list = flair_word_list[word_idx_in_flair_tags + 1 :]\n",
    "                flair_ner_list = flair_ner_list[word_idx_in_flair_tags + 1 :]\n",
    "\n",
    "        raise ValueError(\n",
    "            f\"Did not find word from index {desired_word_idx} in flair POS tag\"\n",
    "        )\n",
    "\n",
    "    def _text_index_of_word_index(self, i):\n",
    "        \"\"\"\n",
    "        Returns the index of word following i in self.text.\n",
    "\n",
    "        Args:\n",
    "          i: Integer\n",
    "            Index of word upon which perturbation is intended.\n",
    "\n",
    "        Returns:\n",
    "          look_after_index: Index\n",
    "            Index of the word following word[i]\n",
    "        \"\"\"\n",
    "        pre_words = self.words[: i + 1]\n",
    "        lower_text = self.text.lower()\n",
    "        # Find all words until `i` in string.\n",
    "        look_after_index = 0\n",
    "        for word in pre_words:\n",
    "            look_after_index = lower_text.find(word.lower(), look_after_index) + len(\n",
    "                word\n",
    "            )\n",
    "        look_after_index -= len(self.words[i])\n",
    "        return look_after_index\n",
    "\n",
    "    def text_until_word_index(self, i):\n",
    "        \"\"\"\n",
    "        Returns the text before the beginning of word at index i.\n",
    "\n",
    "        Args:\n",
    "          i: Integer\n",
    "            Index of word upon which perturbation is intended.\n",
    "\n",
    "        Returns:\n",
    "          Text before the beginning of word at index i.\n",
    "        \"\"\"\n",
    "        look_after_index = self._text_index_of_word_index(i)\n",
    "        return self.text[:look_after_index]\n",
    "\n",
    "    def text_after_word_index(self, i):\n",
    "        \"\"\"\n",
    "        Returns the text after the end of word at index i.\n",
    "\n",
    "        Args:\n",
    "          i: Integer\n",
    "            Index of word upon which perturbation is intended.\n",
    "\n",
    "        Returns:\n",
    "          Text after the end of word at index i.\n",
    "        \"\"\"\n",
    "        # Get index of beginning of word then jump to end of word.\n",
    "        look_after_index = self._text_index_of_word_index(i) + len(self.words[i])\n",
    "        return self.text[look_after_index:]\n",
    "\n",
    "    def first_word_diff(self, other_attacked_text):\n",
    "        \"\"\"\n",
    "        Returns the first word in self.words that differs from\n",
    "        other_attacked_text.\n",
    "        Useful for word swap strategies.\n",
    "\n",
    "        Args:\n",
    "          other_attacked_text: String Object\n",
    "            Sentence/sequence to be compared with given input\n",
    "\n",
    "        Returns:\n",
    "          w1: String\n",
    "            First differing word in self.words if difference exists\n",
    "            None otherwise\n",
    "        \"\"\"\n",
    "        w1 = self.words\n",
    "        w2 = other_attacked_text.words\n",
    "        for i in range(min(len(w1), len(w2))):\n",
    "            if w1[i] != w2[i]:\n",
    "                return w1[i]\n",
    "        return None\n",
    "\n",
    "    def first_word_diff_index(self, other_attacked_text):\n",
    "        \"\"\"\n",
    "        Returns the index of the first word in self.words that differs from\n",
    "        other_attacked_text.\n",
    "        Useful for word swap strategies.\n",
    "\n",
    "        Args:\n",
    "          other_attacked_text: String object\n",
    "            Sentence/sequence to be compared with given input\n",
    "\n",
    "        Returns:\n",
    "          w1: String\n",
    "            First differing word in self.words if difference exists\n",
    "            None otherwise\n",
    "        \"\"\"\n",
    "        w1 = self.words\n",
    "        w2 = other_attacked_text.words\n",
    "        for i in range(min(len(w1), len(w2))):\n",
    "            if w1[i] != w2[i]:\n",
    "                return i\n",
    "        return None\n",
    "\n",
    "    def all_words_diff(self, other_attacked_text):\n",
    "        \"\"\"\n",
    "        Returns the set of indices for which this and other_attacked_text\n",
    "        have different words.\n",
    "\n",
    "        Args:\n",
    "          other_attacked_text: String object\n",
    "            Sentence/sequence to be compared with given input\n",
    "\n",
    "        Returns:\n",
    "          indices: Set\n",
    "            differing indices for corresponding words betwee self.words and other_attacked_text\n",
    "        \"\"\"\n",
    "        indices = set()\n",
    "        w1 = self.words\n",
    "        w2 = other_attacked_text.words\n",
    "        for i in range(min(len(w1), len(w2))):\n",
    "            if w1[i] != w2[i]:\n",
    "                indices.add(i)\n",
    "        return indices\n",
    "\n",
    "    def ith_word_diff(self, other_attacked_text, i):\n",
    "        \"\"\"\n",
    "        Returns whether the word at index i differs from\n",
    "        other_attacked_text.\n",
    "\n",
    "        Args:\n",
    "          other_attacked_text: String object\n",
    "            Sentence/sequence to be compared with given input\n",
    "          i: Integer\n",
    "            Index of word of interest within input sequence\n",
    "\n",
    "        Returns:\n",
    "          w1: Boolean\n",
    "            Checks for differing words in self.words at index i\n",
    "        \"\"\"\n",
    "        w1 = self.words\n",
    "        w2 = other_attacked_text.words\n",
    "        if len(w1) - 1 < i or len(w2) - 1 < i:\n",
    "            return True\n",
    "        return w1[i] != w2[i]\n",
    "\n",
    "    def words_diff_num(self, other_attacked_text):\n",
    "        # using edit distance to calculate words diff num\n",
    "        def generate_tokens(words):\n",
    "            \"\"\"\n",
    "            Generates token for given sequence of words\n",
    "\n",
    "            Args:\n",
    "              words: List\n",
    "                Sequence of words\n",
    "\n",
    "            Returns:\n",
    "              result: Dictionary\n",
    "                Word mapped to corresponding index\n",
    "            \"\"\"\n",
    "            result = {}\n",
    "            idx = 1\n",
    "            for w in words:\n",
    "                if w not in result:\n",
    "                    result[w] = idx\n",
    "                    idx += 1\n",
    "            return result\n",
    "\n",
    "        def words_to_tokens(words, tokens):\n",
    "            \"\"\"\n",
    "            Helper function to extract corresponding words from tokens\n",
    "\n",
    "            Args:\n",
    "              words: List\n",
    "                Sequence of words\n",
    "              tokens: List\n",
    "                Sequence of tokens\n",
    "\n",
    "            Returns:\n",
    "              result: List\n",
    "                Corresponding token for each word\n",
    "            \"\"\"\n",
    "            result = []\n",
    "            for w in words:\n",
    "                result.append(tokens[w])\n",
    "            return result\n",
    "\n",
    "        def edit_distance(w1_t, w2_t):\n",
    "            \"\"\"\n",
    "            Function to find the edit distance between given pair of words\n",
    "\n",
    "            Args:\n",
    "              w1_t: String\n",
    "                Input Sequence #1\n",
    "              w2_t: String\n",
    "                Input Sequence #2\n",
    "\n",
    "            Returns:\n",
    "              matrix: 2D Tensor\n",
    "                Distance between each letter in input sequence #1 in\n",
    "                relation to letter in input sequence #2\n",
    "            \"\"\"\n",
    "            matrix = [\n",
    "                [i + j for j in range(len(w2_t) + 1)] for i in range(len(w1_t) + 1)\n",
    "            ]\n",
    "\n",
    "            for i in range(1, len(w1_t) + 1):\n",
    "                for j in range(1, len(w2_t) + 1):\n",
    "                    if w1_t[i - 1] == w2_t[j - 1]:\n",
    "                        d = 0\n",
    "                    else:\n",
    "                        d = 1\n",
    "                    matrix[i][j] = min(\n",
    "                        matrix[i - 1][j] + 1,\n",
    "                        matrix[i][j - 1] + 1,\n",
    "                        matrix[i - 1][j - 1] + d,\n",
    "                    )\n",
    "\n",
    "            return matrix[len(w1_t)][len(w2_t)]\n",
    "\n",
    "        def cal_dif(w1, w2):\n",
    "            \"\"\"\n",
    "            Calculate the edit distance given any pair of characters\n",
    "\n",
    "            Args:\n",
    "              w1: String\n",
    "                Input Character #1\n",
    "              w2: String\n",
    "                Input Character #2\n",
    "\n",
    "            Returns:\n",
    "              Distance between token of input sequence #1 in\n",
    "              relation to token of input sequence #2\n",
    "            \"\"\"\n",
    "            tokens = generate_tokens(w1 + w2)\n",
    "            w1_t = words_to_tokens(w1, tokens)\n",
    "            w2_t = words_to_tokens(w2, tokens)\n",
    "            return edit_distance(w1_t, w2_t)\n",
    "\n",
    "        w1 = self.words\n",
    "        w2 = other_attacked_text.words\n",
    "        return cal_dif(w1, w2)\n",
    "\n",
    "    def convert_from_original_idxs(self, idxs):\n",
    "        \"\"\"\n",
    "        Takes indices of words from original string and converts them to\n",
    "        indices of the same words in the current string.\n",
    "        Uses information from\n",
    "        self.attack_attrs['original_index_map'], which maps word\n",
    "        indices from the original to perturbed text.\n",
    "\n",
    "        Args:\n",
    "          idxs: List\n",
    "            List of indexes\n",
    "\n",
    "        Returns:\n",
    "          List of mapping of word indices from the original to perturbed text\n",
    "        \"\"\"\n",
    "        if len(self.attack_attrs[\"original_index_map\"]) == 0:\n",
    "            return idxs\n",
    "        elif isinstance(idxs, set):\n",
    "            idxs = list(idxs)\n",
    "\n",
    "        elif not isinstance(idxs, [list, np.ndarray]):\n",
    "            raise TypeError(\n",
    "                f\"convert_from_original_idxs got invalid idxs type {type(idxs)}\"\n",
    "            )\n",
    "\n",
    "        return [self.attack_attrs[\"original_index_map\"][i] for i in idxs]\n",
    "\n",
    "    def replace_words_at_indices(self, indices, new_words):\n",
    "        \"\"\"\n",
    "        This code returns a new AttackedText object where the word at\n",
    "        index is replaced with a new word.\n",
    "\n",
    "        Args:\n",
    "          indices: List\n",
    "            List of indexes of words in input sequence\n",
    "          new_words: List\n",
    "            List of words with new word as replacement for original word\n",
    "\n",
    "        Returns:\n",
    "          New AttackedText object where the word at\n",
    "          index is replaced with a new word.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(indices) != len(new_words):\n",
    "            raise ValueError(\n",
    "                f\"Cannot replace {len(new_words)} words at {len(indices)} indices.\"\n",
    "            )\n",
    "        words = self.words[:]\n",
    "        for i, new_word in zip(indices, new_words):\n",
    "            if not isinstance(new_word, str):\n",
    "                raise TypeError(\n",
    "                    f\"replace_words_at_indices requires ``str`` words, got {type(new_word)}\"\n",
    "                )\n",
    "            if (i < 0) or (i > len(words)):\n",
    "                raise ValueError(f\"Cannot assign word at index {i}\")\n",
    "            words[i] = new_word\n",
    "        return self.generate_new_attacked_text(words)\n",
    "\n",
    "    def replace_word_at_index(self, index, new_word):\n",
    "        \"\"\"\n",
    "        This code returns a new AttackedText object where the word at\n",
    "        index is replaced with a new word.\n",
    "\n",
    "        Args:\n",
    "          indices: Integer\n",
    "            Index of word\n",
    "          new_word: String\n",
    "            New word for replacement at index of word\n",
    "\n",
    "        Returns:\n",
    "          New AttackedText object where the word at\n",
    "          index is replaced with a new word.\n",
    "        \"\"\"\n",
    "        if not isinstance(new_word, str):\n",
    "            raise TypeError(\n",
    "                f\"replace_word_at_index requires ``str`` new_word, got {type(new_word)}\"\n",
    "            )\n",
    "        return self.replace_words_at_indices([index], [new_word])\n",
    "\n",
    "    def delete_word_at_index(self, index):\n",
    "        \"\"\"\n",
    "        This code returns a new AttackedText object where the word at\n",
    "        index is removed.\n",
    "\n",
    "         Args:\n",
    "          index: Integer\n",
    "            Index of word\n",
    "\n",
    "        Returns:\n",
    "          New AttackedText object where the word at\n",
    "          index is removed.\n",
    "        \"\"\"\n",
    "        return self.replace_word_at_index(index, \"\")\n",
    "\n",
    "    def insert_text_after_word_index(self, index, text):\n",
    "        \"\"\"\n",
    "        Inserts a string before word at index \"index\" and attempts to add\n",
    "        appropriate spacing.\n",
    "\n",
    "        Args:\n",
    "          index: Integer\n",
    "            Index of word\n",
    "          text: String\n",
    "            Input Sequence\n",
    "\n",
    "        Returns:\n",
    "          New AttackedText object where new word is inserted\n",
    "          before word at index \"index\".\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(f\"text must be an str, got type {type(text)}\")\n",
    "        word_at_index = self.words[index]\n",
    "        new_text = \" \".join((word_at_index, text))\n",
    "        return self.replace_word_at_index(index, new_text)\n",
    "\n",
    "    def insert_text_before_word_index(self, index, text):\n",
    "        \"\"\"\n",
    "        Inserts a string before word at index \"index\" and attempts to add\n",
    "        appropriate spacing.\n",
    "\n",
    "        Args:\n",
    "          index: Integer\n",
    "            Index of word\n",
    "          text: String\n",
    "            Input Sequence\n",
    "\n",
    "        Returns:\n",
    "          New AttackedText object where the word before\n",
    "          index \"index\" is replaced with a new word.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise TypeError(f\"text must be an str, got type {type(text)}\")\n",
    "        word_at_index = self.words[index]\n",
    "        # TODO if ``word_at_index`` is at the beginning of a sentence, we should\n",
    "        # optionally capitalize ``text``.\n",
    "        new_text = \" \".join((text, word_at_index))\n",
    "        return self.replace_word_at_index(index, new_text)\n",
    "\n",
    "    def get_deletion_indices(self):\n",
    "        \"\"\"\n",
    "        Returns attack attributes based on corresponding\n",
    "        attributes in original_index_map\n",
    "\n",
    "        Args:\n",
    "          None\n",
    "\n",
    "        Returns:\n",
    "          Attack attributes based on corresponding\n",
    "          attributes in original_index_map\n",
    "        \"\"\"\n",
    "        return self.attack_attrs[\"original_index_map\"][\n",
    "            self.attack_attrs[\"original_index_map\"] == -1\n",
    "        ]\n",
    "\n",
    "    def generate_new_attacked_text(self, new_words):\n",
    "        \"\"\"\n",
    "        Returns a new AttackedText object and replaces old list of words\n",
    "        with a new list of words, but preserves the punctuation and spacing of\n",
    "        the original message.\n",
    "        self.words is a list of the words in the current text with\n",
    "        punctuation removed. However, each \"word\" in new_words could\n",
    "        be an empty string, representing a word deletion, or a string\n",
    "        with multiple space-separated words, representation an insertion\n",
    "        of one or more words.\n",
    "\n",
    "        Args:\n",
    "          new_words: String\n",
    "            New word for potential replacement\n",
    "\n",
    "        Returns:\n",
    "          TextAttack object with preturbed text and attack attributes\n",
    "        \"\"\"\n",
    "        perturbed_text = \"\"\n",
    "        original_text = AttackedText.SPLIT_TOKEN.join(self._text_input.values())\n",
    "        new_attack_attrs = dict()\n",
    "        if \"label_names\" in self.attack_attrs:\n",
    "            new_attack_attrs[\"label_names\"] = self.attack_attrs[\"label_names\"]\n",
    "        new_attack_attrs[\"newly_modified_indices\"] = set()\n",
    "        # Point to previously monitored text.\n",
    "        new_attack_attrs[\"previous_attacked_text\"] = self\n",
    "        # Use `new_attack_attrs` to track indices with respect to the original\n",
    "        # text.\n",
    "        new_attack_attrs[\"modified_indices\"] = self.attack_attrs[\n",
    "            \"modified_indices\"\n",
    "        ].copy()\n",
    "        new_attack_attrs[\"original_index_map\"] = self.attack_attrs[\n",
    "            \"original_index_map\"\n",
    "        ].copy()\n",
    "        new_i = 0\n",
    "        # Create the new attacked text by swapping out words from the original\n",
    "        # text with a sequence of 0+ words in the new text.\n",
    "        for i, (input_word, adv_word_seq) in enumerate(zip(self.words, new_words)):\n",
    "            word_start = original_text.index(input_word)\n",
    "            word_end = word_start + len(input_word)\n",
    "            perturbed_text += original_text[:word_start]\n",
    "            original_text = original_text[word_end:]\n",
    "            adv_words = words_from_text(adv_word_seq)\n",
    "            adv_num_words = len(adv_words)\n",
    "            num_words_diff = adv_num_words - len(words_from_text(input_word))\n",
    "            # Track indices on insertions and deletions.\n",
    "            if num_words_diff != 0:\n",
    "                # Re-calculated modified indices. If words are inserted or deleted,\n",
    "                # they could change.\n",
    "                shifted_modified_indices = set()\n",
    "                for modified_idx in new_attack_attrs[\"modified_indices\"]:\n",
    "                    if modified_idx < i:\n",
    "                        shifted_modified_indices.add(modified_idx)\n",
    "                    elif modified_idx > i:\n",
    "                        shifted_modified_indices.add(modified_idx + num_words_diff)\n",
    "                    else:\n",
    "                        pass\n",
    "                new_attack_attrs[\"modified_indices\"] = shifted_modified_indices\n",
    "                # Track insertions and deletions wrt original text.\n",
    "                # original_modification_idx = i\n",
    "                new_idx_map = new_attack_attrs[\"original_index_map\"].copy()\n",
    "                if num_words_diff == -1:\n",
    "                    # Word deletion\n",
    "                    new_idx_map[new_idx_map == i] = -1\n",
    "                new_idx_map[new_idx_map > i] += num_words_diff\n",
    "\n",
    "                if num_words_diff > 0 and input_word != adv_words[0]:\n",
    "                    # If insertion happens before the `input_word`\n",
    "                    new_idx_map[new_idx_map == i] += num_words_diff\n",
    "\n",
    "                new_attack_attrs[\"original_index_map\"] = new_idx_map\n",
    "            # Move pointer and save indices of new modified words.\n",
    "            for j in range(i, i + adv_num_words):\n",
    "                if input_word != adv_word_seq:\n",
    "                    new_attack_attrs[\"modified_indices\"].add(new_i)\n",
    "                    new_attack_attrs[\"newly_modified_indices\"].add(new_i)\n",
    "                new_i += 1\n",
    "            # Check spaces for deleted text.\n",
    "            if adv_num_words == 0 and len(original_text):\n",
    "                # Remove extra space (or else there would be two spaces for each\n",
    "                # deleted word).\n",
    "                # @TODO What to do with punctuation in this case? This behavior is undefined.\n",
    "                if i == 0:\n",
    "                    # If the first word was deleted, take a subsequent space.\n",
    "                    if original_text[0] == \" \":\n",
    "                        original_text = original_text[1:]\n",
    "                else:\n",
    "                    # If a word other than the first was deleted, take a preceding space.\n",
    "                    if perturbed_text[-1] == \" \":\n",
    "                        perturbed_text = perturbed_text[:-1]\n",
    "            # Add substitute word(s) to new sentence.\n",
    "            perturbed_text += adv_word_seq\n",
    "        perturbed_text += original_text  # Add all of the ending punctuation.\n",
    "        # Reform perturbed_text into an OrderedDict.\n",
    "        perturbed_input_texts = perturbed_text.split(AttackedText.SPLIT_TOKEN)\n",
    "        perturbed_input = OrderedDict(\n",
    "            zip(self._text_input.keys(), perturbed_input_texts)\n",
    "        )\n",
    "        return AttackedText(perturbed_input, attack_attrs=new_attack_attrs)\n",
    "\n",
    "    def words_diff_ratio(self, x):\n",
    "        \"\"\"\n",
    "        Get the ratio of word differences between current text and x.\n",
    "        Note that current text and x must have same number of words.\n",
    "\n",
    "        Args:\n",
    "          x: String\n",
    "            Compares x with input text for ratio of word differences\n",
    "\n",
    "        Returns:\n",
    "          Ratio of word differences between current text and x.\n",
    "        \"\"\"\n",
    "        assert self.num_words == x.num_words\n",
    "        return float(np.sum(self.words != x.words)) / self.num_words\n",
    "\n",
    "    def align_with_model_tokens(self, model_wrapper):\n",
    "        \"\"\"\n",
    "        Align AttackedText's words with target model's tokenization scheme\n",
    "        (e.g. word, character, subword).\n",
    "        Specifically, we map each word to list\n",
    "        of indices of tokens that compose the\n",
    "        word (e.g. embedding --> [\"em\",\"##bed\", \"##ding\"])\n",
    "\n",
    "        Args:\n",
    "          model_wrapper: textattack.models.wrappers.ModelWrapper\n",
    "            ModelWrapper of the target model\n",
    "\n",
    "        Returns:\n",
    "          word2token_mapping: (dict[int, list[int]])\n",
    "            Dictionary that maps i-th word to list of indices.\n",
    "        \"\"\"\n",
    "        tokens = model_wrapper.tokenize([self.tokenizer_input], strip_prefix=True)[0]\n",
    "        word2token_mapping = {}\n",
    "        j = 0\n",
    "        last_matched = 0\n",
    "\n",
    "        for i, word in enumerate(self.words):\n",
    "            matched_tokens = []\n",
    "            while j < len(tokens) and len(word) > 0:\n",
    "                token = tokens[j].lower()\n",
    "                idx = word.lower().find(token)\n",
    "                if idx == 0:\n",
    "                    word = word[idx + len(token) :]\n",
    "                    matched_tokens.append(j)\n",
    "                    last_matched = j\n",
    "                j += 1\n",
    "\n",
    "            if not matched_tokens:\n",
    "                word2token_mapping[i] = None\n",
    "                j = last_matched\n",
    "            else:\n",
    "                word2token_mapping[i] = matched_tokens\n",
    "\n",
    "        return word2token_mapping\n",
    "\n",
    "    @property\n",
    "    def tokenizer_input(self):\n",
    "        \"\"\"\n",
    "        The tuple of inputs to be passed to the tokenizer.\n",
    "        \"\"\"\n",
    "        input_tuple = tuple(self._text_input.values())\n",
    "        # Prefer to return a string instead of a tuple with a single value.\n",
    "        if len(input_tuple) == 1:\n",
    "            return input_tuple[0]\n",
    "        else:\n",
    "            return input_tuple\n",
    "\n",
    "    @property\n",
    "    def column_labels(self):\n",
    "        \"\"\"\n",
    "        Returns the labels for this text's columns.\n",
    "        For single-sequence inputs, this simply returns ['text'].\n",
    "        \"\"\"\n",
    "        return list(self._text_input.keys())\n",
    "\n",
    "    @property\n",
    "    def words_per_input(self):\n",
    "        \"\"\"\n",
    "        Returns a list of lists of words corresponding to each input.\n",
    "        \"\"\"\n",
    "        if not self._words_per_input:\n",
    "            self._words_per_input = [\n",
    "                words_from_text(_input) for _input in self._text_input.values()\n",
    "            ]\n",
    "        return self._words_per_input\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        if not self._words:\n",
    "            self._words = words_from_text(self.text)\n",
    "        return self._words\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        \"\"\"\n",
    "        Represents full text input.\n",
    "        Multiply inputs are joined with a line break.\n",
    "        \"\"\"\n",
    "        return \"\\n\".join(self._text_input.values())\n",
    "\n",
    "    @property\n",
    "    def num_words(self):\n",
    "        \"\"\"\n",
    "        Returns the number of words in the sequence.\n",
    "        \"\"\"\n",
    "        return len(self.words)\n",
    "\n",
    "    def printable_text(self, key_color=\"bold\", key_color_method=None):\n",
    "        \"\"\"\n",
    "        Represents full text input. Adds field descriptions.\n",
    "\n",
    "        Args:\n",
    "        key_color: String\n",
    "          Field description of input text\n",
    "        key_color_method: String\n",
    "          Color method description of input text\n",
    "\n",
    "        Usage/Example:\n",
    "            entailment inputs look like:\n",
    "            premise: ...\n",
    "            hypothesis: ...\n",
    "\n",
    "        Returns:\n",
    "          Next iterable value for single sequence inputs\n",
    "          Shared field attributes for multi-sequence inputs\n",
    "        \"\"\"\n",
    "        # For single-sequence inputs, don't show a prefix.\n",
    "        if len(self._text_input) == 1:\n",
    "            return next(iter(self._text_input.values()))\n",
    "        # For multiple-sequence inputs, show a prefix and a colon. Optionally,\n",
    "        # color the key.\n",
    "        else:\n",
    "            if key_color_method:\n",
    "\n",
    "                def ck(k):\n",
    "                    return textattack.shared.utils.color_text(\n",
    "                        k, key_color, key_color_method\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "\n",
    "                def ck(k):\n",
    "                    return k\n",
    "\n",
    "            return \"\\n\".join(\n",
    "                f\"{ck(key.capitalize())}: {value}\"\n",
    "                for key, value in self._text_input.items()\n",
    "            )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'<AttackedText \"{self.text}\">'\n",
    "\n",
    "\n",
    "class Augmenter:\n",
    "    \"\"\"\n",
    "    A class for performing data augmentation using TextAttack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformation,\n",
    "        constraints=[],\n",
    "        pct_words_to_swap=0.1,\n",
    "        transformations_per_example=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initiates the following attributes:\n",
    "\n",
    "        Args:\n",
    "          transformation: Transformation Object\n",
    "            The transformation that suggests new texts from an input.\n",
    "          constraints: List\n",
    "            Constraints that each transformation must meet\n",
    "          pct_words_to_swap: Float [0., 1.],\n",
    "            Percentage of words to swap per augmented example\n",
    "          transformations_per_example: Integer\n",
    "            Maximum number of augmentations per input\n",
    "\n",
    "         Returns:\n",
    "          None\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            transformations_per_example > 0\n",
    "        ), \"transformations_per_example must be a positive integer\"\n",
    "        assert 0.0 <= pct_words_to_swap <= 1.0, \"pct_words_to_swap must be in [0., 1.]\"\n",
    "        self.transformation = transformation\n",
    "        self.pct_words_to_swap = pct_words_to_swap\n",
    "        self.transformations_per_example = transformations_per_example\n",
    "\n",
    "        self.constraints = []\n",
    "        self.pre_transformation_constraints = []\n",
    "        for constraint in constraints:\n",
    "            if isinstance(constraint, PreTransformationConstraint):\n",
    "                self.pre_transformation_constraints.append(constraint)\n",
    "            else:\n",
    "                self.constraints.append(constraint)\n",
    "\n",
    "    def _filter_transformations(self, transformed_texts, current_text, original_text):\n",
    "        \"\"\"\n",
    "        Filters a list of AttackedText objects to include only the ones\n",
    "        that pass self.constraints.\n",
    "\n",
    "        Args:\n",
    "          Transformed_text: List\n",
    "            List of Strings corresponding to transformations\n",
    "          Current_text: String\n",
    "            String to be compared against for transformation\n",
    "            when original does not meet constraint requirement\n",
    "          Original_text: String\n",
    "            Original Input String\n",
    "\n",
    "        Returns:\n",
    "          All possible transformations for a given string. Currently only\n",
    "        supports transformations which are word swaps.\n",
    "        \"\"\"\n",
    "        for C in self.constraints:\n",
    "            if len(transformed_texts) == 0:\n",
    "                break\n",
    "            if C.compare_against_original:\n",
    "                if not original_text:\n",
    "                    raise ValueError(\n",
    "                        f\"Missing `original_text` argument when constraint {type(C)} is set to compare against \"\n",
    "                        f\"`original_text` \"\n",
    "                    )\n",
    "\n",
    "                transformed_texts = C.call_many(transformed_texts, original_text)\n",
    "            else:\n",
    "                transformed_texts = C.call_many(transformed_texts, current_text)\n",
    "        return transformed_texts\n",
    "\n",
    "\n",
    "    def augment(self, text):\n",
    "        \"\"\"\n",
    "        Returns all possible augmentations of text according to\n",
    "        self.transformation.\n",
    "\n",
    "        Args:\n",
    "          text: String\n",
    "            Text to be augmented via transformation\n",
    "\n",
    "        Returns:\n",
    "          Sorted list of all possible augmentations of text according to\n",
    "          compatible self.transformation.\n",
    "        \"\"\"\n",
    "        attacked_text = AttackedText(text)\n",
    "        original_text = attacked_text\n",
    "        all_transformed_texts = set()\n",
    "        num_words_to_swap = max(\n",
    "            int(self.pct_words_to_swap * len(attacked_text.words)), 1\n",
    "        )\n",
    "        for _ in range(self.transformations_per_example):\n",
    "            current_text = attacked_text\n",
    "            words_swapped = len(current_text.attack_attrs[\"modified_indices\"])\n",
    "\n",
    "            while words_swapped < num_words_to_swap:\n",
    "                transformed_texts = self.transformation(\n",
    "                    current_text, self.pre_transformation_constraints\n",
    "                )\n",
    "\n",
    "                # Get rid of transformations we already have\n",
    "                transformed_texts = [\n",
    "                    t for t in transformed_texts if t not in all_transformed_texts\n",
    "                ]\n",
    "\n",
    "                # Filter out transformations that don't match the constraints.\n",
    "                transformed_texts = self._filter_transformations(\n",
    "                    transformed_texts, current_text, original_text\n",
    "                )\n",
    "\n",
    "                # if there's no more transformed texts after filter, terminate\n",
    "                if not len(transformed_texts):\n",
    "                    break\n",
    "\n",
    "                current_text = random.choice(transformed_texts)\n",
    "\n",
    "                # update words_swapped based on modified indices\n",
    "                words_swapped = max(\n",
    "                    len(current_text.attack_attrs[\"modified_indices\"]),\n",
    "                    words_swapped + 1,\n",
    "                )\n",
    "            all_transformed_texts.add(current_text)\n",
    "        return sorted([at.printable_text() for at in all_transformed_texts])\n",
    "\n",
    "\n",
    "    def augment_many(self, text_list, show_progress=False):\n",
    "        \"\"\"\n",
    "        Returns all possible augmentations of a list of strings according to\n",
    "        self.transformation.\n",
    "\n",
    "        Args:\n",
    "          text_list: List of strings\n",
    "            A list of strings for data augmentation\n",
    "          show_progress: Boolean\n",
    "            A variable that controls visibility of Augmentation progress\n",
    "\n",
    "        Returns:\n",
    "          A list(string) of augmented texts.\n",
    "        \"\"\"\n",
    "        if show_progress:\n",
    "            text_list = tqdm.tqdm(text_list, desc=\"Augmenting data...\")\n",
    "        return [self.augment(text) for text in text_list]\n",
    "\n",
    "\n",
    "    def augment_text_with_ids(self, text_list, id_list, show_progress=True):\n",
    "        \"\"\"\n",
    "        Supplements a list of text with more text data.\n",
    "\n",
    "         Args:\n",
    "          text_list: List of strings\n",
    "            A list of strings for data augmentation\n",
    "          id_list: List of indexes\n",
    "            A list of indexes for corresponding strings\n",
    "          show_progress: Boolean\n",
    "            A variable that controls visibility of augmentation progress\n",
    "\n",
    "        Returns:\n",
    "          all_text_list, all_id_list: List, List\n",
    "            The augmented text along with the corresponding IDs for\n",
    "            each augmented example.\n",
    "        \"\"\"\n",
    "        if len(text_list) != len(id_list):\n",
    "            raise ValueError(\"List of text must be same length as list of IDs\")\n",
    "        if self.transformations_per_example == 0:\n",
    "            return text_list, id_list\n",
    "        all_text_list = []\n",
    "        all_id_list = []\n",
    "        if show_progress:\n",
    "            text_list = tqdm.tqdm(text_list, desc=\"Augmenting data...\")\n",
    "        for text, _id in zip(text_list, id_list):\n",
    "            all_text_list.append(text)\n",
    "            all_id_list.append(_id)\n",
    "            augmented_texts = self.augment(text)\n",
    "            all_text_list.extend\n",
    "            all_text_list.extend([text] + augmented_texts)\n",
    "            all_id_list.extend([_id] * (1 + len(augmented_texts)))\n",
    "        return all_text_list, all_id_list\n",
    "\n",
    "    def __repr__(self):\n",
    "        main_str = \"Augmenter\" + \"(\"\n",
    "        lines = []\n",
    "        # self.transformation\n",
    "        lines.append(utils.add_indent(f\"(transformation):  {self.transformation}\", 2))\n",
    "        # self.constraints\n",
    "        constraints_lines = []\n",
    "        constraints = self.constraints + self.pre_transformation_constraints\n",
    "        if len(constraints):\n",
    "            for i, constraint in enumerate(constraints):\n",
    "                constraints_lines.append(utils.add_indent(f\"({i}): {constraint}\", 2))\n",
    "            constraints_str = utils.add_indent(\"\\n\" + \"\\n\".join(constraints_lines), 2)\n",
    "        else:\n",
    "            constraints_str = \"None\"\n",
    "        lines.append(utils.add_indent(f\"(constraints): {constraints_str}\", 2))\n",
    "        main_str += \"\\n  \" + \"\\n  \".join(lines) + \"\\n\"\n",
    "        main_str += \")\"\n",
    "        return main_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus 3.2: Augment the original review\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown Word-level Augmentations\n",
    "word_swap_contract = True  # @param {type:\"boolean\"}\n",
    "word_swap_extend = False  # @param {type:\"boolean\"}\n",
    "word_swap_homoglyph_swap = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown Character-level Augmentations\n",
    "word_swap_neighboring_character_swap = True  # @param {type:\"boolean\"}\n",
    "word_swap_qwerty = False  # @param {type:\"boolean\"}\n",
    "word_swap_random_character_deletion = False  # @param {type:\"boolean\"}\n",
    "word_swap_random_character_insertion = False  # @param {type:\"boolean\"}\n",
    "word_swap_random_character_substitution = False  # @param {type:\"boolean\"}\n",
    "# @markdown ---\n",
    "\n",
    "# @markdown Check all the augmentations that you wish to apply!\n",
    "\n",
    "# @markdown **NOTE:** *Try applying each augmentation individually, and observe the changes.*\n",
    "\n",
    "# Apply augmentations\n",
    "augmentations = []\n",
    "if word_swap_contract:\n",
    "  augmentations.append(WordSwapContract())\n",
    "if word_swap_extend:\n",
    "  augmentations.append(WordSwapExtend())\n",
    "if word_swap_homoglyph_swap:\n",
    "  augmentations.append(WordSwapHomoglyphSwap())\n",
    "if word_swap_neighboring_character_swap:\n",
    "  augmentations.append(WordSwapNeighboringCharacterSwap())\n",
    "if word_swap_qwerty:\n",
    "  augmentations.append(WordSwapQWERTY())\n",
    "if word_swap_random_character_deletion:\n",
    "  augmentations.append(WordSwapRandomCharacterDeletion())\n",
    "if word_swap_random_character_insertion:\n",
    "  augmentations.append(WordSwapRandomCharacterInsertion())\n",
    "if word_swap_random_character_substitution:\n",
    "  augmentations.append(WordSwapRandomCharacterSubstitution())\n",
    "\n",
    "transformation = CompositeTransformation(augmentations)\n",
    "augmenter = Augmenter(transformation=transformation,\n",
    "                      transformations_per_example=1)\n",
    "augmented_review = clean_text(augmenter.augment(context)[0])\n",
    "print(\"Augmented review:\\n\")\n",
    "pprint(augmented_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "We can now check the predictions for the original text and its augmented version! Try to find the perfect combination of perturbations to break the model, i.e., model giving incorrect prediction for the augmented text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Bonus 3.3: Check model predictions\n",
    "def getPrediction(text):\n",
    "  \"\"\"\n",
    "  Outputs model prediction based on the input text.\n",
    "\n",
    "  Args:\n",
    "    text: String\n",
    "      Input text\n",
    "\n",
    "  Returns:\n",
    "    item of pred: Iterable\n",
    "      Prediction on the input text\n",
    "  \"\"\"\n",
    "  inputs = tokenizer(text, padding=\"max_length\",\n",
    "                     truncation=True, return_tensors=\"pt\")\n",
    "  for key, value in inputs.items():\n",
    "    inputs[key] = value.to(model.device)\n",
    "\n",
    "  outputs = model(**inputs)\n",
    "  logits = outputs.logits\n",
    "  pred = torch.argmax(logits, dim=1)\n",
    "  return pred.item()\n",
    "\n",
    "print(\"original Review:\\n\")\n",
    "pprint(context)\n",
    "print(\"\\nPredicted Sentiment =\", getPrediction(context))\n",
    "print(\"########################################\")\n",
    "print(\"\\nAugmented Review:\\n\")\n",
    "pprint(augmented_review)\n",
    "print(\"\\nPredicted Sentiment =\", getPrediction(augmented_review))\n",
    "print(\"########################################\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W3D1_Tutorial2",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
