{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/tutorials/W2D3_ModernConvnets/instructor/W2D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> &nbsp; <a href=\"https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernConvnets/instructor/W2D3_Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open in Kaggle\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Tutorial 1: Learn how to use modern convnets\n",
    "\n",
    "**Week 2, Day 3: Modern Convnets**\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Laura Pede, Richard Vogg, Marissa Weis, Timo LÃ¼ddecke, Alexander Ecker (based on an initial version by Ben Heil)\n",
    "\n",
    "__Content reviewers:__ Arush Tagade, Polina Turishcheva, Yu-Fang Yang, Bettina Hein, Melvin Selim Atay, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Roberto Guidotti, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Anoop Kulkarni, Roberto Guidotti, Cary Murray, Gagana B, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2022 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "In this tutorial we are going to learn more about Convnets. More specifically, we will:\n",
    "\n",
    "1. Learn about modern CNNs and Transfer Learning.\n",
    "2. Understand how architectures incorporate ideas we have about the world.\n",
    "3. Understand the operating principles underlying the basic building blocks of modern CNNs.\n",
    "4. Understand the concept of transfer learning and learn to recognize opportunities for applying it.\n",
    "5. (Bonus) Understand the speed vs. accuracy trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Tutorial slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src=f\"https://mfr.ca-1.osf.io/render?url=https://osf.io/tzfsn/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "These are the slides for all videos in this tutorial. If you want to download locally the slides, click [here](https://osf.io/tzfsn/download)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install Pillow --quiet\n",
    "\n",
    "!pip install git+https://github.com/NeuromatchAcademy/evaltools --quiet\n",
    "from evaltools.airtable import AirtableForm\n",
    "\n",
    "# Generate airtable form\n",
    "atform = AirtableForm('appn7VdPRseSoMXEG','W2D2_T1','https://portal.neuromatchacademy.org/api/redirect/to/96ec754b-e76b-43f7-903e-76df0ac63749')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import IPython\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import AlexNet\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import ipywidgets as widgets       # Interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# For DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# Inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 1: Modern CNNs and Transfer Learning\n",
    "\n",
    "*Time estimate: ~25mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 1: Modern CNNs and Transfer Learning\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Wf4y157wE\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"mfOd2EKzscM\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 1: Modern CNNs and Transfer Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Images are high dimensional. That is to say that `image_length` * `image_width` * `image_channels` is a big number, and multiplying that big number by a normal sized fully-connected layer leads to a ton of parameters to learn. Yesterday, we learned about convolutional neural networks, one way of working around high dimensionality in images and other domains. \n",
    "\n",
    "The widget below (i.e., *Interactive Demo 1*) calculates the parameters required for a single convolutional or fully connected layer that operates on an image of a certain height and width.\n",
    "\n",
    "Recall that, the number of parameters of a convolutional layer $l$ are calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{num_of_params}_l = \\left[ \\left( H \\times W \\times K_{l-1} \\right) + 1 \\right] \\times K_l\n",
    "\\end{equation}\n",
    "\n",
    "where $H$ denotes the shape of the height of the filter, $W$ the shape of the width of the filter, and $K_l$ denotes the number of the filters in the $l$-th layer. The added $1$ is because of the bias term for each filter.\n",
    "\n",
    "While a fully connected layer contains:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{num_of_params}_l = \\left[ \\left( N_{l-1} \\times N_l \\right) + 1 \\times N_l \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where $N_l$ denotes the number of nodes in the $l$-th layer.\n",
    "\n",
    "Adjust the sliders to gain an intuition for how different model and data characteristics affect the number of parameters your model need to fit.\n",
    "\n",
    "Note: these classes are designed to show parameter scaling in the first layer of a network, to be actually useful they would need more layers, an activation function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Fully connected network with the following structure:\n",
    "  nn.Linear(self.input_size, 256)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Initialize parameters of FullyConnectedNet\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(FullyConnectedNet, self).__init__()\n",
    "\n",
    "    image_width = 128\n",
    "    image_channels = 3\n",
    "    self.input_size = image_channels * image_width ** 2\n",
    "\n",
    "    self.fc1 = nn.Linear(self.input_size, 256)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass of FullyConnectedNet\n",
    "\n",
    "    Args:\n",
    "      x: torch.tensor\n",
    "        Input data\n",
    "\n",
    "    Returns:\n",
    "      x: torch.tensor\n",
    "        Output from FullyConnectedNet\n",
    "    \"\"\"\n",
    "    x = x.view(-1, self.input_size)\n",
    "    return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  \"\"\"\n",
    "  Convolutional Neural Network\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    \"\"\"\n",
    "    Initialize parameters of ConvNet\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(ConvNet, self).__init__()\n",
    "\n",
    "    self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                            out_channels=256,\n",
    "                            kernel_size=(3, 3),\n",
    "                            padding=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass of ConvNet\n",
    "\n",
    "    Args:\n",
    "      x: torch.tensor\n",
    "        Input data\n",
    "\n",
    "    Returns:\n",
    "      x: torch.tensor\n",
    "        Output after passing x through Conv2d layer\n",
    "    \"\"\"\n",
    "    return self.conv1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 1: Calculate number of parameters in FCNN vs ConvNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Write a function that calculates the number of parameters of a given network. Apply the function to the above defined fully-connected network and convolutional network and compare the parameter counts.\n",
    "\n",
    "**Hint:** `torch.numel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "def get_parameter_count(network):\n",
    "  \"\"\"\n",
    "  Calculate the number of parameters used by the fully connected/convolutional network.\n",
    "  Hint: Casting the result of network.parameters() to a list may make it\n",
    "        easier to work with\n",
    "\n",
    "  Args:\n",
    "    network: nn.module\n",
    "      Network to calculate the parameters of fully connected/convolutional network\n",
    "\n",
    "  Returns:\n",
    "    param_count: int\n",
    "      The number of parameters in the network\n",
    "  \"\"\"\n",
    "\n",
    "  ####################################################################\n",
    "  # Fill in all missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"Convolution math\")\n",
    "  ####################################################################\n",
    "  # Get the network's parameters\n",
    "  parameters = ...\n",
    "\n",
    "  param_count = 0\n",
    "  # Loop over all layers\n",
    "  for layer in parameters:\n",
    "    param_count += ...\n",
    "\n",
    "  return param_count\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 1: Calculate number of parameters in FCNN vs ConvNet')\n",
    "\n",
    "# Initialize networks\n",
    "fccnet = FullyConnectedNet()\n",
    "convnet = ConvNet()\n",
    "## Apply the above defined function to both networks by uncommenting the following lines\n",
    "# print(f\"FCCN parameter count: {get_parameter_count(fccnet)}\")\n",
    "# print(f\"ConvNet parameter count: {get_parameter_count(convnet)}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def get_parameter_count(network):\n",
    "  \"\"\"\n",
    "  Calculate the number of parameters used by the fully connected/convolutional network.\n",
    "  Hint: Casting the result of network.parameters() to a list may make it\n",
    "        easier to work with\n",
    "\n",
    "  Args:\n",
    "    network: nn.module\n",
    "      Network to calculate the parameters of fully connected/convolutional network\n",
    "\n",
    "  Returns:\n",
    "    param_count: int\n",
    "      The number of parameters in the network\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the network's parameters\n",
    "  parameters = network.parameters()\n",
    "\n",
    "  param_count = 0\n",
    "  # Loop over all layers\n",
    "  for layer in parameters:\n",
    "    param_count += torch.numel(layer)\n",
    "\n",
    "  return param_count\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 1: Calculate number of parameters in FCNN vs ConvNet')\n",
    "\n",
    "# Initialize networks\n",
    "fccnet = FullyConnectedNet()\n",
    "convnet = ConvNet()\n",
    "## Apply the above defined function to both networks by uncommenting the following lines\n",
    "print(f\"FCCN parameter count: {get_parameter_count(fccnet)}\")\n",
    "print(f\"ConvNet parameter count: {get_parameter_count(convnet)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "FCCN parameter count: 12583168\n",
    "ConvNet parameter count: 7168\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interactive Demo 1: Check your results\n",
    "The widget below calculates the number of parameters in a FCNN and CNN with the same architecture as our models above. Our models had an input image that was 128x128, and used 256 filters (or 256 nodes in the FCNN case). Check that the calculations you made above are correct.\n",
    "\n",
    "Note how few parameters the convolutional networks take, especially as you increase the input image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Parameter Calculator\n",
    "# @markdown Run this cell to enable the widget!\n",
    "\n",
    "def calculate_parameters(filter_count, image_width,\n",
    "                         fcnn_nodes):\n",
    "  \"\"\"\n",
    "  Implement how parameters\n",
    "  scale as a function of image size\n",
    "  between convnets and FCNN\n",
    "\n",
    "  Args:\n",
    "    filter_count: int\n",
    "      Number of filters\n",
    "    image_width: int\n",
    "      Width of image\n",
    "    fcnn_nodes: int\n",
    "      Number of fCNN nodes\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  filter_width = 3\n",
    "  image_channels = 3\n",
    "\n",
    "  # Assuming a square, RGB image\n",
    "  image_area = image_width ** 2\n",
    "  image_volume = image_area * image_channels\n",
    "\n",
    "  # If we're using padding=same, the output of a\n",
    "  # convnet will be the same shape\n",
    "  # as the original image, but with more features\n",
    "  fcnn_parameters = image_volume * fcnn_nodes\n",
    "  cnn_parameters = image_channels * filter_count * filter_width ** 2\n",
    "\n",
    "  # Add bias\n",
    "  fcnn_parameters += fcnn_nodes\n",
    "  cnn_parameters += filter_count\n",
    "\n",
    "  print(f\"CNN parameters: {cnn_parameters}\")\n",
    "  print(f\"Fully Connected parameters: {fcnn_parameters}\")\n",
    "\n",
    "  return None\n",
    "\n",
    "\n",
    "_ = widgets.interact(calculate_parameters,\n",
    "                     filter_count=(16, 512, 16),\n",
    "                     image_width=(16, 512, 16),\n",
    "                     fcnn_nodes=(16, 512, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "--- \n",
    "# Section 2: The History of Convnets\n",
    "\n",
    "*Time estimate: ~15mins*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Convolutional neural networks have been around for a long time. [The first CNN model](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) was published in 1980, and was based on ideas in neuroscience that [predated it by decades](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1359523/). Why is it then that [AlexNet](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html), a CNN model published in 2012, is generally considered to mark the start of the deep learning revolution?\n",
    "\n",
    "Watch the video below to get a better idea of the role that hardware and the internet have played in progressing deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 2: History of convnets\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1364y167Qy\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"xtoLjKSPrUQ\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 2: History of convnets')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Think! 2: Challenges of improving CNNs\n",
    "As we shall see today, the story of deep learning and CNNs has been one of scaling networks: making them bigger and deeper.\n",
    "\n",
    "Based on what you know so far from previous days, what challenges might researchers have faced when trying to scale up CNNs and applying them to different visual recognition tasks? Do you already have some ideas how these challenges might have been addressed?\n",
    "\n",
    "Discuss this with your group for ~10 minutes.\n",
    "\n",
    "(Hint: labeled data, compute and memory are all finite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q1', text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {},
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Challenge 1: limited data / overfitting. Limited amount of labeled data for many tasks beyond ImageNet.\n",
    "Labels are expensive, for many tasks we don't have enough of them. --> Large networks will overfit.\n",
    "[Solution: transfer learning: adapt networks trained on ImageNet to other tasks]\n",
    "\n",
    "Challenge 2: hardware limitations. Making networks bigger/deeper will increase\n",
    "compute and memory requirements. --> There are physical limits what can be done, and completed in reasonable time.\n",
    "[Solution: more efficient architectures than standard convolutions]\n",
    "\n",
    "Challenge 3: training deep networks \"out-of-the-box\" is unstable / training diverges\n",
    "[Solution:.1 mechanisms like batch normalization and architectures like ResNets]\n",
    "\n",
    "NOTE: Students cannot know any of the solutions to the problems from the material.\n",
    "The intention of the question is to have them discuss and think about the\n",
    "challenges (primarily the first two). The solutions are just provided for completeness for the tutors.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 3: Big and Deep Convnets\n",
    "\n",
    "*Time estimate: 18mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 3: AlexNet & VGG\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV12U4y1n7q5\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"ZB87qC7yPiE\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 3: AlexNet & VGG')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.1: Introduction to AlexNet\n",
    "\n",
    "AlexNet arguably marked the start of the current age of deep learning.\n",
    "It incorporates a number of the defining characteristics of successful DL today: deep networks, GPU-powered paralellization, and building blocks encoding task-specific priors.\n",
    "In this section you'll have the opportunity to play with AlexNet and see the world through its eyes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Import Alexnet\n",
    "# @markdown This cell gives you the `alexnet` model as well as the `input_image` and `input_batch` variables used below\n",
    "import requests, urllib\n",
    "\n",
    "# Original link: https://s3.amazonaws.com/pytorch/models/alexnet-owt-4df8aa71.pth\n",
    "state_dict = torch.hub.load_state_dict_from_url(\"https://osf.io/9dzeu/download\")\n",
    "\n",
    "alexnet = AlexNet()\n",
    "alexnet.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "url, filename = (\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernConvnets/static/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "                                 transforms.Resize(256),\n",
    "                                 transforms.CenterCrop(224),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                      std=[0.229, 0.224, 0.225]),\n",
    "                                 ])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "\n",
    "# Move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "  input_batch = input_batch.cuda()\n",
    "  alexnet.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 3.2: What does AlexNet learn?\n",
    "This code visualizes the top-layer filters learned by AlexNet.\n",
    "What do these filters remind you of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  params = list(alexnet.parameters())\n",
    "  fig, axs = plt.subplots(8, 8, figsize=(8, 8))\n",
    "  filters = []\n",
    "  for filter_index in range(params[0].shape[0]):\n",
    "    row_index = filter_index // 8\n",
    "    col_index = filter_index % 8\n",
    "\n",
    "    filter = params[0][filter_index,:,:,:]\n",
    "    filter_image = filter.permute(1, 2, 0).cpu()\n",
    "    scale = np.abs(filter_image).max()\n",
    "    scaled_image = filter_image / (2 * scale) + 0.5\n",
    "    filters.append(scaled_image.cpu())\n",
    "    axs[row_index, col_index].imshow(scaled_image.cpu())\n",
    "    axs[row_index, col_index].axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 3.2.1: Filter Similarity\n",
    "\n",
    "What do these filters remind you of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q2', text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\"\"\"\n",
    "Some of the filters look like edge detectors,\n",
    "as they are color insensitive and consist of vertical or horizontal patterns.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Interactive Demo 3.2: What does AlexNet see?\n",
    "One way of visualizing CNNs is to look at the output of individual filters for a given image. Below is a widget that lets you examine the outputs of various filters used in AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Image Widget Code\n",
    "# @markdown Run this cell to enable the widget\n",
    "\n",
    "def alexnet_intermediate_output(net, image):\n",
    "    \"\"\"\n",
    "    Function to extract AlexNet's intermediate output\n",
    "\n",
    "    Args:\n",
    "      net: nn.module\n",
    "        AlexNet instance\n",
    "      image: torch.tensor\n",
    "        Input features\n",
    "\n",
    "    Returns:\n",
    "      ReLU output on processing features\n",
    "    \"\"\"\n",
    "    return F.relu(net.features[0](image))\n",
    "\n",
    "\n",
    "def browse_images(input_batch, input_image):\n",
    "  \"\"\"\n",
    "  Helper function to browse images\n",
    "\n",
    "  Args:\n",
    "    input_batch: torch.tensor\n",
    "      Input batch\n",
    "    input_image: torch.tensor\n",
    "      Input features\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  intermediate_output = alexnet_intermediate_output(alexnet, input_batch)\n",
    "  n = intermediate_output.shape[1]\n",
    "\n",
    "  def view_image(i):\n",
    "    \"\"\"\n",
    "    Function to view incoming image frame\n",
    "\n",
    "    Args:\n",
    "      i: int\n",
    "        Iteration\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "      channel = intermediate_output[0, i, :].squeeze()\n",
    "      fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "      ax[0].imshow(input_image)\n",
    "      ax[1].imshow(filters[i])\n",
    "      ax[1].set_xlim([-22, 33])\n",
    "      ax[2].imshow(channel.cpu())\n",
    "      ax[0].set_title('Input image')\n",
    "      ax[1].set_title(f\"Filter {i}\")\n",
    "      ax[2].set_title(f\"Filter {i} on input image\")\n",
    "      [axi.set_axis_off() for axi in ax.ravel()]\n",
    "\n",
    "  widgets.interact(view_image, i=(0, n-1))\n",
    "\n",
    "\n",
    "browse_images(input_batch, input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 3.2.2 Filter Purpose\n",
    "What do these filters appear to be doing? Note that different filters play different roles so there are several good answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q3', text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Based on the areas that are highlighted in the outupt, some filters seem to be detecting edges,\n",
    "while others seem to react to the white color of the dog or the green of the background.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Further Reading\n",
    "If the question \"what are neural network filters looking for\" is at all interesting to you, or if you like geometric art, you'll enjoy [this post](https://distill.pub/2017/feature-visualization/) creating images that maximize output of various CNN neurons. There is also a good article showing what the space of images looks like as models train [here](https://distill.pub/2020/grand-tour/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 4: Convnets After AlexNet\n",
    "\n",
    "*Time estimate: ~25mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 4: Residual Networks (ResNets)\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1bf4y1j7od\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"EJSZnJyy4PI\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 4: Residual Networks (ResNets)')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "In this section we'll be working with a state of the art CNN model called [ResNet](https://arxiv.org/abs/1512.03385). ResNet has two particularly interesting features. First, it uses skip connections to avoid the vanishing gradient problem. Second, each block (collection of layers) in a ResNet can be treated as learning a residual function.\n",
    "\n",
    "Mathematically, a neural network can be thought of as a series of operations that maps an input (like an image of a dog) to an output (like the label \"dog\"). In math-speak a mapping from an input to an output is called a function. Neural networks are a flexible way of expressing that function. \n",
    "\n",
    "If you were to subtract out the true function mapping images to class labels from the function learned by a network, you'd be left with the residual error or \"residual function\". ResNets try to learn the original function, then the residual function, then the residual of the residual, and so on, using their residual blocks and adding them to the output of the preceeding layers.\n",
    "\n",
    "In this section we'll run several images through a pre-trained ResNet and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download imagenette\n",
    "import requests, tarfile, os\n",
    "\n",
    "fname = 'imagenette2-320'\n",
    "url = 'https://osf.io/mnve4/download'\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "  print(\"Data is being downloaded...\")\n",
    "  r = requests.get(url, stream=True)\n",
    "  with open(fname+'tgz', 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "\n",
    "  with tarfile.open(fname+'tgz', \"r\") as ft:\n",
    "    ft.extractall()\n",
    "\n",
    "  os.remove(fname+'tgz')\n",
    "  print(\"The download has been completed.\")\n",
    "else:\n",
    "  print(\"Data has already been downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Set Up Textual ImageNet labels\n",
    "dict_map={0: 'tench, Tinca tinca',\n",
    " 1: 'goldfish, Carassius auratus',\n",
    " 2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
    " 3: 'tiger shark, Galeocerdo cuvieri',\n",
    " 4: 'hammerhead, hammerhead shark',\n",
    " 5: 'electric ray, crampfish, numbfish, torpedo',\n",
    " 6: 'stingray',\n",
    " 7: 'cock',\n",
    " 8: 'hen',\n",
    " 9: 'ostrich, Struthio camelus',\n",
    " 10: 'brambling, Fringilla montifringilla',\n",
    " 11: 'goldfinch, Carduelis carduelis',\n",
    " 12: 'house finch, linnet, Carpodacus mexicanus',\n",
    " 13: 'junco, snowbird',\n",
    " 14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
    " 15: 'robin, American robin, Turdus migratorius',\n",
    " 16: 'bulbul',\n",
    " 17: 'jay',\n",
    " 18: 'magpie',\n",
    " 19: 'chickadee',\n",
    " 20: 'water ouzel, dipper',\n",
    " 21: 'kite',\n",
    " 22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
    " 23: 'vulture',\n",
    " 24: 'great grey owl, great gray owl, Strix nebulosa',\n",
    " 25: 'European fire salamander, Salamandra salamandra',\n",
    " 26: 'common newt, Triturus vulgaris',\n",
    " 27: 'eft',\n",
    " 28: 'spotted salamander, Ambystoma maculatum',\n",
    " 29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
    " 30: 'bullfrog, Rana catesbeiana',\n",
    " 31: 'tree frog, tree-frog',\n",
    " 32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
    " 33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
    " 34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
    " 35: 'mud turtle',\n",
    " 36: 'terrapin',\n",
    " 37: 'box turtle, box tortoise',\n",
    " 38: 'banded gecko',\n",
    " 39: 'common iguana, iguana, Iguana iguana',\n",
    " 40: 'American chameleon, anole, Anolis carolinensis',\n",
    " 41: 'whiptail, whiptail lizard',\n",
    " 42: 'agama',\n",
    " 43: 'frilled lizard, Chlamydosaurus kingi',\n",
    " 44: 'alligator lizard',\n",
    " 45: 'Gila monster, Heloderma suspectum',\n",
    " 46: 'green lizard, Lacerta viridis',\n",
    " 47: 'African chameleon, Chamaeleo chamaeleon',\n",
    " 48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
    " 49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
    " 50: 'American alligator, Alligator mississipiensis',\n",
    " 51: 'triceratops',\n",
    " 52: 'thunder snake, worm snake, Carphophis amoenus',\n",
    " 53: 'ringneck snake, ring-necked snake, ring snake',\n",
    " 54: 'hognose snake, puff adder, sand viper',\n",
    " 55: 'green snake, grass snake',\n",
    " 56: 'king snake, kingsnake',\n",
    " 57: 'garter snake, grass snake',\n",
    " 58: 'water snake',\n",
    " 59: 'vine snake',\n",
    " 60: 'night snake, Hypsiglena torquata',\n",
    " 61: 'boa constrictor, Constrictor constrictor',\n",
    " 62: 'rock python, rock snake, Python sebae',\n",
    " 63: 'Indian cobra, Naja naja',\n",
    " 64: 'green mamba',\n",
    " 65: 'sea snake',\n",
    " 66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
    " 67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
    " 68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
    " 69: 'trilobite',\n",
    " 70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
    " 71: 'scorpion',\n",
    " 72: 'black and gold garden spider, Argiope aurantia',\n",
    " 73: 'barn spider, Araneus cavaticus',\n",
    " 74: 'garden spider, Aranea diademata',\n",
    " 75: 'black widow, Latrodectus mactans',\n",
    " 76: 'tarantula',\n",
    " 77: 'wolf spider, hunting spider',\n",
    " 78: 'tick',\n",
    " 79: 'centipede',\n",
    " 80: 'black grouse',\n",
    " 81: 'ptarmigan',\n",
    " 82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
    " 83: 'prairie chicken, prairie grouse, prairie fowl',\n",
    " 84: 'peacock',\n",
    " 85: 'quail',\n",
    " 86: 'partridge',\n",
    " 87: 'African grey, African gray, Psittacus erithacus',\n",
    " 88: 'macaw',\n",
    " 89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
    " 90: 'lorikeet',\n",
    " 91: 'coucal',\n",
    " 92: 'bee eater',\n",
    " 93: 'hornbill',\n",
    " 94: 'hummingbird',\n",
    " 95: 'jacamar',\n",
    " 96: 'toucan',\n",
    " 97: 'drake',\n",
    " 98: 'red-breasted merganser, Mergus serrator',\n",
    " 99: 'goose',\n",
    " 100: 'black swan, Cygnus atratus',\n",
    " 101: 'tusker',\n",
    " 102: 'echidna, spiny anteater, anteater',\n",
    " 103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
    " 104: 'wallaby, brush kangaroo',\n",
    " 105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
    " 106: 'wombat',\n",
    " 107: 'jellyfish',\n",
    " 108: 'sea anemone, anemone',\n",
    " 109: 'brain coral',\n",
    " 110: 'flatworm, platyhelminth',\n",
    " 111: 'nematode, nematode worm, roundworm',\n",
    " 112: 'conch',\n",
    " 113: 'snail',\n",
    " 114: 'slug',\n",
    " 115: 'sea slug, nudibranch',\n",
    " 116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
    " 117: 'chambered nautilus, pearly nautilus, nautilus',\n",
    " 118: 'Dungeness crab, Cancer magister',\n",
    " 119: 'rock crab, Cancer irroratus',\n",
    " 120: 'fiddler crab',\n",
    " 121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
    " 122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
    " 123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
    " 124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
    " 125: 'hermit crab',\n",
    " 126: 'isopod',\n",
    " 127: 'white stork, Ciconia ciconia',\n",
    " 128: 'black stork, Ciconia nigra',\n",
    " 129: 'spoonbill',\n",
    " 130: 'flamingo',\n",
    " 131: 'little blue heron, Egretta caerulea',\n",
    " 132: 'American egret, great white heron, Egretta albus',\n",
    " 133: 'bittern',\n",
    " 134: 'crane',\n",
    " 135: 'limpkin, Aramus pictus',\n",
    " 136: 'European gallinule, Porphyrio porphyrio',\n",
    " 137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
    " 138: 'bustard',\n",
    " 139: 'ruddy turnstone, Arenaria interpres',\n",
    " 140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
    " 141: 'redshank, Tringa totanus',\n",
    " 142: 'dowitcher',\n",
    " 143: 'oystercatcher, oyster catcher',\n",
    " 144: 'pelican',\n",
    " 145: 'king penguin, Aptenodytes patagonica',\n",
    " 146: 'albatross, mollymawk',\n",
    " 147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
    " 148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
    " 149: 'dugong, Dugong dugon',\n",
    " 150: 'sea lion',\n",
    " 151: 'Chihuahua',\n",
    " 152: 'Japanese spaniel',\n",
    " 153: 'Maltese dog, Maltese terrier, Maltese',\n",
    " 154: 'Pekinese, Pekingese, Peke',\n",
    " 155: 'Shih-Tzu',\n",
    " 156: 'Blenheim spaniel',\n",
    " 157: 'papillon',\n",
    " 158: 'toy terrier',\n",
    " 159: 'Rhodesian ridgeback',\n",
    " 160: 'Afghan hound, Afghan',\n",
    " 161: 'basset, basset hound',\n",
    " 162: 'beagle',\n",
    " 163: 'bloodhound, sleuthhound',\n",
    " 164: 'bluetick',\n",
    " 165: 'black-and-tan coonhound',\n",
    " 166: 'Walker hound, Walker foxhound',\n",
    " 167: 'English foxhound',\n",
    " 168: 'redbone',\n",
    " 169: 'borzoi, Russian wolfhound',\n",
    " 170: 'Irish wolfhound',\n",
    " 171: 'Italian greyhound',\n",
    " 172: 'whippet',\n",
    " 173: 'Ibizan hound, Ibizan Podenco',\n",
    " 174: 'Norwegian elkhound, elkhound',\n",
    " 175: 'otterhound, otter hound',\n",
    " 176: 'Saluki, gazelle hound',\n",
    " 177: 'Scottish deerhound, deerhound',\n",
    " 178: 'Weimaraner',\n",
    " 179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
    " 180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
    " 181: 'Bedlington terrier',\n",
    " 182: 'Border terrier',\n",
    " 183: 'Kerry blue terrier',\n",
    " 184: 'Irish terrier',\n",
    " 185: 'Norfolk terrier',\n",
    " 186: 'Norwich terrier',\n",
    " 187: 'Yorkshire terrier',\n",
    " 188: 'wire-haired fox terrier',\n",
    " 189: 'Lakeland terrier',\n",
    " 190: 'Sealyham terrier, Sealyham',\n",
    " 191: 'Airedale, Airedale terrier',\n",
    " 192: 'cairn, cairn terrier',\n",
    " 193: 'Australian terrier',\n",
    " 194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
    " 195: 'Boston bull, Boston terrier',\n",
    " 196: 'miniature schnauzer',\n",
    " 197: 'giant schnauzer',\n",
    " 198: 'standard schnauzer',\n",
    " 199: 'Scotch terrier, Scottish terrier, Scottie',\n",
    " 200: 'Tibetan terrier, chrysanthemum dog',\n",
    " 201: 'silky terrier, Sydney silky',\n",
    " 202: 'soft-coated wheaten terrier',\n",
    " 203: 'West Highland white terrier',\n",
    " 204: 'Lhasa, Lhasa apso',\n",
    " 205: 'flat-coated retriever',\n",
    " 206: 'curly-coated retriever',\n",
    " 207: 'golden retriever',\n",
    " 208: 'Labrador retriever',\n",
    " 209: 'Chesapeake Bay retriever',\n",
    " 210: 'German short-haired pointer',\n",
    " 211: 'vizsla, Hungarian pointer',\n",
    " 212: 'English setter',\n",
    " 213: 'Irish setter, red setter',\n",
    " 214: 'Gordon setter',\n",
    " 215: 'Brittany spaniel',\n",
    " 216: 'clumber, clumber spaniel',\n",
    " 217: 'English springer, English springer spaniel',\n",
    " 218: 'Welsh springer spaniel',\n",
    " 219: 'cocker spaniel, English cocker spaniel, cocker',\n",
    " 220: 'Sussex spaniel',\n",
    " 221: 'Irish water spaniel',\n",
    " 222: 'kuvasz',\n",
    " 223: 'schipperke',\n",
    " 224: 'groenendael',\n",
    " 225: 'malinois',\n",
    " 226: 'briard',\n",
    " 227: 'kelpie',\n",
    " 228: 'komondor',\n",
    " 229: 'Old English sheepdog, bobtail',\n",
    " 230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
    " 231: 'collie',\n",
    " 232: 'Border collie',\n",
    " 233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
    " 234: 'Rottweiler',\n",
    " 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
    " 236: 'Doberman, Doberman pinscher',\n",
    " 237: 'miniature pinscher',\n",
    " 238: 'Greater Swiss Mountain dog',\n",
    " 239: 'Bernese mountain dog',\n",
    " 240: 'Appenzeller',\n",
    " 241: 'EntleBucher',\n",
    " 242: 'boxer',\n",
    " 243: 'bull mastiff',\n",
    " 244: 'Tibetan mastiff',\n",
    " 245: 'French bulldog',\n",
    " 246: 'Great Dane',\n",
    " 247: 'Saint Bernard, St Bernard',\n",
    " 248: 'Eskimo dog, husky',\n",
    " 249: 'malamute, malemute, Alaskan malamute',\n",
    " 250: 'Siberian husky',\n",
    " 251: 'dalmatian, coach dog, carriage dog',\n",
    " 252: 'affenpinscher, monkey pinscher, monkey dog',\n",
    " 253: 'basenji',\n",
    " 254: 'pug, pug-dog',\n",
    " 255: 'Leonberg',\n",
    " 256: 'Newfoundland, Newfoundland dog',\n",
    " 257: 'Great Pyrenees',\n",
    " 258: 'Samoyed, Samoyede',\n",
    " 259: 'Pomeranian',\n",
    " 260: 'chow, chow chow',\n",
    " 261: 'keeshond',\n",
    " 262: 'Brabancon griffon',\n",
    " 263: 'Pembroke, Pembroke Welsh corgi',\n",
    " 264: 'Cardigan, Cardigan Welsh corgi',\n",
    " 265: 'toy poodle',\n",
    " 266: 'miniature poodle',\n",
    " 267: 'standard poodle',\n",
    " 268: 'Mexican hairless',\n",
    " 269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
    " 270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
    " 271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
    " 272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
    " 273: 'dingo, warrigal, warragal, Canis dingo',\n",
    " 274: 'dhole, Cuon alpinus',\n",
    " 275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
    " 276: 'hyena, hyaena',\n",
    " 277: 'red fox, Vulpes vulpes',\n",
    " 278: 'kit fox, Vulpes macrotis',\n",
    " 279: 'Arctic fox, white fox, Alopex lagopus',\n",
    " 280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
    " 281: 'tabby, tabby cat',\n",
    " 282: 'tiger cat',\n",
    " 283: 'Persian cat',\n",
    " 284: 'Siamese cat, Siamese',\n",
    " 285: 'Egyptian cat',\n",
    " 286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
    " 287: 'lynx, catamount',\n",
    " 288: 'leopard, Panthera pardus',\n",
    " 289: 'snow leopard, ounce, Panthera uncia',\n",
    " 290: 'jaguar, panther, Panthera onca, Felis onca',\n",
    " 291: 'lion, king of beasts, Panthera leo',\n",
    " 292: 'tiger, Panthera tigris',\n",
    " 293: 'cheetah, chetah, Acinonyx jubatus',\n",
    " 294: 'brown bear, bruin, Ursus arctos',\n",
    " 295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
    " 296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
    " 297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
    " 298: 'mongoose',\n",
    " 299: 'meerkat, mierkat',\n",
    " 300: 'tiger beetle',\n",
    " 301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
    " 302: 'ground beetle, carabid beetle',\n",
    " 303: 'long-horned beetle, longicorn, longicorn beetle',\n",
    " 304: 'leaf beetle, chrysomelid',\n",
    " 305: 'dung beetle',\n",
    " 306: 'rhinoceros beetle',\n",
    " 307: 'weevil',\n",
    " 308: 'fly',\n",
    " 309: 'bee',\n",
    " 310: 'ant, emmet, pismire',\n",
    " 311: 'grasshopper, hopper',\n",
    " 312: 'cricket',\n",
    " 313: 'walking stick, walkingstick, stick insect',\n",
    " 314: 'cockroach, roach',\n",
    " 315: 'mantis, mantid',\n",
    " 316: 'cicada, cicala',\n",
    " 317: 'leafhopper',\n",
    " 318: 'lacewing, lacewing fly',\n",
    " 319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
    " 320: 'damselfly',\n",
    " 321: 'admiral',\n",
    " 322: 'ringlet, ringlet butterfly',\n",
    " 323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
    " 324: 'cabbage butterfly',\n",
    " 325: 'sulphur butterfly, sulfur butterfly',\n",
    " 326: 'lycaenid, lycaenid butterfly',\n",
    " 327: 'starfish, sea star',\n",
    " 328: 'sea urchin',\n",
    " 329: 'sea cucumber, holothurian',\n",
    " 330: 'wood rabbit, cottontail, cottontail rabbit',\n",
    " 331: 'hare',\n",
    " 332: 'Angora, Angora rabbit',\n",
    " 333: 'hamster',\n",
    " 334: 'porcupine, hedgehog',\n",
    " 335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
    " 336: 'marmot',\n",
    " 337: 'beaver',\n",
    " 338: 'guinea pig, Cavia cobaya',\n",
    " 339: 'sorrel',\n",
    " 340: 'zebra',\n",
    " 341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
    " 342: 'wild boar, boar, Sus scrofa',\n",
    " 343: 'warthog',\n",
    " 344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
    " 345: 'ox',\n",
    " 346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
    " 347: 'bison',\n",
    " 348: 'ram, tup',\n",
    " 349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
    " 350: 'ibex, Capra ibex',\n",
    " 351: 'hartebeest',\n",
    " 352: 'impala, Aepyceros melampus',\n",
    " 353: 'gazelle',\n",
    " 354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
    " 355: 'llama',\n",
    " 356: 'weasel',\n",
    " 357: 'mink',\n",
    " 358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
    " 359: 'black-footed ferret, ferret, Mustela nigripes',\n",
    " 360: 'otter',\n",
    " 361: 'skunk, polecat, wood pussy',\n",
    " 362: 'badger',\n",
    " 363: 'armadillo',\n",
    " 364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
    " 365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
    " 366: 'gorilla, Gorilla gorilla',\n",
    " 367: 'chimpanzee, chimp, Pan troglodytes',\n",
    " 368: 'gibbon, Hylobates lar',\n",
    " 369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
    " 370: 'guenon, guenon monkey',\n",
    " 371: 'patas, hussar monkey, Erythrocebus patas',\n",
    " 372: 'baboon',\n",
    " 373: 'macaque',\n",
    " 374: 'langur',\n",
    " 375: 'colobus, colobus monkey',\n",
    " 376: 'proboscis monkey, Nasalis larvatus',\n",
    " 377: 'marmoset',\n",
    " 378: 'capuchin, ringtail, Cebus capucinus',\n",
    " 379: 'howler monkey, howler',\n",
    " 380: 'titi, titi monkey',\n",
    " 381: 'spider monkey, Ateles geoffroyi',\n",
    " 382: 'squirrel monkey, Saimiri sciureus',\n",
    " 383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
    " 384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
    " 385: 'Indian elephant, Elephas maximus',\n",
    " 386: 'African elephant, Loxodonta africana',\n",
    " 387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
    " 388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
    " 389: 'barracouta, snoek',\n",
    " 390: 'eel',\n",
    " 391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
    " 392: 'rock beauty, Holocanthus tricolor',\n",
    " 393: 'anemone fish',\n",
    " 394: 'sturgeon',\n",
    " 395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
    " 396: 'lionfish',\n",
    " 397: 'puffer, pufferfish, blowfish, globefish',\n",
    " 398: 'abacus',\n",
    " 399: 'abaya',\n",
    " 400: \"academic gown, academic robe, judge's robe\",\n",
    " 401: 'accordion, piano accordion, squeeze box',\n",
    " 402: 'acoustic guitar',\n",
    " 403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
    " 404: 'airliner',\n",
    " 405: 'airship, dirigible',\n",
    " 406: 'altar',\n",
    " 407: 'ambulance',\n",
    " 408: 'amphibian, amphibious vehicle',\n",
    " 409: 'analog clock',\n",
    " 410: 'apiary, bee house',\n",
    " 411: 'apron',\n",
    " 412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
    " 413: 'assault rifle, assault gun',\n",
    " 414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
    " 415: 'bakery, bakeshop, bakehouse',\n",
    " 416: 'balance beam, beam',\n",
    " 417: 'balloon',\n",
    " 418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
    " 419: 'Band Aid',\n",
    " 420: 'banjo',\n",
    " 421: 'bannister, banister, balustrade, balusters, handrail',\n",
    " 422: 'barbell',\n",
    " 423: 'barber chair',\n",
    " 424: 'barbershop',\n",
    " 425: 'barn',\n",
    " 426: 'barometer',\n",
    " 427: 'barrel, cask',\n",
    " 428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
    " 429: 'baseball',\n",
    " 430: 'basketball',\n",
    " 431: 'bassinet',\n",
    " 432: 'bassoon',\n",
    " 433: 'bathing cap, swimming cap',\n",
    " 434: 'bath towel',\n",
    " 435: 'bathtub, bathing tub, bath, tub',\n",
    " 436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
    " 437: 'beacon, lighthouse, beacon light, pharos',\n",
    " 438: 'beaker',\n",
    " 439: 'bearskin, busby, shako',\n",
    " 440: 'beer bottle',\n",
    " 441: 'beer glass',\n",
    " 442: 'bell cote, bell cot',\n",
    " 443: 'bib',\n",
    " 444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
    " 445: 'bikini, two-piece',\n",
    " 446: 'binder, ring-binder',\n",
    " 447: 'binoculars, field glasses, opera glasses',\n",
    " 448: 'birdhouse',\n",
    " 449: 'boathouse',\n",
    " 450: 'bobsled, bobsleigh, bob',\n",
    " 451: 'bolo tie, bolo, bola tie, bola',\n",
    " 452: 'bonnet, poke bonnet',\n",
    " 453: 'bookcase',\n",
    " 454: 'bookshop, bookstore, bookstall',\n",
    " 455: 'bottlecap',\n",
    " 456: 'bow',\n",
    " 457: 'bow tie, bow-tie, bowtie',\n",
    " 458: 'brass, memorial tablet, plaque',\n",
    " 459: 'brassiere, bra, bandeau',\n",
    " 460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
    " 461: 'breastplate, aegis, egis',\n",
    " 462: 'broom',\n",
    " 463: 'bucket, pail',\n",
    " 464: 'buckle',\n",
    " 465: 'bulletproof vest',\n",
    " 466: 'bullet train, bullet',\n",
    " 467: 'butcher shop, meat market',\n",
    " 468: 'cab, hack, taxi, taxicab',\n",
    " 469: 'caldron, cauldron',\n",
    " 470: 'candle, taper, wax light',\n",
    " 471: 'cannon',\n",
    " 472: 'canoe',\n",
    " 473: 'can opener, tin opener',\n",
    " 474: 'cardigan',\n",
    " 475: 'car mirror',\n",
    " 476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
    " 477: \"carpenter's kit, tool kit\",\n",
    " 478: 'carton',\n",
    " 479: 'car wheel',\n",
    " 480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
    " 481: 'cassette',\n",
    " 482: 'cassette player',\n",
    " 483: 'castle',\n",
    " 484: 'catamaran',\n",
    " 485: 'CD player',\n",
    " 486: 'cello, violoncello',\n",
    " 487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
    " 488: 'chain',\n",
    " 489: 'chainlink fence',\n",
    " 490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
    " 491: 'chain saw, chainsaw',\n",
    " 492: 'chest',\n",
    " 493: 'chiffonier, commode',\n",
    " 494: 'chime, bell, gong',\n",
    " 495: 'china cabinet, china closet',\n",
    " 496: 'Christmas stocking',\n",
    " 497: 'church, church building',\n",
    " 498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
    " 499: 'cleaver, meat cleaver, chopper',\n",
    " 500: 'cliff dwelling',\n",
    " 501: 'cloak',\n",
    " 502: 'clog, geta, patten, sabot',\n",
    " 503: 'cocktail shaker',\n",
    " 504: 'coffee mug',\n",
    " 505: 'coffeepot',\n",
    " 506: 'coil, spiral, volute, whorl, helix',\n",
    " 507: 'combination lock',\n",
    " 508: 'computer keyboard, keypad',\n",
    " 509: 'confectionery, confectionary, candy store',\n",
    " 510: 'container ship, containership, container vessel',\n",
    " 511: 'convertible',\n",
    " 512: 'corkscrew, bottle screw',\n",
    " 513: 'cornet, horn, trumpet, trump',\n",
    " 514: 'cowboy boot',\n",
    " 515: 'cowboy hat, ten-gallon hat',\n",
    " 516: 'cradle',\n",
    " 517: 'crane',\n",
    " 518: 'crash helmet',\n",
    " 519: 'crate',\n",
    " 520: 'crib, cot',\n",
    " 521: 'Crock Pot',\n",
    " 522: 'croquet ball',\n",
    " 523: 'crutch',\n",
    " 524: 'cuirass',\n",
    " 525: 'dam, dike, dyke',\n",
    " 526: 'desk',\n",
    " 527: 'desktop computer',\n",
    " 528: 'dial telephone, dial phone',\n",
    " 529: 'diaper, nappy, napkin',\n",
    " 530: 'digital clock',\n",
    " 531: 'digital watch',\n",
    " 532: 'dining table, board',\n",
    " 533: 'dishrag, dishcloth',\n",
    " 534: 'dishwasher, dish washer, dishwashing machine',\n",
    " 535: 'disk brake, disc brake',\n",
    " 536: 'dock, dockage, docking facility',\n",
    " 537: 'dogsled, dog sled, dog sleigh',\n",
    " 538: 'dome',\n",
    " 539: 'doormat, welcome mat',\n",
    " 540: 'drilling platform, offshore rig',\n",
    " 541: 'drum, membranophone, tympan',\n",
    " 542: 'drumstick',\n",
    " 543: 'dumbbell',\n",
    " 544: 'Dutch oven',\n",
    " 545: 'electric fan, blower',\n",
    " 546: 'electric guitar',\n",
    " 547: 'electric locomotive',\n",
    " 548: 'entertainment center',\n",
    " 549: 'envelope',\n",
    " 550: 'espresso maker',\n",
    " 551: 'face powder',\n",
    " 552: 'feather boa, boa',\n",
    " 553: 'file, file cabinet, filing cabinet',\n",
    " 554: 'fireboat',\n",
    " 555: 'fire engine, fire truck',\n",
    " 556: 'fire screen, fireguard',\n",
    " 557: 'flagpole, flagstaff',\n",
    " 558: 'flute, transverse flute',\n",
    " 559: 'folding chair',\n",
    " 560: 'football helmet',\n",
    " 561: 'forklift',\n",
    " 562: 'fountain',\n",
    " 563: 'fountain pen',\n",
    " 564: 'four-poster',\n",
    " 565: 'freight car',\n",
    " 566: 'French horn, horn',\n",
    " 567: 'frying pan, frypan, skillet',\n",
    " 568: 'fur coat',\n",
    " 569: 'garbage truck, dustcart',\n",
    " 570: 'gasmask, respirator, gas helmet',\n",
    " 571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
    " 572: 'goblet',\n",
    " 573: 'go-kart',\n",
    " 574: 'golf ball',\n",
    " 575: 'golfcart, golf cart',\n",
    " 576: 'gondola',\n",
    " 577: 'gong, tam-tam',\n",
    " 578: 'gown',\n",
    " 579: 'grand piano, grand',\n",
    " 580: 'greenhouse, nursery, glasshouse',\n",
    " 581: 'grille, radiator grille',\n",
    " 582: 'grocery store, grocery, food market, market',\n",
    " 583: 'guillotine',\n",
    " 584: 'hair slide',\n",
    " 585: 'hair spray',\n",
    " 586: 'half track',\n",
    " 587: 'hammer',\n",
    " 588: 'hamper',\n",
    " 589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
    " 590: 'hand-held computer, hand-held microcomputer',\n",
    " 591: 'handkerchief, hankie, hanky, hankey',\n",
    " 592: 'hard disc, hard disk, fixed disk',\n",
    " 593: 'harmonica, mouth organ, harp, mouth harp',\n",
    " 594: 'harp',\n",
    " 595: 'harvester, reaper',\n",
    " 596: 'hatchet',\n",
    " 597: 'holster',\n",
    " 598: 'home theater, home theatre',\n",
    " 599: 'honeycomb',\n",
    " 600: 'hook, claw',\n",
    " 601: 'hoopskirt, crinoline',\n",
    " 602: 'horizontal bar, high bar',\n",
    " 603: 'horse cart, horse-cart',\n",
    " 604: 'hourglass',\n",
    " 605: 'iPod',\n",
    " 606: 'iron, smoothing iron',\n",
    " 607: \"jack-o'-lantern\",\n",
    " 608: 'jean, blue jean, denim',\n",
    " 609: 'jeep, landrover',\n",
    " 610: 'jersey, T-shirt, tee shirt',\n",
    " 611: 'jigsaw puzzle',\n",
    " 612: 'jinrikisha, ricksha, rickshaw',\n",
    " 613: 'joystick',\n",
    " 614: 'kimono',\n",
    " 615: 'knee pad',\n",
    " 616: 'knot',\n",
    " 617: 'lab coat, laboratory coat',\n",
    " 618: 'ladle',\n",
    " 619: 'lampshade, lamp shade',\n",
    " 620: 'laptop, laptop computer',\n",
    " 621: 'lawn mower, mower',\n",
    " 622: 'lens cap, lens cover',\n",
    " 623: 'letter opener, paper knife, paperknife',\n",
    " 624: 'library',\n",
    " 625: 'lifeboat',\n",
    " 626: 'lighter, light, igniter, ignitor',\n",
    " 627: 'limousine, limo',\n",
    " 628: 'liner, ocean liner',\n",
    " 629: 'lipstick, lip rouge',\n",
    " 630: 'Loafer',\n",
    " 631: 'lotion',\n",
    " 632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
    " 633: \"loupe, jeweler's loupe\",\n",
    " 634: 'lumbermill, sawmill',\n",
    " 635: 'magnetic compass',\n",
    " 636: 'mailbag, postbag',\n",
    " 637: 'mailbox, letter box',\n",
    " 638: 'maillot',\n",
    " 639: 'maillot, tank suit',\n",
    " 640: 'manhole cover',\n",
    " 641: 'maraca',\n",
    " 642: 'marimba, xylophone',\n",
    " 643: 'mask',\n",
    " 644: 'matchstick',\n",
    " 645: 'maypole',\n",
    " 646: 'maze, labyrinth',\n",
    " 647: 'measuring cup',\n",
    " 648: 'medicine chest, medicine cabinet',\n",
    " 649: 'megalith, megalithic structure',\n",
    " 650: 'microphone, mike',\n",
    " 651: 'microwave, microwave oven',\n",
    " 652: 'military uniform',\n",
    " 653: 'milk can',\n",
    " 654: 'minibus',\n",
    " 655: 'miniskirt, mini',\n",
    " 656: 'minivan',\n",
    " 657: 'missile',\n",
    " 658: 'mitten',\n",
    " 659: 'mixing bowl',\n",
    " 660: 'mobile home, manufactured home',\n",
    " 661: 'Model T',\n",
    " 662: 'modem',\n",
    " 663: 'monastery',\n",
    " 664: 'monitor',\n",
    " 665: 'moped',\n",
    " 666: 'mortar',\n",
    " 667: 'mortarboard',\n",
    " 668: 'mosque',\n",
    " 669: 'mosquito net',\n",
    " 670: 'motor scooter, scooter',\n",
    " 671: 'mountain bike, all-terrain bike, off-roader',\n",
    " 672: 'mountain tent',\n",
    " 673: 'mouse, computer mouse',\n",
    " 674: 'mousetrap',\n",
    " 675: 'moving van',\n",
    " 676: 'muzzle',\n",
    " 677: 'nail',\n",
    " 678: 'neck brace',\n",
    " 679: 'necklace',\n",
    " 680: 'nipple',\n",
    " 681: 'notebook, notebook computer',\n",
    " 682: 'obelisk',\n",
    " 683: 'oboe, hautboy, hautbois',\n",
    " 684: 'ocarina, sweet potato',\n",
    " 685: 'odometer, hodometer, mileometer, milometer',\n",
    " 686: 'oil filter',\n",
    " 687: 'organ, pipe organ',\n",
    " 688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
    " 689: 'overskirt',\n",
    " 690: 'oxcart',\n",
    " 691: 'oxygen mask',\n",
    " 692: 'packet',\n",
    " 693: 'paddle, boat paddle',\n",
    " 694: 'paddlewheel, paddle wheel',\n",
    " 695: 'padlock',\n",
    " 696: 'paintbrush',\n",
    " 697: \"pajama, pyjama, pj's, jammies\",\n",
    " 698: 'palace',\n",
    " 699: 'panpipe, pandean pipe, syrinx',\n",
    " 700: 'paper towel',\n",
    " 701: 'parachute, chute',\n",
    " 702: 'parallel bars, bars',\n",
    " 703: 'park bench',\n",
    " 704: 'parking meter',\n",
    " 705: 'passenger car, coach, carriage',\n",
    " 706: 'patio, terrace',\n",
    " 707: 'pay-phone, pay-station',\n",
    " 708: 'pedestal, plinth, footstall',\n",
    " 709: 'pencil box, pencil case',\n",
    " 710: 'pencil sharpener',\n",
    " 711: 'perfume, essence',\n",
    " 712: 'Petri dish',\n",
    " 713: 'photocopier',\n",
    " 714: 'pick, plectrum, plectron',\n",
    " 715: 'pickelhaube',\n",
    " 716: 'picket fence, paling',\n",
    " 717: 'pickup, pickup truck',\n",
    " 718: 'pier',\n",
    " 719: 'piggy bank, penny bank',\n",
    " 720: 'pill bottle',\n",
    " 721: 'pillow',\n",
    " 722: 'ping-pong ball',\n",
    " 723: 'pinwheel',\n",
    " 724: 'pirate, pirate ship',\n",
    " 725: 'pitcher, ewer',\n",
    " 726: \"plane, carpenter's plane, woodworking plane\",\n",
    " 727: 'planetarium',\n",
    " 728: 'plastic bag',\n",
    " 729: 'plate rack',\n",
    " 730: 'plow, plough',\n",
    " 731: \"plunger, plumber's helper\",\n",
    " 732: 'Polaroid camera, Polaroid Land camera',\n",
    " 733: 'pole',\n",
    " 734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
    " 735: 'poncho',\n",
    " 736: 'pool table, billiard table, snooker table',\n",
    " 737: 'pop bottle, soda bottle',\n",
    " 738: 'pot, flowerpot',\n",
    " 739: \"potter's wheel\",\n",
    " 740: 'power drill',\n",
    " 741: 'prayer rug, prayer mat',\n",
    " 742: 'printer',\n",
    " 743: 'prison, prison house',\n",
    " 744: 'projectile, missile',\n",
    " 745: 'projector',\n",
    " 746: 'puck, hockey puck',\n",
    " 747: 'punching bag, punch bag, punching ball, punchball',\n",
    " 748: 'purse',\n",
    " 749: 'quill, quill pen',\n",
    " 750: 'quilt, comforter, comfort, puff',\n",
    " 751: 'racer, race car, racing car',\n",
    " 752: 'racket, racquet',\n",
    " 753: 'radiator',\n",
    " 754: 'radio, wireless',\n",
    " 755: 'radio telescope, radio reflector',\n",
    " 756: 'rain barrel',\n",
    " 757: 'recreational vehicle, RV, R.V.',\n",
    " 758: 'reel',\n",
    " 759: 'reflex camera',\n",
    " 760: 'refrigerator, icebox',\n",
    " 761: 'remote control, remote',\n",
    " 762: 'restaurant, eating house, eating place, eatery',\n",
    " 763: 'revolver, six-gun, six-shooter',\n",
    " 764: 'rifle',\n",
    " 765: 'rocking chair, rocker',\n",
    " 766: 'rotisserie',\n",
    " 767: 'rubber eraser, rubber, pencil eraser',\n",
    " 768: 'rugby ball',\n",
    " 769: 'rule, ruler',\n",
    " 770: 'running shoe',\n",
    " 771: 'safe',\n",
    " 772: 'safety pin',\n",
    " 773: 'saltshaker, salt shaker',\n",
    " 774: 'sandal',\n",
    " 775: 'sarong',\n",
    " 776: 'sax, saxophone',\n",
    " 777: 'scabbard',\n",
    " 778: 'scale, weighing machine',\n",
    " 779: 'school bus',\n",
    " 780: 'schooner',\n",
    " 781: 'scoreboard',\n",
    " 782: 'screen, CRT screen',\n",
    " 783: 'screw',\n",
    " 784: 'screwdriver',\n",
    " 785: 'seat belt, seatbelt',\n",
    " 786: 'sewing machine',\n",
    " 787: 'shield, buckler',\n",
    " 788: 'shoe shop, shoe-shop, shoe store',\n",
    " 789: 'shoji',\n",
    " 790: 'shopping basket',\n",
    " 791: 'shopping cart',\n",
    " 792: 'shovel',\n",
    " 793: 'shower cap',\n",
    " 794: 'shower curtain',\n",
    " 795: 'ski',\n",
    " 796: 'ski mask',\n",
    " 797: 'sleeping bag',\n",
    " 798: 'slide rule, slipstick',\n",
    " 799: 'sliding door',\n",
    " 800: 'slot, one-armed bandit',\n",
    " 801: 'snorkel',\n",
    " 802: 'snowmobile',\n",
    " 803: 'snowplow, snowplough',\n",
    " 804: 'soap dispenser',\n",
    " 805: 'soccer ball',\n",
    " 806: 'sock',\n",
    " 807: 'solar dish, solar collector, solar furnace',\n",
    " 808: 'sombrero',\n",
    " 809: 'soup bowl',\n",
    " 810: 'space bar',\n",
    " 811: 'space heater',\n",
    " 812: 'space shuttle',\n",
    " 813: 'spatula',\n",
    " 814: 'speedboat',\n",
    " 815: \"spider web, spider's web\",\n",
    " 816: 'spindle',\n",
    " 817: 'sports car, sport car',\n",
    " 818: 'spotlight, spot',\n",
    " 819: 'stage',\n",
    " 820: 'steam locomotive',\n",
    " 821: 'steel arch bridge',\n",
    " 822: 'steel drum',\n",
    " 823: 'stethoscope',\n",
    " 824: 'stole',\n",
    " 825: 'stone wall',\n",
    " 826: 'stopwatch, stop watch',\n",
    " 827: 'stove',\n",
    " 828: 'strainer',\n",
    " 829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
    " 830: 'stretcher',\n",
    " 831: 'studio couch, day bed',\n",
    " 832: 'stupa, tope',\n",
    " 833: 'submarine, pigboat, sub, U-boat',\n",
    " 834: 'suit, suit of clothes',\n",
    " 835: 'sundial',\n",
    " 836: 'sunglass',\n",
    " 837: 'sunglasses, dark glasses, shades',\n",
    " 838: 'sunscreen, sunblock, sun blocker',\n",
    " 839: 'suspension bridge',\n",
    " 840: 'swab, swob, mop',\n",
    " 841: 'sweatshirt',\n",
    " 842: 'swimming trunks, bathing trunks',\n",
    " 843: 'swing',\n",
    " 844: 'switch, electric switch, electrical switch',\n",
    " 845: 'syringe',\n",
    " 846: 'table lamp',\n",
    " 847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
    " 848: 'tape player',\n",
    " 849: 'teapot',\n",
    " 850: 'teddy, teddy bear',\n",
    " 851: 'television, television system',\n",
    " 852: 'tennis ball',\n",
    " 853: 'thatch, thatched roof',\n",
    " 854: 'theater curtain, theatre curtain',\n",
    " 855: 'thimble',\n",
    " 856: 'thresher, thrasher, threshing machine',\n",
    " 857: 'throne',\n",
    " 858: 'tile roof',\n",
    " 859: 'toaster',\n",
    " 860: 'tobacco shop, tobacconist shop, tobacconist',\n",
    " 861: 'toilet seat',\n",
    " 862: 'torch',\n",
    " 863: 'totem pole',\n",
    " 864: 'tow truck, tow car, wrecker',\n",
    " 865: 'toyshop',\n",
    " 866: 'tractor',\n",
    " 867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
    " 868: 'tray',\n",
    " 869: 'trench coat',\n",
    " 870: 'tricycle, trike, velocipede',\n",
    " 871: 'trimaran',\n",
    " 872: 'tripod',\n",
    " 873: 'triumphal arch',\n",
    " 874: 'trolleybus, trolley coach, trackless trolley',\n",
    " 875: 'trombone',\n",
    " 876: 'tub, vat',\n",
    " 877: 'turnstile',\n",
    " 878: 'typewriter keyboard',\n",
    " 879: 'umbrella',\n",
    " 880: 'unicycle, monocycle',\n",
    " 881: 'upright, upright piano',\n",
    " 882: 'vacuum, vacuum cleaner',\n",
    " 883: 'vase',\n",
    " 884: 'vault',\n",
    " 885: 'velvet',\n",
    " 886: 'vending machine',\n",
    " 887: 'vestment',\n",
    " 888: 'viaduct',\n",
    " 889: 'violin, fiddle',\n",
    " 890: 'volleyball',\n",
    " 891: 'waffle iron',\n",
    " 892: 'wall clock',\n",
    " 893: 'wallet, billfold, notecase, pocketbook',\n",
    " 894: 'wardrobe, closet, press',\n",
    " 895: 'warplane, military plane',\n",
    " 896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
    " 897: 'washer, automatic washer, washing machine',\n",
    " 898: 'water bottle',\n",
    " 899: 'water jug',\n",
    " 900: 'water tower',\n",
    " 901: 'whiskey jug',\n",
    " 902: 'whistle',\n",
    " 903: 'wig',\n",
    " 904: 'window screen',\n",
    " 905: 'window shade',\n",
    " 906: 'Windsor tie',\n",
    " 907: 'wine bottle',\n",
    " 908: 'wing',\n",
    " 909: 'wok',\n",
    " 910: 'wooden spoon',\n",
    " 911: 'wool, woolen, woollen',\n",
    " 912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
    " 913: 'wreck',\n",
    " 914: 'yawl',\n",
    " 915: 'yurt',\n",
    " 916: 'web site, website, internet site, site',\n",
    " 917: 'comic book',\n",
    " 918: 'crossword puzzle, crossword',\n",
    " 919: 'street sign',\n",
    " 920: 'traffic light, traffic signal, stoplight',\n",
    " 921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
    " 922: 'menu',\n",
    " 923: 'plate',\n",
    " 924: 'guacamole',\n",
    " 925: 'consomme',\n",
    " 926: 'hot pot, hotpot',\n",
    " 927: 'trifle',\n",
    " 928: 'ice cream, icecream',\n",
    " 929: 'ice lolly, lolly, lollipop, popsicle',\n",
    " 930: 'French loaf',\n",
    " 931: 'bagel, beigel',\n",
    " 932: 'pretzel',\n",
    " 933: 'cheeseburger',\n",
    " 934: 'hotdog, hot dog, red hot',\n",
    " 935: 'mashed potato',\n",
    " 936: 'head cabbage',\n",
    " 937: 'broccoli',\n",
    " 938: 'cauliflower',\n",
    " 939: 'zucchini, courgette',\n",
    " 940: 'spaghetti squash',\n",
    " 941: 'acorn squash',\n",
    " 942: 'butternut squash',\n",
    " 943: 'cucumber, cuke',\n",
    " 944: 'artichoke, globe artichoke',\n",
    " 945: 'bell pepper',\n",
    " 946: 'cardoon',\n",
    " 947: 'mushroom',\n",
    " 948: 'Granny Smith',\n",
    " 949: 'strawberry',\n",
    " 950: 'orange',\n",
    " 951: 'lemon',\n",
    " 952: 'fig',\n",
    " 953: 'pineapple, ananas',\n",
    " 954: 'banana',\n",
    " 955: 'jackfruit, jak, jack',\n",
    " 956: 'custard apple',\n",
    " 957: 'pomegranate',\n",
    " 958: 'hay',\n",
    " 959: 'carbonara',\n",
    " 960: 'chocolate sauce, chocolate syrup',\n",
    " 961: 'dough',\n",
    " 962: 'meat loaf, meatloaf',\n",
    " 963: 'pizza, pizza pie',\n",
    " 964: 'potpie',\n",
    " 965: 'burrito',\n",
    " 966: 'red wine',\n",
    " 967: 'espresso',\n",
    " 968: 'cup',\n",
    " 969: 'eggnog',\n",
    " 970: 'alp',\n",
    " 971: 'bubble',\n",
    " 972: 'cliff, drop, drop-off',\n",
    " 973: 'coral reef',\n",
    " 974: 'geyser',\n",
    " 975: 'lakeside, lakeshore',\n",
    " 976: 'promontory, headland, head, foreland',\n",
    " 977: 'sandbar, sand bar',\n",
    " 978: 'seashore, coast, seacoast, sea-coast',\n",
    " 979: 'valley, vale',\n",
    " 980: 'volcano',\n",
    " 981: 'ballplayer, baseball player',\n",
    " 982: 'groom, bridegroom',\n",
    " 983: 'scuba diver',\n",
    " 984: 'rapeseed',\n",
    " 985: 'daisy',\n",
    " 986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
    " 987: 'corn',\n",
    " 988: 'acorn',\n",
    " 989: 'hip, rose hip, rosehip',\n",
    " 990: 'buckeye, horse chestnut, conker',\n",
    " 991: 'coral fungus',\n",
    " 992: 'agaric',\n",
    " 993: 'gyromitra',\n",
    " 994: 'stinkhorn, carrion fungus',\n",
    " 995: 'earthstar',\n",
    " 996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
    " 997: 'bolete',\n",
    " 998: 'ear, spike, capitulum',\n",
    " 999: 'toilet tissue, toilet paper, bathroom tissue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Map Imagenette Labels to Imagenet Labels\n",
    "dir_to_imagenet_index = {\n",
    "    'n03888257': 1,\n",
    "    'n03425413': 571,\n",
    "    'n03394916': 566,\n",
    "    'n03000684': 491,\n",
    "    'n02102040': 217,\n",
    "    'n03445777': 574,\n",
    "    'n03417042': 569,\n",
    "    'n03028079': 497,\n",
    "    'n02979186': 482,\n",
    "    'n01440764': 701\n",
    "    }\n",
    "\n",
    "dir_index_to_imagenet_label = {}\n",
    "ordered_dirs = sorted(list(dir_to_imagenet_index.keys()))\n",
    "\n",
    "for dir_index, dir_name in enumerate(ordered_dirs):\n",
    "  dir_index_to_imagenet_label[dir_index] = dir_to_imagenet_index[dir_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Prepare Imagenette Data\n",
    "val_transform = transforms.Compose((transforms.Resize((256, 256)),\n",
    "                                    transforms.ToTensor()))\n",
    "\n",
    "imagenette_val = ImageFolder('imagenette2-320/val', transform=val_transform)\n",
    "\n",
    "train_transform = transforms.Compose((transforms.Resize((256, 256)),\n",
    "                                      transforms.ToTensor()))\n",
    "\n",
    "imagenette_train = ImageFolder('imagenette2-320/train',\n",
    "                               transform=train_transform)\n",
    "random.seed(SEED)\n",
    "random_indices = random.sample(range(len(imagenette_train)), 400)\n",
    "imagenette_train_subset = torch.utils.data.Subset(imagenette_train,\n",
    "                                                  random_indices)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Subset to only one tenth of the data for faster runtime\n",
    "random_indices = random.sample(range(len(imagenette_val)), int(len(imagenette_val) * .1))\n",
    "imagenette_val = torch.utils.data.Subset(imagenette_val, random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# To preserve reproducibility\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "imagenette_train_loader = torch.utils.data.DataLoader(imagenette_train_subset,\n",
    "                                                      batch_size=16,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=2,\n",
    "                                                      worker_init_fn=seed_worker,\n",
    "                                                      generator=g_seed\n",
    "                                                      )\n",
    "\n",
    "imagenette_val_loader = torch.utils.data.DataLoader(imagenette_val,\n",
    "                                                    batch_size=16,\n",
    "                                                    shuffle=False,\n",
    "                                                    num_workers=2,\n",
    "                                                    worker_init_fn=seed_worker,\n",
    "                                                    generator=g_seed)\n",
    "\n",
    "dataiter = iter(imagenette_val_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Show images\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(make_grid(images, nrow=4).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title eval_imagenette function\n",
    "def eval_imagenette(resnet, data_loader, dataset_length, device):\n",
    "  resnet.eval()\n",
    "  with torch.no_grad():\n",
    "    loss_sum = 0\n",
    "    total_1_correct = 0\n",
    "    total_5_correct = 0\n",
    "    total = dataset_length\n",
    "    for batch in tqdm.notebook.tqdm(data_loader):\n",
    "      images, labels = batch\n",
    "\n",
    "      # Map the imagenette labels onto the network's output\n",
    "      for i, label in enumerate(labels):\n",
    "          labels[i] = dir_index_to_imagenet_label[label.item()]\n",
    "\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      output = resnet(images)\n",
    "\n",
    "      # Calculate top-5 accuracy\n",
    "      # Implementation from https://github.com/bearpaw/pytorch-classification/blob/cc9106d598ff1fe375cc030873ceacfea0499d77/utils/eval.py\n",
    "      batch_size = labels.size(0)\n",
    "\n",
    "      _, predictions = output.topk(5, 1, True, True)\n",
    "      predictions = predictions.t()\n",
    "\n",
    "      top_k_correct = predictions.eq(labels.view(1, -1).expand_as(predictions))\n",
    "      top_k_correct = top_k_correct.sum()\n",
    "\n",
    "      predictions = torch.argmax(output, dim=1)\n",
    "      top_1_correct = torch.sum(predictions == labels)\n",
    "      total_1_correct += top_1_correct\n",
    "      total_5_correct += top_k_correct\n",
    "\n",
    "    top_1_acc = total_1_correct / total\n",
    "    top_5_acc = total_5_correct / total\n",
    "\n",
    "    return top_1_acc, top_5_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Imagenette Train Loop\n",
    "\n",
    "def imagenette_train_loop(model, optimizer, train_loader,\n",
    "                          loss_fn, device):\n",
    "  \"\"\"\n",
    "  Training loop for Imagenette\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      Untrained model\n",
    "    optimizer: function\n",
    "      Optimizer\n",
    "    train_loader: torch.loader\n",
    "      Training loader\n",
    "    loss_fn: function\n",
    "      Criterion\n",
    "    device: string\n",
    "      If available, GPU/CUDA. CPU otherwise\n",
    "\n",
    "  Returns:\n",
    "    model: nn.module\n",
    "      Trained model\n",
    "  \"\"\"\n",
    "  for epoch in tqdm.notebook.tqdm(range(5)):\n",
    "    # Set model to use the imagenette classifier head\n",
    "    model.train()\n",
    "    # Train on a batch of images\n",
    "    for imagenette_batch in train_loader:\n",
    "      images, labels = imagenette_batch\n",
    "\n",
    "      # Convert labels from imagenette indices to imagenet labels\n",
    "      for i, label in enumerate(labels):\n",
    "        labels[i] = dir_index_to_imagenet_label[label.item()]\n",
    "\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      output = model(images)\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_fn(output, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "This cell creates a ResNet model pretrained on [ImageNet](http://www.image-net.org/), a 1000 class image prediction dataset. The model is then trained to make predictions on [Imagenette](https://github.com/fastai/imagenette), a small subset of ImageNet classes that is useful for demonstrations and prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Original network\n",
    "top_1_accuracies = []\n",
    "top_5_accuracies = []\n",
    "\n",
    "# Instantiate a pretrained resnet model\n",
    "set_seed(seed=SEED)\n",
    "resnet = torchvision.models.resnet18(pretrained=True).to(DEVICE)\n",
    "resnet_opt = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "imagenette_train_loop(resnet,\n",
    "                      resnet_opt,\n",
    "                      imagenette_train_loader,\n",
    "                      loss_fn,\n",
    "                      device=DEVICE)\n",
    "\n",
    "top_1_acc, top_5_acc = eval_imagenette(resnet,\n",
    "                                       imagenette_val_loader,\n",
    "                                       len(imagenette_val),\n",
    "                                       device=DEVICE)\n",
    "top_1_accuracies.append(top_1_acc.item())\n",
    "top_5_accuracies.append(top_5_acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Coding Exercise 4.1: Use the ResNet model\n",
    "\n",
    "Complete the function below that runs a batch of images through the trained ResNet and returns the Top 5 class predictions and their probabilities. Note that the ResNet model returns unnormalized logits$^\\dagger$. To obtain probabilities, you need to normalize the logits using softmax.\n",
    "\n",
    "<br>\n",
    "\n",
    "$^\\dagger$ $ \\text{logit}(p) = \\sigma^{-1}(p) = \\text{log} \\left( \\frac{p}{1-p} \\right), \\, \\text{for} \\, p \\in (0,1)$, where $\\sigma(\\cdot)$ is the sigmoid function, i.e., $\\sigma(z) = 1/(1+e^{-z})$. For more information see [here](http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "def predict_top5(images, device, seed):\n",
    "  \"\"\"\n",
    "  Function to predict top 5 classes\n",
    "\n",
    "  Args:\n",
    "    images: torch.tensor\n",
    "      Image data with dimensionality B x C x H x W batch size x number of channels x height x width)\n",
    "    device: STRING\n",
    "      `cuda` if GPU is available, else `cpu`.\n",
    "\n",
    "  Output:\n",
    "    top5_probs: torch.tensor\n",
    "      Tensor(B, 5) with top 5 class probabilities\n",
    "    top5_names: list\n",
    "      List of top 5 class names (B, 5)\n",
    "  \"\"\"\n",
    "  ####################################################################\n",
    "  # Fill in all missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"Predict top 5\")\n",
    "  ####################################################################\n",
    "  set_seed(seed=seed)\n",
    "\n",
    "  B = images.size(0)\n",
    "  with torch.no_grad():\n",
    "    # Run images through model\n",
    "    images = ...\n",
    "    output = ...\n",
    "    # The model output is unnormalized. To get probabilities, run a softmax on it.\n",
    "    probs = ...\n",
    "    # Fetch output from GPU and convert to numpy array\n",
    "    probs = ...\n",
    "\n",
    "  # Get top 5 predictions\n",
    "  _, top5_idcs = output.topk(5, 1, True, True)\n",
    "  top5_idcs = top5_idcs.t().cpu().numpy()\n",
    "  top5_probs = probs[torch.arange(B), top5_idcs]\n",
    "\n",
    "  # Convert indices to class names\n",
    "  top5_names = []\n",
    "  for b in range(B):\n",
    "    temp = [dict_map[key].split(',')[0] for key in top5_idcs[:, b]]\n",
    "    top5_names.append(temp)\n",
    "\n",
    "  return top5_names, top5_probs\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 4.1: Use the ResNet model')\n",
    "\n",
    "# Get batch of images\n",
    "dataiter = iter(imagenette_val_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "## Uncomment to test your function and retrieve top 5 predictions\n",
    "# top5_names, top5_probs = predict_top5(images, DEVICE, SEED)\n",
    "# print(top5_names[1])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def predict_top5(images, device, seed):\n",
    "  \"\"\"\n",
    "  Function to predict top 5 classes\n",
    "\n",
    "  Args:\n",
    "    images: torch.tensor\n",
    "      Image data with dimensionality B x C x H x W batch size x number of channels x height x width)\n",
    "    device: STRING\n",
    "      `cuda` if GPU is available, else `cpu`.\n",
    "\n",
    "  Output:\n",
    "    top5_probs: torch.tensor\n",
    "      Tensor(B, 5) with top 5 class probabilities\n",
    "    top5_names: list\n",
    "      List of top 5 class names (B, 5)\n",
    "  \"\"\"\n",
    "  set_seed(seed=seed)\n",
    "\n",
    "  B = images.size(0)\n",
    "  with torch.no_grad():\n",
    "    # Run images through model\n",
    "    images = images.to(device)\n",
    "    output = resnet(images)\n",
    "    # The model output is unnormalized. To get probabilities, run a softmax on it.\n",
    "    probs = torch.nn.functional.softmax(output, dim=1)\n",
    "    # Fetch output from GPU and convert to numpy array\n",
    "    probs = probs.cpu().numpy()\n",
    "\n",
    "  # Get top 5 predictions\n",
    "  _, top5_idcs = output.topk(5, 1, True, True)\n",
    "  top5_idcs = top5_idcs.t().cpu().numpy()\n",
    "  top5_probs = probs[torch.arange(B), top5_idcs]\n",
    "\n",
    "  # Convert indices to class names\n",
    "  top5_names = []\n",
    "  for b in range(B):\n",
    "    temp = [dict_map[key].split(',')[0] for key in top5_idcs[:, b]]\n",
    "    top5_names.append(temp)\n",
    "\n",
    "  return top5_names, top5_probs\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 4.1: Use the ResNet model')\n",
    "\n",
    "# Get batch of images\n",
    "dataiter = iter(imagenette_val_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "## Uncomment to test your function and retrieve top 5 predictions\n",
    "top5_names, top5_probs = predict_top5(images, DEVICE, SEED)\n",
    "print(top5_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Random seed 2021 has been set.\n",
    "['gas pump', 'chain saw', 'jinrikisha', 'French horn', 'laptop']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Visualize probabilities of top 5 predictions\n",
    "fig, ax = plt.subplots(5, 2, figsize=(10, 20))\n",
    "\n",
    "for i in range(5):\n",
    "  ax[i, 0].imshow(np.moveaxis(images[i].numpy(), 0, -1))\n",
    "  ax[i, 0].axis('off')\n",
    "\n",
    "  ax[i, 1].bar(np.arange(5), top5_probs[:, i])\n",
    "  ax[i, 1].set_xticks(np.arange(5))\n",
    "  ax[i, 1].set_xticklabels(top5_names[i], rotation=30)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Out-of-distribution examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The code below runs two out-of-distribution examples through the trained ResNet. Look at the predictions and discuss, why the model might fail to make accurate predictions on these images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "loc = 'https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernConvnets/static/'\n",
    "\n",
    "fname1 = 'bonsai-svg-5.png'\n",
    "response = requests.get(loc + fname1)\n",
    "image = Image.open(BytesIO(response.content)).resize((256, 256))\n",
    "data = torch.from_numpy(np.asarray(image)[:, :, :3]) / 255.\n",
    "\n",
    "fname2 = 'PokÃ©mon_Pikachu_art.png'\n",
    "response = requests.get(loc + fname2)\n",
    "image = Image.open(BytesIO(response.content)).resize((256, 256))\n",
    "data2 = torch.from_numpy(np.asarray(image)[:, :, :3]) / 255.\n",
    "\n",
    "images = torch.stack([data, data2]).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Retrieve top 5 predictions\n",
    "top5_names, top5_probs  = predict_top5(images, DEVICE, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Visualize probabilities of top 5 predictions\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "for i in range(2):\n",
    "  ax[i, 0].imshow(np.moveaxis(images[i].numpy(), 0, -1))\n",
    "  ax[i, 0].axis('off')\n",
    "\n",
    "  ax[i, 1].bar(np.arange(5), top5_probs[:, i])\n",
    "  ax[i, 1].set_xticks(np.arange(5))\n",
    "  ax[i, 1].set_xticklabels(top5_names[i], rotation=30)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 5: Inception + ResNeXt\n",
    "\n",
    "*Time estimate: ~27mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 5: Improving efficiency: Inceptrion and ResNeXt\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1Zq4y1W7Px\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"TDHn7X1wNQ4\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 5: Improving efficiency: Inceptrion and ResNeXt')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## ResNet vs ResNeXt\n",
    "\n",
    "<img height=300 src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernConvnets/static/ResNets.png\">\n",
    "\n",
    "[Xie et al., 2016](https://arxiv.org/abs/1611.05431)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Interactive Demo 5: ResNet vs. ResNeXt\n",
    "\n",
    "The widgets below calculate the number of parameters in a ResNet (top) and the parameters in a ResNeXt (bottom). We assume that the number of input and output channels (or feature maps) is the same (labeled \"Channels in+out\" in the widget). We refer to the number of channels after the first and the second layer of one block of either ResNet or ResNeXt as \"bottleneck channels\".\n",
    "\n",
    "The sliders are currently in the position that is displayed in the figure above. The goal of the following tasks is to investigate the difference in expressiveness and numbers of parameters in ResNet and ResNeXt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Parameter Calculator\n",
    "# @markdown Run this cell to enable the widget\n",
    "from IPython.display import display as dis\n",
    "\n",
    "def calculate_parameters_resnet(d_in, resnet_channels):\n",
    "    \"\"\"\n",
    "    ResNet math: Implement how parameters scale\n",
    "\n",
    "    Args:\n",
    "      d_in: int\n",
    "        Input dimensionality\n",
    "      resnet_channels: int\n",
    "        Number of channels in ResNet\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    d_out = d_in\n",
    "    resnet_parameters = d_in*resnet_channels + 3*3*resnet_channels*resnet_channels + resnet_channels*d_out\n",
    "\n",
    "    print('ResNet parameters: {}'.format(resnet_parameters))\n",
    "    return None\n",
    "\n",
    "\n",
    "def calculate_parameters_resnext(d_in, resnext_channels,\n",
    "                                 num_paths):\n",
    "    \"\"\"\n",
    "    ResNext math: Implement how parameters scale\n",
    "\n",
    "    Args:\n",
    "      d_in: int\n",
    "        Input dimensionality\n",
    "      resnet_channels: int\n",
    "        Number of channels in ResNext\n",
    "      num_paths: int\n",
    "        Number of pathways in ResNext\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    d_out = d_in\n",
    "    d = resnext_channels\n",
    "\n",
    "    resnext_parameters = (d_in*d + 3*3*d*d + d*d_out)*num_paths\n",
    "\n",
    "    print('ResNeXt parameters: {}'.format(resnext_parameters))\n",
    "    return None\n",
    "\n",
    "\n",
    "labels = ['ResNet', 'ResNeXt']\n",
    "descriptions_resnet = ['Channels in+out', 'Bottleneck channels']\n",
    "descriptions_resnext = ['Channels in+out', 'Bottleneck channels',\n",
    "                        'Number of paths (cardinality)']\n",
    "lbox_resnet = widgets.VBox([widgets.Label(description) for description in descriptions_resnet])\n",
    "lbox_resnext = widgets.VBox([widgets.Label(description) for description in descriptions_resnext])\n",
    "\n",
    "d_in = widgets.FloatLogSlider(\n",
    "    value=256,\n",
    "    base=2,\n",
    "    min=1, # Max exponent of base\n",
    "    max=10, # Min exponent of base\n",
    "    step=1, # Exponent step\n",
    ")\n",
    "resnet_channels = widgets.FloatLogSlider(\n",
    "    value=64,\n",
    "    base=2,\n",
    "    min=5, # Max exponent of base\n",
    "    max=10, # Min exponent of base\n",
    "    step=1, # Exponent step\n",
    ")\n",
    "resnext_channels = widgets.FloatLogSlider(\n",
    "    value=4,\n",
    "    base=2,\n",
    "    min=1, # Max exponent of base\n",
    "    max=10, # Min exponent of base\n",
    "    step=1, # Exponent step\n",
    ")\n",
    "num_paths = widgets.FloatLogSlider(\n",
    "    value=32,\n",
    "    base=2,\n",
    "    min=0, # Max exponent of base\n",
    "    max=7, # Min exponent of base\n",
    "    step=1, # Exponent step\n",
    ")\n",
    "\n",
    "rbox_resnet = widgets.VBox([d_in, resnet_channels])\n",
    "rbox_resnext = widgets.VBox([d_in, resnext_channels, num_paths])\n",
    "ui_resnet = widgets.HBox([lbox_resnet, rbox_resnet])\n",
    "ui_resnet_labeled = widgets.VBox(\n",
    "    [widgets.HTML(value=\"<b>\" + labels[0] + \"</b>\"), ui_resnet],\n",
    "    layout=widgets.Layout(border='1px solid black'))\n",
    "ui_resnext = widgets.HBox([lbox_resnext, rbox_resnext])\n",
    "ui_resnext_labeled = widgets.VBox(\n",
    "    [widgets.HTML(value=\"<b>\" + labels[1] + \"</b>\"), ui_resnext],\n",
    "    layout=widgets.Layout(border='1px solid black'))\n",
    "ui = widgets.VBox([ui_resnet_labeled, ui_resnext_labeled])\n",
    "\n",
    "out_resnet = widgets.interactive_output(calculate_parameters_resnet,\n",
    "                     {'d_in':d_in,\n",
    "                     'resnet_channels':resnet_channels})\n",
    "\n",
    "out_resnext = widgets.interactive_output(calculate_parameters_resnext,\n",
    "                     {'d_in':d_in,\n",
    "                     'resnext_channels':resnext_channels,\n",
    "                     'num_paths':num_paths})\n",
    "\n",
    "d1 = dis(ui, out_resnet, out_resnext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Think! 5: ResNet vs. ResNeXt\n",
    "\n",
    "In the figure above, both networks â ResNet and ResNeXt â have a similar number of parameters. \n",
    "\n",
    "1. How many channels are there in the bottleneck of the two networks, respectively?\n",
    "1. How are these channels connected to each other from the first to the second layer in the blocks of the two networks, respectively? \n",
    "1. What does it mean for the expressiveness of the two models relative to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q4', text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "1. The ResNeXt has 32 * 4 = 128 channels in the bottleneck, whereas the ResNet has only 64.\n",
    "\n",
    "2. In ResNet all 64 output channels of the first layer are connected to all 64 output channels of the second layer.\n",
    "In ResNeXt, channels are connected only within paths, i.e. in groups of 4.\n",
    "\n",
    "3. ResNeXt contains more channels in the bottleneck (potentially more expressive),\n",
    "   but each of them \"sees\" only a subset of the previous layer's output (potentially less expressive).\n",
    "   The former tends to outweigh the latter, which is why the ResNeXt architecture tends to outperform the\n",
    "   vanilla ResNet architecture.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now we want to look at the number of parameters.\n",
    "* How does the difference in number of parameters change if we fix the number of channels in the bottleneck of both ResNet and ResNeXt to be 64, but vary the number of paths in ResNeXt? (8 paths with 8 channels each would be one such example)\n",
    "* Which number of paths results in the biggest parameter savings?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "  -- if number of paths=1 and channels per path=64 -> same architecture as ResNet, no parameter saving\n",
    "  -- if number of paths=8 and channels per path=8 -> around half the number of parameters\n",
    "  -- if number of paths=32 and channels per path=2 -> biggest parameter saving the more paths, the more saving\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "---\n",
    "# Section 6: Depthwise separable convolutions\n",
    "\n",
    "*Time estimate: ~23mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# @title Video 6: Improving efficiency: MobileNet\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1D44y127fS\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"kdbGpn1JfmU\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# add event to airtable\n",
    "atform.add_event('Video 6: Improving efficiency: MobileNet')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 6.1: Depthwise separable convolutions\n",
    "\n",
    "Another way to reduce the computational cost of large models is the use of depthwise separable convolutions ([introduced here](https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf)). Depthwise separable convolutions are the key component making [MobileNets](https://arxiv.org/abs/1704.04861) efficient.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernConvnets/static/SchematicCNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Coding Exercise 6.1: Calculation of parameters\n",
    "\n",
    "Fill in the calculation of the parameters of regular convolution and depthwise separable convolution in the function below.\n",
    "Above you can see the example given in the video for you to check if your calculation is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "def convolution_math(in_channels, filter_size, out_channels):\n",
    "  \"\"\"\n",
    "  Convolution math: Implement how parameters scale as a function of feature maps\n",
    "  and filter size in convolution vs depthwise separable convolution.\n",
    "\n",
    "  Args:\n",
    "    in_channels : int\n",
    "      Number of input channels\n",
    "    filter_size : int\n",
    "      Size of the filter\n",
    "    out_channels : int\n",
    "      Number of output channels\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  ####################################################################\n",
    "  # Fill in all missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"Convolution math\")\n",
    "  ####################################################################\n",
    "  # Calculate the number of parameters for regular convolution\n",
    "  conv_parameters = ...\n",
    "  # Calculate the number of parameters for depthwise separable convolution\n",
    "  depthwise_conv_parameters = ...\n",
    "\n",
    "  print(f\"Depthwise separable: {depthwise_conv_parameters} parameters\")\n",
    "  print(f\"Regular convolution: {conv_parameters} parameters\")\n",
    "\n",
    "  return None\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 6.1: Calculation of parameters')\n",
    "\n",
    "## Uncomment to test your function\n",
    "# convolution_math(in_channels=4, filter_size=3, out_channels=2)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def convolution_math(in_channels, filter_size, out_channels):\n",
    "  \"\"\"\n",
    "  Convolution math: Implement how parameters scale as a function of feature maps\n",
    "  and filter size in convolution vs depthwise separable convolution.\n",
    "\n",
    "  Args:\n",
    "    in_channels : int\n",
    "      Number of input channels\n",
    "    filter_size : int\n",
    "      Size of the filter\n",
    "    out_channels : int\n",
    "      Number of output channels\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  # Calculate the number of parameters for regular convolution\n",
    "  conv_parameters = in_channels * filter_size * filter_size * out_channels\n",
    "  # Calculate the number of parameters for depthwise separable convolution\n",
    "  depthwise_conv_parameters = in_channels * filter_size * filter_size + in_channels * out_channels\n",
    "\n",
    "  print(f\"Depthwise separable: {depthwise_conv_parameters} parameters\")\n",
    "  print(f\"Regular convolution: {conv_parameters} parameters\")\n",
    "\n",
    "  return None\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 6.1: Calculation of parameters')\n",
    "\n",
    "## Uncomment to test your function\n",
    "convolution_math(in_channels=4, filter_size=3, out_channels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "```\n",
    "Depthwise separable: 44 parameters\n",
    "Regular convolution: 72 parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Think! 6.1: How do parameter savings depend the on number of input feature maps, 4 vs. 64?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Student Response\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "text=widgets.Textarea(\n",
    "   value='Type your answer here and click on `Submit!`',\n",
    "   placeholder='Type something',\n",
    "   description='',\n",
    "   disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "\n",
    "display(text,button)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "   atform.add_answer('q5', text.value)\n",
    "   print(\"Submission successful!\")\n",
    "\n",
    "\n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "The more input features the more parameter saving\n",
    "  4: 28 less params\n",
    "  64: 448 less params\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Section 7: Transfer Learning\n",
    "\n",
    "*Time estimate: ~24mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 7: Transfer Learning\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1z54y1E714\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"Qr5l-an5ac4\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 7: Transfer Learning')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "The most common way large image models are trained in practice is via transfer learning. One first pretrains a network on a large classification dataset like ImageNet, then uses the weights of this network as initialization for training (\"fine-tuning\") that network on your task of choice. \n",
    "\n",
    "While training a network twice sounds like a strange thing to do, the model ends up training faster on the target dataset and often outperforms training \"from scratch\". There are also other benefits such as [robustness to noise](https://arxiv.org/pdf/1901.09960.pdf) that are the subject of [active research](https://arxiv.org/abs/2008.11687).\n",
    "\n",
    "In this section we will demonstrate transfer learning by taking a model trained on ImageNet and teaching it to classify Pokemon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 7.1: Download and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Download Data\n",
    "import zipfile, io\n",
    "\n",
    "# Original link: https://github.com/ben-heil/cis_522_data.git\n",
    "url = 'https://osf.io/u4njm/download'\n",
    "\n",
    "fname = 'small_pokemon_dataset'\n",
    "\n",
    "if not os.path.exists(fname+'zip'):\n",
    "  print(\"Data is being downloaded...\")\n",
    "  r = requests.get(url, stream=True)\n",
    "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "  z.extractall()\n",
    "  print(\"The download has been completed.\")\n",
    "else:\n",
    "  print(\"Data has already been downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# List the different Pokemon\n",
    "os.listdir(\"small_pokemon_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Determine number of classes\n",
    "num_classes = 0\n",
    "for folders in os.listdir('small_pokemon_dataset/'):\n",
    "  num_classes += 1\n",
    "print(f\"{num_classes} types of Pokemon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Display Example Images\n",
    "train_transform = transforms.Compose((transforms.Resize((256, 256)),\n",
    "                                      transforms.ToTensor()))\n",
    "\n",
    "pokemon_dataset = ImageFolder('small_pokemon_dataset',\n",
    "                              transform=train_transform)\n",
    "\n",
    "image_count = len(pokemon_dataset)\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "for i in range(image_count):\n",
    "  # Put ten percent of the images in the test set\n",
    "  if random.random() < .1:\n",
    "    test_indices.append(i)\n",
    "  else:\n",
    "    train_indices.append(i)\n",
    "\n",
    "pokemon_test_set = torch.utils.data.Subset(pokemon_dataset, test_indices)\n",
    "pokemon_train_set = torch.utils.data.Subset(pokemon_dataset, train_indices)\n",
    "\n",
    "pokemon_train_loader = torch.utils.data.DataLoader(pokemon_train_set,\n",
    "                                                   batch_size=16,\n",
    "                                                   shuffle=True,)\n",
    "pokemon_test_loader = torch.utils.data.DataLoader(pokemon_test_set,\n",
    "                                                  batch_size=16)\n",
    "\n",
    "dataiter = iter(pokemon_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Show images\n",
    "plt.imshow(make_grid(images, nrow=4).permute(1, 2, 0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 7.2: Fine-tuning a ResNet\n",
    "\n",
    "It is common in computer vision to take a large model trained on a large dataset (often ImageNet), replace the classification layer and fine-tune the entire network to perform a different task. \n",
    "\n",
    "Here we'll be using a pre-trained ResNet model to classify types of Pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "# Reset final fully connected layer, number of classes = types of Pokemon = 9\n",
    "resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Finetune ResNet\n",
    "\n",
    "pretrained_accs = []\n",
    "for epoch in tqdm.tqdm(range(10)):\n",
    "  # Train loop\n",
    "  for batch in pokemon_train_loader:\n",
    "    images, labels = batch\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = resnet(images)\n",
    "    loss = loss_fn(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Eval loop\n",
    "  with torch.no_grad():\n",
    "    loss_sum = 0\n",
    "    total_correct = 0\n",
    "    total = len(pokemon_test_set)\n",
    "    for batch in pokemon_test_loader:\n",
    "      images, labels = batch\n",
    "      images = images.to(DEVICE)\n",
    "      labels = labels.to(DEVICE)\n",
    "      output = resnet(images)\n",
    "      loss = loss_fn(output, labels)\n",
    "      loss_sum += loss.item()\n",
    "\n",
    "      predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "      num_correct = torch.sum(predictions == labels)\n",
    "      total_correct += num_correct\n",
    "\n",
    "    # Plot accuracy\n",
    "    pretrained_accs.append(total_correct.cpu() / total)\n",
    "    plt.plot(pretrained_accs)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Pokemon prediction accuracy')\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    IPython.display.display(plt.gcf())\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 7.3: Train only classification layer\n",
    "\n",
    "Another possible way to make use of transfer learning is to take a pre-trained model and replace the last layer, the classification layer (sometimes also called the \"linear readout\"). Instead of fine-tuning the whole model as before, we train only the classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=True)\n",
    "for param in resnet.parameters():\n",
    "  param.requires_grad = False\n",
    "num_ftrs = resnet.fc.in_features\n",
    "# ResNet final fully connected layer\n",
    "resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(resnet.fc.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Finetune readout of ResNet\n",
    "linreadout_accs = []\n",
    "for epoch in range(10):\n",
    "  # Train loop\n",
    "  for batch in pokemon_train_loader:\n",
    "    images, labels = batch\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = resnet(images)\n",
    "    loss = loss_fn(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Eval loop\n",
    "  with torch.no_grad():\n",
    "    loss_sum = 0\n",
    "    total_correct = 0\n",
    "    total = len(pokemon_test_set)\n",
    "    for batch in pokemon_test_loader:\n",
    "      images, labels = batch\n",
    "      images = images.to(DEVICE)\n",
    "      labels = labels.to(DEVICE)\n",
    "      output = resnet(images)\n",
    "      loss = loss_fn(output, labels)\n",
    "      loss_sum += loss.item()\n",
    "\n",
    "      predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "      num_correct = torch.sum(predictions == labels)\n",
    "      total_correct += num_correct\n",
    "\n",
    "    # Plot accuracy\n",
    "    linreadout_accs.append(total_correct.cpu() / total)\n",
    "    plt.plot(linreadout_accs)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Pokemon prediction accuracy')\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    IPython.display.display(plt.gcf())\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 7.4: Training ResNet from scratch\n",
    "\n",
    "As a baseline and for comparison reasons we will also train the ResNet \"from scratch\" â that is: initialize the weights randomly and train the entire network exclusively on the Pokemon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=False)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "# ResNet final fully connected layer\n",
    "resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
    "resnet.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Train ResNet from scratch\n",
    "scratch_accs = []\n",
    "for epoch in tqdm.tqdm(range(10)):\n",
    "  # Train loop\n",
    "  for batch in pokemon_train_loader:\n",
    "    images, labels = batch\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = resnet(images)\n",
    "    loss = loss_fn(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Eval loop\n",
    "  with torch.no_grad():\n",
    "    loss_sum = 0\n",
    "    total_correct = 0\n",
    "    total = len(pokemon_test_set)\n",
    "    for batch in pokemon_test_loader:\n",
    "      images, labels = batch\n",
    "      images = images.to(DEVICE)\n",
    "      labels = labels.to(DEVICE)\n",
    "      output = resnet(images)\n",
    "      loss = loss_fn(output, labels)\n",
    "      loss_sum += loss.item()\n",
    "\n",
    "      predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "      num_correct = torch.sum(predictions == labels)\n",
    "      total_correct += num_correct\n",
    "\n",
    "    scratch_accs.append(total_correct.cpu() / total)\n",
    "    plt.plot(scratch_accs)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Pokemon prediction accuracy')\n",
    "\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    IPython.display.display(plt.gcf())\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Section 7.5: Head to Head Comparison\n",
    "Starting from a randomly initialized network works less well, especially in the case of small datsets. Note that the model converges more slowly and less evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot Accuracies\n",
    "plt.plot(pretrained_accs, label='Pretrained: fine-tuning')\n",
    "plt.plot(linreadout_accs, label='Pretrained: linear Readout')\n",
    "plt.plot(scratch_accs, label='Trained from Scratch')\n",
    "plt.title('Pokemon prediction accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Exercise 7.5.1\n",
    "\n",
    "First, we compare the Pretrained ResNet with the ResNet trained from scratch. Why might pretrained models outperform models trained from scratch? In what cases would you expect them to be worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "1. The closer your pretraining and target data domains are, the better pretraining will work\n",
    "2. The more pretraining data you have, the better pretraining will work\n",
    "3. The better your model is able to take advantage of your pretraining data (that is to say\n",
    "   the larger your model is as you have enough data), the better pretraining will work\n",
    "\n",
    "Pretraining isn't necessarily always a benefit though. If your source domain is very different from\n",
    "the domain you're trying to predict, your models might learn unhelpful features.\n",
    "\n",
    "Additionally, if you have a lot of training data in your target domain, pretraining data might\n",
    "cause your model to converge to a local minimum (this process is referred to as ossification in\n",
    "the Scaling Laws for Transfer paper cited in the Further Reading section)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Exercise 7.5.2\n",
    "\n",
    "Second, take a look at the different transfer learning methods - fine-tuning the whole network and training only the classification layer. Why might fine-tuning the whole network outperform training only the classification layer? What are the benefits of training only the classification layer? In what cases would you expect a similar performance of both methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "1. More weights are adjusted to the pretraining domain.\n",
    "2. Since only one layer is trained, the training procedure is faster/less computationally expensive.\n",
    "3. If your pretraining and target data domains are close.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Further Reading\n",
    "Supervised pretraining as you've seen here is useful, but there are several other ways of using outside data to improve your models. The ones that are particularly popular right now are self-supervised techniques like [contrastive learning](https://arxiv.org/pdf/2002.05709.pdf).\n",
    "\n",
    "There is also a [recent paper](https://arxiv.org/abs/2102.01293) that seeks to quantify the relationship between model size, pretraining dataset size, training dataset size, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this tutorial, you have learned about the modern Convnets (CNNs), their architecture, and operating principles. Also, you are now familiar with the notion of *Transfer Learning*, and you have learned when to apply it. If you have time left, you will learn more about the speed vs. accuracy trade-off. In the next tutorial, we will see the modern convnets in a facial recognition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 8: Summary and Outlook\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1So4y1D7Ev\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"sjj0-7i6XfE\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 8: Summary and Outlook')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Airtable Submission Link\n",
    "from IPython import display as IPydisplay\n",
    "IPydisplay.HTML(\n",
    "   f\"\"\"\n",
    " <div>\n",
    "   <a href= \"{atform.url()}\" target=\"_blank\">\n",
    "   <img src=\"https://github.com/NeuromatchAcademy/course-content-dl/blob/main/tutorials/static/SurveyButton.png?raw=1\"\n",
    " alt=\"button link end of day Survey\" style=\"width:410px\"></a>\n",
    "   </div>\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Bonus: Speed-Accuracy Trade-Off / Different Backbones\n",
    "\n",
    "*Time estimate: ~ 21mins*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Video 9: Speed-accuracy trade-off\n",
    "from ipywidgets import widgets\n",
    "\n",
    "out2 = widgets.Output()\n",
    "with out2:\n",
    "  from IPython.display import IFrame\n",
    "  class BiliVideo(IFrame):\n",
    "    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n",
    "      self.id=id\n",
    "      src = \"https://player.bilibili.com/player.html?bvid={0}&page={1}\".format(id, page)\n",
    "      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n",
    "\n",
    "  video = BiliVideo(id=f\"BV1v64y1z7PT\", width=854, height=480, fs=1)\n",
    "  print(\"Video available at https://www.bilibili.com/video/{0}\".format(video.id))\n",
    "  display(video)\n",
    "\n",
    "out1 = widgets.Output()\n",
    "with out1:\n",
    "  from IPython.display import YouTubeVideo\n",
    "  video = YouTubeVideo(id=f\"9p4gD-QnbIQ\", width=854, height=480, fs=1, rel=0)\n",
    "  print(\"Video available at https://youtube.com/watch?v=\" + video.id)\n",
    "  display(video)\n",
    "\n",
    "out = widgets.Tab([out1, out2])\n",
    "out.set_title(0, 'Youtube')\n",
    "out.set_title(1, 'Bilibili')\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Video 8: Speed-accuracy trade-off')\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "As the models got larger and the number of connections increased so did the computational costs involved. In the modern era of image processing, there is a tradeoff between model performance and computational cost. Models can reach extremely high performance on many problems, but achieving state of the art results requires [huge amounts of compute power](https://arxiv.org/pdf/1810.00736.pdf).\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D3_ModernConvnets/static/compute_vs_performance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Coding Exercise: Compare accuracy and training speed of different models\n",
    "\n",
    "The goal is to load three pretrained models and fine-tune them.\n",
    "`models` is a dictionary where the keys are the names of the models and the values are the corresponding model objects.\n",
    "Currently the names are *ResNet18, AlexNet* and *VGG-19*.\n",
    "For a start, load these models from torchvision.models and make sure they are pretrained.\n",
    "\n",
    "If you want to try other models, just change the dictionary, or if you want to even try out more than three models, just add them to the dictionary and add their learning rates in the array below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Imagenette Train Loop: `train_loop(model, optimizer, train_loader, loss_fn, device)`\n",
    "def train_loop(model, optimizer, train_loader,\n",
    "               loss_fn, device):\n",
    "  \"\"\"\n",
    "  Imagenette Train Loop\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      Model\n",
    "    optimizer: function\n",
    "      Optimizer\n",
    "    train_loader: torch.loader\n",
    "      Training dataset\n",
    "    loss_fn: function\n",
    "      Criterion\n",
    "    device: string\n",
    "      GPU/CUDA if available. CPU otherwise.\n",
    "\n",
    "  Returns:\n",
    "    Average Training time\n",
    "  \"\"\"\n",
    "  times = []\n",
    "  model.to(device)\n",
    "  for epoch in tqdm.notebook.tqdm(range(5)):\n",
    "    model.train()\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Train on a batch of images\n",
    "    for imagenette_batch in train_loader:\n",
    "      images, labels = imagenette_batch\n",
    "\n",
    "      # Convert labels from imagenette indices to imagenet labels\n",
    "      for i, label in enumerate(labels):\n",
    "        labels[i] = dir_index_to_imagenet_label[label.item()]\n",
    "\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      output = model(images)\n",
    "      optimizer.zero_grad()\n",
    "      loss = loss_fn(output, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "      times += [time.time() - t_start]\n",
    "\n",
    "  return np.mean(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Run the models: `run_models(models, lr_rates)`\n",
    "def run_models(models, lr_rates):\n",
    "  \"\"\"\n",
    "  Run the models\n",
    "\n",
    "  Args:\n",
    "    models: dict\n",
    "      Models\n",
    "    lr_rates: list\n",
    "      Learning rates\n",
    "\n",
    "  Returns:\n",
    "    times: list\n",
    "      Running time for models\n",
    "    top_1_acciracies: list\n",
    "      Top 1 accuracy per model\n",
    "  \"\"\"\n",
    "  times, top_1_accuracies = [], []\n",
    "\n",
    "  for (name, model), lr in zip(models.items(), lr_rates):\n",
    "\n",
    "    print(name, lr)\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.aux_logits = False  # Important only for googlenet\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model_time = train_loop(model, optimizer, imagenette_train_loader, loss_fn,\n",
    "                            DEVICE)\n",
    "    times.append(model_time)\n",
    "\n",
    "    top_1_acc, _ = eval_imagenette(model, imagenette_val_loader,\n",
    "                                  len(imagenette_val), device=DEVICE)\n",
    "    top_1_accuracies.append(top_1_acc.item())\n",
    "\n",
    "  return times, top_1_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plot accuracies vs. training speed\n",
    "def get_parameter_count(model):\n",
    "  \"\"\"\n",
    "  Get parameter count per model\n",
    "\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      Model\n",
    "\n",
    "  Returns:\n",
    "    Parameter count for model\n",
    "  \"\"\"\n",
    "  return sum([torch.numel(p) for p in model.parameters()])\n",
    "\n",
    "\n",
    "def plot_acc_speed(times, accs, models):\n",
    "  \"\"\"\n",
    "  Plots Accuracy vs Speed\n",
    "\n",
    "  Args:\n",
    "    times: list\n",
    "      Log of running times\n",
    "    accs: list\n",
    "      Log of accuracies\n",
    "    models: dict\n",
    "      Log of models\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  ti = [t*1000 for t in times]\n",
    "  for i, model in enumerate(list(models.keys())):\n",
    "    scale = get_parameter_count(models[model])*1e-6\n",
    "    plt.scatter(ti[i], accs[i], s=scale, label=model)\n",
    "  plt.grid(True)\n",
    "  plt.xlabel('Speed [ms]')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Accuracy vs. Speed')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {}
   },
   "source": [
    "```python\n",
    "def create_models(pretrained=False):\n",
    "  \"\"\"\n",
    "  Creates models\n",
    "\n",
    "  Args:\n",
    "    pretrained: boolean\n",
    "      If True, load pretrained models\n",
    "\n",
    "  Returns:\n",
    "    models: dict\n",
    "      Log of models\n",
    "    lr_rates: list\n",
    "      Log of learning rates\n",
    "  \"\"\"\n",
    "  ####################################################################\n",
    "  # Fill in all missing code below (...),\n",
    "  # then remove or comment the line below to test your function\n",
    "  raise NotImplementedError(\"create pretrained models\")\n",
    "  ####################################################################\n",
    "  # Load three pretrained models from torchvision.models\n",
    "  # [these are just examples, other models are possible as well]\n",
    "  model1 = ...\n",
    "  model2 = ...\n",
    "  model3 = ...\n",
    "\n",
    "  models = {'...': model1, '...': model2, '...-19': model3}\n",
    "  lr_rates = [1e-4, 1e-4, 1e-4]\n",
    "\n",
    "  return models, lr_rates\n",
    "\n",
    "\n",
    "## Uncomment below to test your function\n",
    "# models, lr_rates = create_models(pretrained=True)\n",
    "# times, top_1_accuracies = run_models(models, lr_rates)\n",
    "# plot_acc_speed(times, top_1_accuracies, models)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "def create_models(pretrained=False):\n",
    "  \"\"\"\n",
    "  Creates models\n",
    "\n",
    "  Args:\n",
    "    pretrained: boolean\n",
    "      If True, load pretrained models\n",
    "\n",
    "  Returns:\n",
    "    models: dict\n",
    "      Log of models\n",
    "    lr_rates: list\n",
    "      Log of learning rates\n",
    "  \"\"\"\n",
    "  # Load three pretrained models from torchvision.models\n",
    "  # [these are just examples, other models are possible as well]\n",
    "  model1 = torchvision.models.resnet18(pretrained=pretrained)\n",
    "  model2 = torchvision.models.alexnet(pretrained=pretrained)\n",
    "  model3 = torchvision.models.vgg19(pretrained=pretrained)\n",
    "\n",
    "  models = {'ResNet18': model1, 'AlexNet': model2, 'VGG-19': model3}\n",
    "  lr_rates = [1e-4, 1e-4, 1e-4]\n",
    "\n",
    "  return models, lr_rates\n",
    "\n",
    "\n",
    "## Uncomment below to test your function\n",
    "models, lr_rates = create_models(pretrained=True)\n",
    "times, top_1_accuracies = run_models(models, lr_rates)\n",
    "plot_acc_speed(times, top_1_accuracies, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Bonus Exercise 1\n",
    "\n",
    "Look at the plot above.\n",
    "It shows the training speed vs. the accuracy of the models you chose.\n",
    "The training speed is measured as the mean time the training takes per epoch.\n",
    "The size of the marker visualizes the number of parameters of the model.\n",
    "\n",
    "Which model seems to be the best for this task and why?\n",
    "Explain your conclusion based on speed, accuracy and number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Given the 3 suggested models, the ResNet is the best model because it has the highest accuracy by being\n",
    "almost as fast as AlexNet and having less parameters.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Bonus Exercise 2\n",
    "\n",
    "How does the speed correlate with the accuracy? Are faster models also more accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# to_remove explanation\n",
    "\n",
    "\"\"\"\n",
    "Also depends on the models they chose.\n",
    "For example, if we compare VGG with ResNet the ResNet is both faster and more accurate.\n",
    "AlexNet is a bit faster than ResNet but not as accurate.\n",
    "\"\"\";"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "W2D3_Tutorial1",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
