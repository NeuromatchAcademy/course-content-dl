{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kpc_IPHzR1V"
      },
      "source": [
        "# Transfer Learning \n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ [Jama Hussein Mohamud](https://engmubarak48.github.io/jmohamud/index.html) & [Alex Hernandez-Garcia](https://alexhernandezgarcia.github.io/)\n",
        "\n",
        "__Production editors:__\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvHCbTQtzrH_"
      },
      "source": [
        "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
        "\n",
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSN-hSJnzrKt"
      },
      "source": [
        "Background: One desired capability for machines is the ability to transfer the knowledge (features) learned on one domain to another This can potentially save compute time, enable training when data is scarce, and even improve performance. Unfortunately, there is no single recipe for transfer learning and instead multiple options are possible and much remains to be well understood. In this project, you will explore how transfer learning works in different scenarios. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa_E2XkS8sWx"
      },
      "source": [
        "###Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uPyIdUE8DsT"
      },
      "source": [
        "# check if your GPU, if you don't get the window, please go to Runtime > change runtime > and change to GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBpHN9RGsKQD"
      },
      "source": [
        "#### Imports\n",
        "\n",
        "Import the necessary Python libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru0j_oZr-Eqe"
      },
      "source": [
        "##@title Necessary imports\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import csv\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import multiprocessing\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "import torchvision.models as models\n",
        "from torch.autograd import Variable\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiAF2yrurGrP"
      },
      "source": [
        "### Alternative / Complementary GPU check-up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y9C9CaOrHNO"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "print(\"Torch version: \", torch.__version__)\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LPH_DG9sF3S"
      },
      "source": [
        "### Random seeds\n",
        "\n",
        "If you want to obtain reproducible results, it is a good practice to set seeds for the random number generators of the various libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYeKRCa8tnEe"
      },
      "source": [
        "seed = 3971\n",
        "np.random.seed(seed) # Set the random seed of numpy for the data split.\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKGP9RSStzs7"
      },
      "source": [
        "### Training hyperparameters\n",
        "\n",
        "Here we set some general training hyperparameters such as the learning rate, batch size, etc. as well as other training options such as including data augmentation (`torchvision_transforms`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y008jb7WnKwA"
      },
      "source": [
        "## @title hyper-parameters\n",
        "use_cuda = torch.cuda.is_available()\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "batch_size = 128\n",
        "max_epochs = 200\n",
        "max_epochs_target = 10\n",
        "base_learning_rate = 0.1\n",
        "torchvision_transforms = True # True/False if you want use torchvision augmentations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrxmPY3gsal0"
      },
      "source": [
        "#### Source dataset\n",
        "\n",
        "We will train the source model using CIFAR-100 data set from PyTorch, but with small tweaks we can get any other data we are interested in.\n",
        "\n",
        "Note that the data set is normalised by substracted the mean and dividing by the standard deviation (pre-computed) of the training set. Also, if `torchvision_transforms` is `True`, data augmentation will be applied during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DqmzceI-hPM"
      },
      "source": [
        "##@title Data\n",
        "print('==> Preparing data..')\n",
        "def percentageSplit(full_dataset, percent = 0.0):\n",
        "    set1_size = int(percent * len(full_dataset))\n",
        "    set2_size = len(full_dataset) - set1_size\n",
        "    final_dataset, _ = torch.utils.data.random_split(full_dataset, [set1_size, set2_size])\n",
        "    return final_dataset\n",
        "\n",
        "# CIFAR100 normalizing\n",
        "mean = [0.5071, 0.4866, 0.4409]\n",
        "std = [0.2673, 0.2564, 0.2762]\n",
        "\n",
        "# CIFAR10 normalizing\n",
        "# mean = (0.4914, 0.4822, 0.4465)\n",
        "# std = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "# torchvision transforms\n",
        "transform_train = transforms.Compose([])\n",
        "if torchvision_transforms:\n",
        "    transform_train.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "    transform_train.transforms.append(transforms.RandomHorizontalFlip())\n",
        "\n",
        "transform_train.transforms.append(transforms.ToTensor())\n",
        "transform_train.transforms.append(transforms.Normalize(mean, std))\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(\n",
        "    root='./CIFAR100', train=True, download=True, transform=transform_train)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(\n",
        "    root='./CIFAR100', train=False, download=True, transform=transform_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0UfAEfEuxfY"
      },
      "source": [
        "#### CIFAR-100\n",
        "\n",
        "CIFAR-100 is a data set of 50,000 colour (RGB) training images and 10,000 test images, of size 32 x 32 pixels. Each image is labelled as 1 of 100 possible classes. \n",
        "\n",
        "The data set is stored as a custom `torchvision.datasets.cifar.CIFAR` object. You can check some of its properties with the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC1S_Gepuzv5"
      },
      "source": [
        "print(f\"Object type: {type(trainset)}\")\n",
        "print(f\"Training data shape: {trainset.data.shape}\")\n",
        "print(f\"Test data shape: {testset.data.shape}\")\n",
        "print(f\"Number of classes: {np.unique(trainset.targets).shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuliovENvJDb"
      },
      "source": [
        "### Data loaders\n",
        "\n",
        "A dataloader is an optimized data iterator that provides functionality for efficient shuffling, transformation and batching of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYUN7mzI-hRe"
      },
      "source": [
        "##@title Dataloader \n",
        "num_workers = multiprocessing.cpu_count()\n",
        "\n",
        "print(f'----> number of workers: {num_workers}')\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSbuFuYLvR9K"
      },
      "source": [
        "### Architecture: ResNet\n",
        "\n",
        "ResNet is a family of network architectures whose main property is that the network is organised as a stack of _residual blocks_. Residual blocks consist of a stack of layers whose output is added the input, making a _shortcut connection_.\n",
        "\n",
        "See the [original paper](https://arxiv.org/abs/1512.03385) for more details.\n",
        "\n",
        "ResNet is just a popular choice out of many others, but data augmentation works well in general. We just picked ResNet for illustration purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v59Q5-D6k6We"
      },
      "source": [
        "##@title model\n",
        "'''ResNet in PyTorch.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3JPo--uvZkK"
      },
      "source": [
        "#### Test on random data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWP4VtlVN-9G"
      },
      "source": [
        "##@title load the Model\n",
        "net = ResNet18()\n",
        "print('-----> verify if model is run on random data')\n",
        "y = net(Variable(torch.randn(1,3,32,32)))\n",
        "print('model loaded')\n",
        "\n",
        "result_folder = './results/'\n",
        "if not os.path.exists(result_folder):\n",
        "    os.makedirs(result_folder)\n",
        "\n",
        "logname = result_folder + net.__class__.__name__ + '_pretrain' + '.csv'\n",
        "\n",
        "if use_cuda:\n",
        "    net.cuda()\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    print('Using', torch.cuda.device_count(), 'GPUs.')\n",
        "    cudnn.benchmark = True\n",
        "    print('Using CUDA..')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-3YStW0xF4J"
      },
      "source": [
        "## Set up training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlSjgU3xvjYh"
      },
      "source": [
        "### Set loss function and optimizer\n",
        "\n",
        "We use the cross entropy loss, commonly used for classification, and stochastic gradient descent (SGD) as optimizer, with momentum and weight decay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5xQAAwDN_DT"
      },
      "source": [
        "##@title optimizer and criterion \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()   \n",
        "optimizer = optim.SGD(net.parameters(), lr=base_learning_rate, momentum=0.9, weight_decay=1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chCKez5VvoS9"
      },
      "source": [
        "### Train and test loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8ZQY61fN_Gw"
      },
      "source": [
        "##@title Training & Test functions\n",
        "\n",
        "def train(net, epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        if use_cuda:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = Variable(inputs), Variable(targets)\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets.data).cpu().sum()\n",
        "        \n",
        "        if batch_idx % 500 == 0:\n",
        "            print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "    return (train_loss/batch_idx, 100.*correct/total)\n",
        "\n",
        "def test(net, epoch, outModelName):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            if use_cuda:\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets.data).cpu().sum()\n",
        "\n",
        "            if batch_idx % 200 == 0:\n",
        "                print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                    % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        checkpoint(net, acc, epoch, outModelName)\n",
        "    return (test_loss/batch_idx, 100.*correct/total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzzWOT71xKVw"
      },
      "source": [
        "### Auxiliary functions\n",
        "\n",
        "* `checkpoint()`: Store checkpoints of the model\n",
        "* `adjust_learning_rate()`: Decreases the learning rate (learning rate decay) at certain epochs of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo3YfXEcN_Ji"
      },
      "source": [
        "##@title checkpoint & adjust_learning_rate\n",
        "def checkpoint(model, acc, epoch, outModelName):\n",
        "    # Save checkpoint.\n",
        "    print('Saving..')\n",
        "    state = {\n",
        "        'state_dict': model.state_dict(),\n",
        "        'acc': acc,\n",
        "        'epoch': epoch,\n",
        "        'rng_state': torch.get_rng_state()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, f'./checkpoint/{outModelName}.t7')\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
        "    lr = base_learning_rate\n",
        "    if epoch <= 9 and lr > 0.1:\n",
        "        # warm-up training for large minibatch\n",
        "        lr = 0.1 + (base_learning_rate - 0.1) * epoch / 10.\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    if epoch >= 150:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr6sX2IHyHS0"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "This is the loop where the model is trained for `max_epochs` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5pPpHB1N_NH"
      },
      "source": [
        "##@title start training\n",
        "outModelName = 'pretrain'\n",
        "if not os.path.exists(logname):\n",
        "    with open(logname, 'w') as logfile:\n",
        "        logwriter = csv.writer(logfile, delimiter=',')\n",
        "        logwriter.writerow(['epoch', 'train loss', 'train acc', 'test loss', 'test acc'])\n",
        "\n",
        "for epoch in range(start_epoch, max_epochs):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train_loss, train_acc = train(net, epoch)\n",
        "    test_loss, test_acc = test(net, epoch, outModelName)\n",
        "    with open(logname, 'a') as logfile:\n",
        "        logwriter = csv.writer(logfile, delimiter=',')\n",
        "        logwriter.writerow([epoch, train_loss, train_acc.item(), test_loss, test_acc.item()])\n",
        "    print(f'Epoch: {epoch} | train acc: {train_acc} | test acc: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycQ-Rj4A3Psh"
      },
      "source": [
        "## Transfer learning\n",
        "### Re-use the trained model to improve training on a different data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ZwOyXdzBfI"
      },
      "source": [
        "### Delete variables from the previous model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaCrHRB2IqsG"
      },
      "source": [
        "# delete the backbone network\n",
        "delete = True\n",
        "if delete:\n",
        "    del net\n",
        "    del trainset\n",
        "    del testset\n",
        "    del trainloader\n",
        "    del testloader\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wATyF1Z533su"
      },
      "source": [
        "#### Target dataset\n",
        "\n",
        "We will now use CIFAR-10 as _target_ data set. Again, with small tweaks we can get any other data we are interested in.\n",
        "\n",
        "CIFAR-10 is very similar to CIFAR-100, but it contains only 10 classes instead of 100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR0Cyk1ScnlA"
      },
      "source": [
        "##@title Target domain Data\n",
        "print('==> Preparing target domain data..')\n",
        "\n",
        "# CIFAR10 normalizing\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2023, 0.1994, 0.2010)\n",
        "num_classes = 10\n",
        "lr = 0.0001\n",
        "\n",
        "# torchvision transforms\n",
        "transform_train = transforms.Compose([])\n",
        "if torchvision_transforms:\n",
        "    transform_train.transforms.append(transforms.RandomCrop(32, padding=4))\n",
        "    transform_train.transforms.append(transforms.RandomHorizontalFlip())\n",
        "\n",
        "transform_train.transforms.append(transforms.ToTensor())\n",
        "transform_train.transforms.append(transforms.Normalize(mean, std))\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./CIFAR10', train=True, download=True, transform=transform_train)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./CIFAR10', train=False, download=True, transform=transform_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0IxAl4t0m3x"
      },
      "source": [
        "#### Select a subset of the data\n",
        "\n",
        "To simulate a lower data regime, where transfer learning can be useful.\n",
        "\n",
        "Choose percentage from the trainset. Set `percent = 1.0` to use the whole train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiJP1xc7x-oH"
      },
      "source": [
        "percent = 0.6\n",
        "\n",
        "trainset = percentageSplit(trainset, percent = percent)\n",
        "print('size of the new trainset: ', len(trainset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt599IWd58OU"
      },
      "source": [
        "#### Dataloaders\n",
        "\n",
        "As before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpvbV7lQNKt8"
      },
      "source": [
        "##@title Dataloader \n",
        "num_workers = multiprocessing.cpu_count()\n",
        "\n",
        "print(f'----> number of workers: {num_workers}')\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmcPmeEO6GcZ"
      },
      "source": [
        "### Load pre-trained model\n",
        "\n",
        "Load the checkpoint of the model previously trained on CIFAR-100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFpux8rtcno6"
      },
      "source": [
        "model = ResNet18()\n",
        "\n",
        "checkpointPath = '/content/checkpoint/pretrain.t7'\n",
        "\n",
        "print(' ===> loading pretrained model from: ', checkpointPath)\n",
        "if os.path.isfile(checkpointPath):\n",
        "    state_dict = torch.load(checkpointPath)\n",
        "    best_acc = state_dict['acc']\n",
        "    print('Best Accuracy:', best_acc)\n",
        "    if \"state_dict\" in state_dict:\n",
        "        state_dict = state_dict[\"state_dict\"]\n",
        "    # remove prefixe \"module.\"\n",
        "    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
        "    for k, v in model.state_dict().items():\n",
        "        if k not in list(state_dict):\n",
        "            print('key \"{}\" could not be found in provided state dict'.format(k))\n",
        "        elif state_dict[k].shape != v.shape:\n",
        "            print('key \"{}\" is of different shape in model and provided state dict'.format(k))\n",
        "            state_dict[k] = v\n",
        "    msg = model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"Load pretrained model with msg: {}\".format(msg))\n",
        "else:\n",
        "    raise Exception('No pretrained weights found')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fkhzTjv6Tvh"
      },
      "source": [
        "### Freeze model parameters\n",
        "\n",
        "In transfer learning, we usually do not re-train all the weights of the model, but only a subset of them, for instance the last layer. Here we first _freeze_ all the parameters of the model, and we will _unfreeze_ one layer below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bpCQ4gZcnsC"
      },
      "source": [
        "# Freeze the model parameters, you can also freeze some layers only\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTQUMH-W6yaS"
      },
      "source": [
        "### Loss function, optimizer and _unfreeze_ last layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gENJ9MRKCnRp"
      },
      "source": [
        "num_ftrs = model.linear.in_features\n",
        "model.linear = nn.Linear(num_ftrs, num_classes)\n",
        "model.cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = torch.optim.SGD(\n",
        "    model.linear.parameters(),\n",
        "    lr=lr,\n",
        "    momentum=0.9,\n",
        "    weight_decay=1e-4,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfGPk9lU7m9_"
      },
      "source": [
        "#### Check number of parameters\n",
        "\n",
        "We can calculate the number of total parameters and the number of trainable parameters, that is those that will be updated during training. Since we have freezed most of the parameters, the number of training parameters should be much smaller."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE8WJgXRLWD8"
      },
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Total Parameters:', total_params, 'Trainable parameters: ', trainable_total_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrmLb-gNF2d2"
      },
      "source": [
        "### Train the target model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9gN4sv2CnUn"
      },
      "source": [
        "outModelName = 'finetuned'\n",
        "logname = result_folder + model.__class__.__name__ + f'_{outModelName}.csv'\n",
        "\n",
        "if not os.path.exists(logname):\n",
        "    with open(logname, 'w') as logfile:\n",
        "        logwriter = csv.writer(logfile, delimiter=',')\n",
        "        logwriter.writerow(['epoch', 'train loss', 'train acc', 'test loss', 'test acc'])\n",
        "\n",
        "for epoch in range(start_epoch, max_epochs_target):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train_loss, train_acc = train(model, epoch)\n",
        "    test_loss, test_acc = test(model, epoch, outModelName)\n",
        "    with open(logname, 'a') as logfile:\n",
        "        logwriter = csv.writer(logfile, delimiter=',')\n",
        "        logwriter.writerow([epoch, train_loss, train_acc.item(), test_loss, test_acc.item()])\n",
        "    print(f'Epoch: {epoch} | train acc: {train_acc} | test acc: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PveZei3PF9Y_"
      },
      "source": [
        "## Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HruWfXDjEALw"
      },
      "source": [
        "##@title plot results\n",
        "results = pd.read_csv(f'/content/results/ResNet_{outModelName}.csv', sep =',')\n",
        "results.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qMnN7yXOsF8"
      },
      "source": [
        "train_accuracy = results['train acc'].values\n",
        "test_accuracy = results['test acc'].values\n",
        "\n",
        "print(f'Average Accuracy over {max_epochs_target} epochs:', sum(test_accuracy)//len(test_accuracy))\n",
        "print(f'best accuraccy over {max_epochs_target} epochs:', max(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3myL2_sUO7nw"
      },
      "source": [
        "figureName = 'figure' # change figure name\n",
        "\n",
        "plt.plot(results['epoch'].values, train_accuracy, label='train')\n",
        "plt.plot(results['epoch'].values, test_accuracy, label='test')\n",
        "plt.xlabel('Number of epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title(f'Train/Test Accuracy curve for {max_epochs} epochs')\n",
        "plt.savefig(f'/content/results/{figureName}.png')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}