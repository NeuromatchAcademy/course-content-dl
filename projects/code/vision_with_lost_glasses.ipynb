{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision with Lost Glasses.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e37d05e5fd3242c489c503958b45d66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_127f97561a6045dda006599dd5558c5c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_70fa277c45a74e128d1f01f45d3892bb",
              "IPY_MODEL_2d130ecd82834271b9fdf362cdf7c6b4"
            ]
          }
        },
        "127f97561a6045dda006599dd5558c5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70fa277c45a74e128d1f01f45d3892bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf4329355c29409fb781634b42c18e71",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15f368e5c9f0466fb4c0491a2c2c0758"
          }
        },
        "2d130ecd82834271b9fdf362cdf7c6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1285c0e1e55649ff8c0fe3a532a21ac9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [27:01&lt;00:00, 162.11s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3cc8250afe854e5a95a4f131145c42ff"
          }
        },
        "bf4329355c29409fb781634b42c18e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15f368e5c9f0466fb4c0491a2c2c0758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1285c0e1e55649ff8c0fe3a532a21ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3cc8250afe854e5a95a4f131145c42ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c089d221904046b98cb106066f3dea06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6557fffd76384f21b8068d7519668147",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6d3d3be8e7014116b0e58bbc7f3f3789",
              "IPY_MODEL_a0afe21c558043bdb6c6749e8f59c510"
            ]
          }
        },
        "6557fffd76384f21b8068d7519668147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d3d3be8e7014116b0e58bbc7f3f3789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9b333d7147e4ef6a94a28a7488f1333",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7f46f89752d14e9588c762892f80acba"
          }
        },
        "a0afe21c558043bdb6c6749e8f59c510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c31c04eef2324bc8b0deba15cb4a07b7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [54:59&lt;00:00, 329.96s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4eeaad46f5e24f789d130282c467ed55"
          }
        },
        "e9b333d7147e4ef6a94a28a7488f1333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7f46f89752d14e9588c762892f80acba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c31c04eef2324bc8b0deba15cb4a07b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4eeaad46f5e24f789d130282c467ed55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7caa8c3fdd2046e1a8fc3245d83ed5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eaca9544a2a14115810d02358b2985f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ef10cfa969848dcb4b44a7fe2691d40",
              "IPY_MODEL_2e59e2b7d9af4f97bf8b6033fab75896"
            ]
          }
        },
        "eaca9544a2a14115810d02358b2985f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ef10cfa969848dcb4b44a7fe2691d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_085017a28bf84700915e753b7dd86891",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e8f53a4a74e4eaaa886a2af4575c731"
          }
        },
        "2e59e2b7d9af4f97bf8b6033fab75896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a3569d4d4d840b1a07d1018d82ee2d3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10/10 [26:39&lt;00:00, 159.98s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_afe746e6e0674457a41e4e656e13a903"
          }
        },
        "085017a28bf84700915e753b7dd86891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e8f53a4a74e4eaaa886a2af4575c731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a3569d4d4d840b1a07d1018d82ee2d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "afe746e6e0674457a41e4e656e13a903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6paAcw5lJ7sI"
      },
      "source": [
        "# Vision with Lost Glasses: Modelling how the brain deals with noisy input\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Rishika Mohanta, Furkan Özçelik, Salomey Osei\n",
        "\n",
        "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
        "\n",
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>\n",
        "\n",
        "## Objective:\n",
        "\n",
        "Imagine you lost your spectacle and the world around you is completely blurred out. As you stumble around, you see a small animal walk towards you. Can you figure out what it is? Probably yes right?\n",
        "\n",
        "In this situation, or in foggy/night-time conditions, visual input is of poor quality; images are blurred and have low contrast and yet our brains manage to recognize it. Is it possible to model the process? Does previous experience help?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rg4EPP-fkp3s",
        "cellView": "form",
        "outputId": "5aacae63-6379-4118-e22f-4bbe0d6296c7"
      },
      "source": [
        "#@title Import required modules\n",
        "import os, zipfile, requests\n",
        "from shutil import copyfile,rmtree\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image,ImageFilter\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# For getting output of intermediate layers in Pytorch\n",
        "!pip install torch_intermediate_layer_getter\n",
        "from torch_intermediate_layer_getter import IntermediateLayerGetter as LayerGetter\n",
        "\n",
        "# For interactive visualization\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch_intermediate_layer_getter\n",
            "  Downloading torch_intermediate_layer_getter-0.1.post1.tar.gz (3.0 kB)\n",
            "Building wheels for collected packages: torch-intermediate-layer-getter\n",
            "  Building wheel for torch-intermediate-layer-getter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-intermediate-layer-getter: filename=torch_intermediate_layer_getter-0.1.post1-py3-none-any.whl size=3725 sha256=6421265f80579cebfc20df852971324ddcce6e567cc0893a188bf876be5c29c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/53/37/8b81c4711686fbae03a35d8d1dac5edd9e4af9221fa8e17f6f\n",
            "Successfully built torch-intermediate-layer-getter\n",
            "Installing collected packages: torch-intermediate-layer-getter\n",
            "Successfully installed torch-intermediate-layer-getter-0.1.post1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfLKCuFj_kdu",
        "cellView": "form"
      },
      "source": [
        "#@title Define Helpers\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "  return device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s3cLBy-AP87",
        "outputId": "a4bb2d20-d0e7-4b9c-824d-533d3c9b254c"
      },
      "source": [
        "SEED = 42\n",
        "set_seed(seed=SEED)\n",
        "DEVICE = set_device()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed 42 has been set.\n",
            "GPU is enabled in this notebook.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJcM5JlncF_h"
      },
      "source": [
        "**The Asirra dataset**\n",
        "\n",
        "The CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) or HIP (Human Interactive Proof) challenge is the motivation behind the creation of this dataset.\n",
        "\n",
        "\n",
        "Asirra (Animal Species Image Recognition for Restricting Access) is a HIP that works by asking users to identify photographs of cats and dogs. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately.\n",
        "\n",
        "*Reference: Dataset can be found here* (https://www.kaggle.com/c/dogs-vs-cats)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUYRm3Chd2JH",
        "cellView": "form"
      },
      "source": [
        "#@title Download Cleaned Data\n",
        "#@markdown This particular dataset is a cleaned and processed version of Kaggle's Dogs vs. Cats Competition. The Data is organised as three folders `dataset`, `dataset_blur_2`, `dataset_blur_5`.\n",
        "#@markdown Each folder has a `train` and `test` subfolder and each subfolder has a `cat` and `dog` folder which contain the images.\n",
        "\n",
        "#@markdown `dataset` contains the clear images\n",
        "\n",
        "#@markdown `dataset_blur_2` contains the images with a Gaussian Blur (radius = 2)\n",
        "\n",
        "#@markdown `dataset_blur_5` contains the images with a Gaussian Blur (radius = 5)\n",
        "\n",
        "filenames = [\"catvdog_clear.zip\",\"catvdog_blur_2.zip\",\"catvdog_blur_5.zip\"]\n",
        "urls = [\"https://osf.io/hj2gd/download\",\n",
        "        \"https://osf.io/xp6qd/download\",\n",
        "        \"https://osf.io/wj43a/download\"]\n",
        "for fname, url in zip(filenames, urls):\n",
        "  if not os.path.isfile(fname):\n",
        "    try:\n",
        "      r = requests.get(url)\n",
        "    except requests.ConnectionError:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      if r.status_code != requests.codes.ok:\n",
        "        print(\"!!! Failed to download data !!!\")\n",
        "      else:\n",
        "        with open(fname, \"wb\") as fid:\n",
        "          fid.write(r.content)\n",
        "\n",
        "for fname in filenames:  \n",
        "  zip_ref = zipfile.ZipFile(fname, 'r')\n",
        "  zip_ref.extractall()\n",
        "  zip_ref.close()\n",
        "  os.remove(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2YtNSmhwA-4"
      },
      "source": [
        "## Define preprocessing pipeline and dataloaders\n",
        "Here we only load the dataset with clear images and gaussian blur with radius 5. You can explore the other by loading the appropriate folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw8ZNSFW0eh_"
      },
      "source": [
        "# Define Preprocessing Filters\n",
        "preprocessing = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Resize((256,256)),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "# Create Clean Training Dataset using ImageFolder\n",
        "clear_train_data = torchvision.datasets.ImageFolder(\n",
        "    root=\"dataset/train\",\n",
        "    transform=preprocessing\n",
        ")\n",
        "\n",
        "# Create Clean Test Dataset using ImageFolder\n",
        "clear_test_data = torchvision.datasets.ImageFolder(\n",
        "    root=\"dataset/test\",\n",
        "    transform=preprocessing\n",
        ")\n",
        "\n",
        "# Create Noisy Training Dataset using ImageFolder\n",
        "noisy_train_data = torchvision.datasets.ImageFolder(\n",
        "    root=\"dataset_blur_5/train\",\n",
        "    transform=preprocessing\n",
        ")\n",
        "\n",
        "# Create Noisy Test Dataset using ImageFolder\n",
        "noisy_test_data = torchvision.datasets.ImageFolder(\n",
        "    root=\"dataset_blur_5/test\",\n",
        "    transform=preprocessing\n",
        ")\n",
        "\n",
        "# function to apply a training-validation set split on a dataset\n",
        "def validation_split(train_data,val_ratio = 0.2):\n",
        "  train_indices, val_indices, _, _ = train_test_split(range(len(train_data)),train_data.targets,\n",
        "                                                      stratify=train_data.targets,test_size=val_ratio)\n",
        "  train_split = torch.utils.data.Subset(train_data, train_indices)\n",
        "  val_split = torch.utils.data.Subset(train_data, val_indices)\n",
        "  return train_split,val_split\n",
        "\n",
        "# Define Batch Size\n",
        "batch_size = 128\n",
        "\n",
        "# Define Dataloaders for Training, Validation and Test sets\n",
        "clear_train_split,clear_val_split = validation_split(clear_train_data)\n",
        "clear_train_batches = torch.utils.data.DataLoader(clear_train_split, batch_size=batch_size, shuffle=True)\n",
        "clear_val_batches = torch.utils.data.DataLoader(clear_val_split, batch_size=batch_size, shuffle=True)\n",
        "clear_test_batches = torch.utils.data.DataLoader(clear_test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "noisy_train_split,noisy_val_split = validation_split(noisy_train_data)\n",
        "noisy_train_batches = torch.utils.data.DataLoader(noisy_train_split, batch_size=batch_size, shuffle=True)\n",
        "noisy_val_batches = torch.utils.data.DataLoader(noisy_val_split, batch_size=batch_size, shuffle=True)\n",
        "noisy_test_batches = torch.utils.data.DataLoader(noisy_test_data, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMnfpKkZ3jDJ"
      },
      "source": [
        "# Get an example of a clear and noisy versions of cat and dog image\n",
        "clear_cat_image = clear_train_data[5][0].unsqueeze(0)\n",
        "clear_dog_image = clear_train_data[19997][0].unsqueeze(0)\n",
        "noisy_cat_image = noisy_train_data[5][0].unsqueeze(0)\n",
        "noisy_dog_image = noisy_train_data[19997][0].unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZrKMEbSyqdA"
      },
      "source": [
        "## Plot examples of Noisy and Noise-free Images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxxiFGfX58aG"
      },
      "source": [
        "fig = plt.figure(figsize=(12,3))\n",
        "ax=fig.add_subplot(1,4,1)\n",
        "ax.imshow(clear_cat_image.squeeze(0).permute(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.title('Cat (Noise-free)')\n",
        "ax=fig.add_subplot(1,4,2)\n",
        "ax.imshow(clear_dog_image.squeeze(0).permute(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.title('Dog (Noise-free)')\n",
        "ax=fig.add_subplot(1,4,3)\n",
        "ax.imshow(noisy_cat_image.squeeze(0).permute(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.title('Cat (Noisy)')\n",
        "ax=fig.add_subplot(1,4,4)\n",
        "ax.imshow(noisy_dog_image.squeeze(0).permute(1,2,0))\n",
        "plt.axis('off')\n",
        "plt.title('Dog (Noisy)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in5WMi3St0H_"
      },
      "source": [
        "# Deep Learning Models of the Ventral Visual Stream\n",
        "\n",
        "AlexNet and other Deep Convolutional Neural networks are considered to be a good model of the ventral visual stream for visual categorization. Studies have tried to compare different layers of the Deep Neural Networks with the activity in different brain regions. See Below.\n",
        "\n",
        "<img src=\"https://www.biorxiv.org/content/biorxiv/early/2020/01/02/407007/F1.large.jpg\" width=\"500\" />\n",
        "\n",
        "Figure from [Schrimpf et al. 2020](https://www.biorxiv.org/content/10.1101/407007v2.full)\n",
        "\n",
        "Lets try to model the visual categorisation task using AlexNet as model of the ventral visual stream.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AELhTjQ4BdxG"
      },
      "source": [
        "## Create Test Neural Network (AlexNet, 2012 with Batch Normalization and a downscaling factor)\n",
        "\n",
        "We add Batch Normalization to make training faster and add a scaling factor to test the architechture in a relatively smaller and computationally feasible model.\n",
        "The parameter `downscale` scales down the number of channels in the network by the factor given by the value. For computational feasibility on Google Colab we use an Alexnet downscaled by a factor of 2.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1838/1*bD_DMBtKwveuzIkQTwjKQQ.png\" width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih8YL8HQ4_6f"
      },
      "source": [
        "# Define AlexNet with different modules representing different brain areas\n",
        "class AlexNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes: int = 1000, downscale: int = 1) -> None:\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.retina = nn.Sequential(\n",
        "            nn.Conv2d(3, 64//downscale, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(64//downscale),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.lgn = nn.Sequential(\n",
        "            nn.Conv2d(64//downscale, 192//downscale, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(192//downscale),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.v1 = nn.Sequential(\n",
        "            nn.Conv2d(192//downscale, 384//downscale, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(384//downscale),\n",
        "        )\n",
        "        self.v2 = nn.Sequential(\n",
        "            nn.Conv2d(384//downscale, 256//downscale, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256//downscale),\n",
        "        )\n",
        "        self.v4 = nn.Sequential(\n",
        "            nn.Conv2d(256//downscale, 256//downscale, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256//downscale),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.it = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256//downscale * 6 * 6, 4096//downscale),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096//downscale, 4096//downscale),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.classifier = nn.Linear(4096//downscale, num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.retina(x)\n",
        "        x = self.lgn(x)\n",
        "        x = self.v1(x)\n",
        "        x = self.v2(x)\n",
        "        x = self.v4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.it(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQqobhAN6qMy"
      },
      "source": [
        "# Visualize the Neural Network using TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WllwBNwWVAvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83cc096e-d483-42d4-9234-21660bc0dc7b"
      },
      "source": [
        "# Create an neural network object and visualise it with an example image\n",
        "net=AlexNet(num_classes=2,downscale=2)\n",
        "writer = SummaryWriter('runs/AlexNet')\n",
        "writer.add_graph(net, clear_dog_image)\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsQiC81-WOPe"
      },
      "source": [
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU8TslpZ24uy"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8q0UVpaqXS9"
      },
      "source": [
        "# define function for running some epochs of training\n",
        "def train(num_epochs,train_batch,val_batch,training_losses=None,validation_losses=None): #Training \n",
        "  net.train()\n",
        "  if training_losses is None:\n",
        "    training_losses = []\n",
        "  if validation_losses is None:\n",
        "    validation_losses = []\n",
        "  for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "    for batch_idx, (data, target) in enumerate(train_batch):\n",
        "      data = data.to(DEVICE).float()\n",
        "      target = target.to(DEVICE).long()\n",
        "      # reset the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "      # forward pass + backward pass + optimize\n",
        "      prediction = net(data)\n",
        "      loss = criterion(prediction, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      training_losses += [loss.item()]\n",
        "    for batch_idx, (data, target) in enumerate(val_batch):\n",
        "      data = data.to(DEVICE).float()\n",
        "      target = target.to(DEVICE).long()\n",
        "      # forward pass only\n",
        "      prediction = net(data)\n",
        "      loss = criterion(prediction, target)\n",
        "      validation_losses += [loss.item()]\n",
        "  return training_losses, validation_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFlPXSIE3M5j"
      },
      "source": [
        "## Accuracy Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dPJb_Ru3AMy"
      },
      "source": [
        "# define function to calculate current accuracy with a given dataloader\n",
        "def accuracy(dataloader): #Get the accuracies\n",
        "  net.eval()\n",
        "  correct = 0\n",
        "  count = 0\n",
        "  for data, target in dataloader:\n",
        "    data = data.to(DEVICE).float()\n",
        "    target = target.to(DEVICE).long()\n",
        "\n",
        "    prediction = net(data)\n",
        "    _, predicted = torch.max(prediction, 1)\n",
        "    count += target.size(0)\n",
        "    correct += (predicted == target).sum().item()\n",
        "\n",
        "  acc = 100 * correct / count\n",
        "  return count, acc\n",
        "\n",
        "# define function to evaluate and print training and test accuracy \n",
        "def evaluate(net,title=\"\"):\n",
        "  net.eval()\n",
        "  train_count, train_acc = accuracy(clear_train_batches)\n",
        "  test_count, test_acc = accuracy(clear_test_batches)\n",
        "  print(f'Accuracy on the {train_count} clear training samples {title}: {train_acc:0.2f}')\n",
        "  print(f'Accuracy on the {test_count} clear testing samples {title}: {test_acc:0.2f}')\n",
        "  train_count, train_acc = accuracy(noisy_train_batches)\n",
        "  test_count, test_acc = accuracy(noisy_test_batches)\n",
        "  print(f'Accuracy on the {train_count} blurry training samples {title}: {train_acc:0.2f}')\n",
        "  print(f'Accuracy on the {test_count} blurry testing samples {title}: {test_acc:0.2f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iflMFn9A8_Mp"
      },
      "source": [
        "## Hypothesis:\n",
        "$H_0$: The visual system model with experience of seeing images without blur will have a much better performance on the blurry images out of the box compared to the naive learner.\n",
        "\n",
        "$H_1$: The visual system model with experience of seeing images without blur will have a better performance on the blurry images after training compared to the naive learner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbmkyEC6nkng"
      },
      "source": [
        "# Naive Learner\n",
        "\n",
        "A naive learner model looks at only the blurry images and tries to learn the difference between a cat and a dog \n",
        "\n",
        "*Warning: Training takes approximately 2-3 minutes per epoch so choose the number of epochs with that in mind. With default values, this cell needs ~12-15 minutes to run.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476,
          "referenced_widgets": [
            "e37d05e5fd3242c489c503958b45d66c",
            "127f97561a6045dda006599dd5558c5c",
            "70fa277c45a74e128d1f01f45d3892bb",
            "2d130ecd82834271b9fdf362cdf7c6b4",
            "bf4329355c29409fb781634b42c18e71",
            "15f368e5c9f0466fb4c0491a2c2c0758",
            "1285c0e1e55649ff8c0fe3a532a21ac9",
            "3cc8250afe854e5a95a4f131145c42ff"
          ]
        },
        "id": "uZjnVHnO6q9u",
        "outputId": "bf0efa61-a74f-4668-dfac-ed89cefdb639"
      },
      "source": [
        "# Define network, loss function and optimizer\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
        "net.to(DEVICE)\n",
        "\n",
        "# Evaluate before training\n",
        "evaluate(net,\"before training\")\n",
        "\n",
        "# Define number of epochs\n",
        "num_pretraining_epochs = 0\n",
        "num_training_epochs = 5\n",
        "num_epochs = num_pretraining_epochs+num_training_epochs\n",
        "\n",
        "# Save network weights\n",
        "torch.save(net.state_dict(), \"naive_before_training\")\n",
        "\n",
        "# Training loop\n",
        "naive_training_losses, naive_validation_losses = train(num_training_epochs,noisy_train_batches,noisy_val_batches)\n",
        "\n",
        "# Save network weights\n",
        "torch.save(net.state_dict(), \"naive_after_training\")\n",
        "\n",
        "# Evaluate after training\n",
        "evaluate(net,\"after training\")\n",
        "\n",
        "# Plot Loss over epochs\n",
        "plt.figure()\n",
        "plt.plot(np.arange(1,num_epochs+1),[np.mean(x) for x in np.array_split(naive_training_losses,num_training_epochs)],\"o-\",label=\"Training Loss\")\n",
        "plt.plot(np.arange(1,num_epochs+1),[np.mean(x) for x in np.array_split(naive_validation_losses,num_training_epochs)],\"o-\",label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the 17953 clear training samples before training: 48.65\n",
            "Accuracy on the 2494 clear testing samples before training: 49.40\n",
            "Accuracy on the 17953 blurry training samples before training: 48.73\n",
            "Accuracy on the 2494 blurry testing samples before training: 48.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e37d05e5fd3242c489c503958b45d66c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on the 17953 clear training samples after training: 89.04\n",
            "Accuracy on the 2494 clear testing samples after training: 85.73\n",
            "Accuracy on the 17953 blurry training samples after training: 93.59\n",
            "Accuracy on the 2494 blurry testing samples after training: 86.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d112neKYqGtk"
      },
      "source": [
        "# Expert Experienced Learner\n",
        "\n",
        "A expert learner model first learns to look at less noisy images of cats and dogs and then try to distinguish the blurry images. It then learns on blurry images and tries to understand the difference between a cat and a dog \n",
        "\n",
        "*Warning: Pretraining takes approximately 5-6 minutes and Training takes 2-3 minutes per epoch so choose the number of epochs with that in mind. With default values, this cell needs ~45 minutes to run.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910,
          "referenced_widgets": [
            "c089d221904046b98cb106066f3dea06",
            "6557fffd76384f21b8068d7519668147",
            "6d3d3be8e7014116b0e58bbc7f3f3789",
            "a0afe21c558043bdb6c6749e8f59c510",
            "e9b333d7147e4ef6a94a28a7488f1333",
            "7f46f89752d14e9588c762892f80acba",
            "c31c04eef2324bc8b0deba15cb4a07b7",
            "4eeaad46f5e24f789d130282c467ed55",
            "7caa8c3fdd2046e1a8fc3245d83ed5d9",
            "eaca9544a2a14115810d02358b2985f0",
            "7ef10cfa969848dcb4b44a7fe2691d40",
            "2e59e2b7d9af4f97bf8b6033fab75896",
            "085017a28bf84700915e753b7dd86891",
            "5e8f53a4a74e4eaaa886a2af4575c731",
            "7a3569d4d4d840b1a07d1018d82ee2d3",
            "afe746e6e0674457a41e4e656e13a903"
          ]
        },
        "id": "wJ-jL-SZqF7Y",
        "outputId": "9d606146-31b3-422e-f692-d9a8efbb228d"
      },
      "source": [
        "# Define network, loss function and optimizer\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=3e-4)\n",
        "net.to(DEVICE)\n",
        "\n",
        "# Evaluate before pretraining\n",
        "evaluate(net,\"before training\")\n",
        "\n",
        "# Define number of epochs\n",
        "num_pretraining_epochs = 5\n",
        "num_training_epochs = 5\n",
        "num_epochs = num_pretraining_epochs+num_training_epochs\n",
        "\n",
        "# Save network weights\n",
        "torch.save(net.state_dict(), \"expert_before_training\")\n",
        "\n",
        "# Pretraining loop\n",
        "training_losses, validation_losses = train(num_pretraining_epochs,clear_train_batches,clear_val_batches)\n",
        "\n",
        "# Evaluate after pretraining\n",
        "evaluate(net,\"after pretraining\")\n",
        "\n",
        "# Save network weights\n",
        "torch.save(net.state_dict(), \"expert_after_pretraining\")\n",
        "\n",
        "# Training loop\n",
        "experienced_training_losses, experienced_validation_losses = train(num_training_epochs,noisy_train_batches,noisy_val_batches,\n",
        "                                                       training_losses=training_losses,validation_losses=validation_losses)\n",
        "\n",
        "# Save network weights\n",
        "torch.save(net.state_dict(), \"expert_after_training\")\n",
        "\n",
        "# Evaluate after training\n",
        "evaluate(net,\"after training\")\n",
        "\n",
        "# Plot Loss over epochs\n",
        "plt.figure()\n",
        "plt.plot(np.arange(1,num_epochs+1),[np.mean(x) for x in np.array_split(experienced_training_losses,num_pretraining_epochs+num_training_epochs)],\"o-\",label=\"Training Loss\")\n",
        "plt.plot(np.arange(1,num_epochs+1),[np.mean(x) for x in np.array_split(experienced_validation_losses,num_pretraining_epochs+num_training_epochs)],\"o-\",label=\"Validation Loss\")\n",
        "plt.axvline(num_pretraining_epochs,linestyle='dashed',color='k')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the 17953 clear training samples before training: 50.00\n",
            "Accuracy on the 2494 clear testing samples before training: 50.00\n",
            "Accuracy on the 17953 blurry training samples before training: 50.00\n",
            "Accuracy on the 2494 blurry testing samples before training: 50.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c089d221904046b98cb106066f3dea06",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
            "  \" Skipping tag %s\" % (size, len(data), tag)\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on the 17953 clear training samples after pretraining: 97.69\n",
            "Accuracy on the 2494 clear testing samples after pretraining: 92.46\n",
            "Accuracy on the 17953 blurry training samples after pretraining: 67.43\n",
            "Accuracy on the 2494 blurry testing samples after pretraining: 67.44\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7caa8c3fdd2046e1a8fc3245d83ed5d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy on the 17953 clear training samples after training: 89.59\n",
            "Accuracy on the 2494 clear testing samples after training: 87.77\n",
            "Accuracy on the 17953 blurry training samples after training: 98.42\n",
            "Accuracy on the 2494 blurry testing samples after training: 88.65\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8hNc1FOCdSG"
      },
      "source": [
        "Wow thats interesting! The model of the visual system has a hard time to distinguish between blurry cats and dogs even after having seen images of cats and dog before! It takes further training on the blurry images to be able to learn the difference well. Even then, it is only a little better than the naive model. Would it be same for a human being?\n",
        "\n",
        "Now, if you want to understand thats happening during training and pretraining, is there a way to visualise the effect of on the network itself? Yes!! You can look at either how: \n",
        "\n",
        "1.   Convolution kernels, OR\n",
        "2.   Average output of Intermediate layers\n",
        "\n",
        "have changed over pretraining and training to try and understand how the networks learns to look for more specific features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c5MSVD9njel"
      },
      "source": [
        "## Looking at Convolution Kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As5v2AnESuKf",
        "cellView": "form"
      },
      "source": [
        "#@title Helper Function to plot the 16 filters of the first layer\n",
        "def plot_filter(net,title=\"\"):\n",
        "  layer = 0\n",
        "  with torch.no_grad():\n",
        "    params = list(net.parameters())\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(4, 4))\n",
        "    filters = []\n",
        "    for filter_index in range(min(16,params[layer].shape[0])):\n",
        "      row_index = filter_index // 4\n",
        "      col_index = filter_index % 4\n",
        "\n",
        "      filter = params[layer][filter_index,:,:,:]\n",
        "      filter_image = filter.permute((1,2,0))\n",
        "      scale = np.abs(filter_image).max()\n",
        "      scaled_image = filter_image / (2 * scale) + 0.5\n",
        "      filters.append(scaled_image)\n",
        "      axs[row_index, col_index].imshow(scaled_image)\n",
        "      axs[row_index, col_index].axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taLddy8ZyUVb"
      },
      "source": [
        "### Naive Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4NTpHmrwpgu"
      },
      "source": [
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('naive_before_training'))\n",
        "plot_filter(net,\"Before Training (Naive Model)\")\n",
        "\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('naive_after_training'))\n",
        "plot_filter(net,\"After Training (Naive Model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc9-vqfeyZo-"
      },
      "source": [
        "### Expert Experienced Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeR5v6E_zJqi"
      },
      "source": [
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('expert_before_training'))\n",
        "plot_filter(net,\"Before Training (Expert Model)\")\n",
        "\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('expert_after_pretraining'))\n",
        "plot_filter(net,\"After Pretraining (Expert Model)\")\n",
        "\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('expert_after_training'))\n",
        "plot_filter(net,\"After Training (Expert Model)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXhatjOBB011"
      },
      "source": [
        "Looks like with the small amount of training, the filters are not very meaningful. Also, there seem to be only very small differences are visible at the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcVETQOb0jFI"
      },
      "source": [
        "## Looking at Intermediate Layer Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tluurBckgckx"
      },
      "source": [
        "# choose intermidiate layers from which to get the output\n",
        "return_layers = {\n",
        "    # \"<name of layer in the AlexNet Class>\" : \"<key for the layer output in the returned dictionary>\"\n",
        "      'v1': 'v1',\n",
        "      'v2': 'v2',\n",
        "      'v4': 'v4',\n",
        "  }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYzqFCsziUdb",
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions to get and plot intermediate layer output\n",
        "\n",
        "def plot_intermediate_layers(image,net,return_layers=return_layers):\n",
        "  # Get output for the image from the intermediate layers\n",
        "  intermediate_output = LayerGetter(net, return_layers=return_layers)(image)\n",
        "  fig = plt.figure(figsize=(12,3))\n",
        "  ax=fig.add_subplot(1,4,1)\n",
        "  ax.imshow(image.squeeze(0).permute(1,2,0))\n",
        "  plt.axis('off')\n",
        "  plt.title('Original Image')\n",
        "  ax=fig.add_subplot(1,4,2)\n",
        "  ax.imshow(intermediate_output[0]['v1'].detach().cpu().squeeze(0).mean(axis=0))\n",
        "  plt.axis('off')\n",
        "  plt.title('V1 layer (Average)')\n",
        "  ax=fig.add_subplot(1,4,3)\n",
        "  ax.imshow(intermediate_output[0]['v2'].detach().cpu().squeeze(0).mean(axis=0))\n",
        "  plt.axis('off')\n",
        "  plt.title('V2 layer (Average)')\n",
        "  ax=fig.add_subplot(1,4,4)\n",
        "  ax.imshow(intermediate_output[0]['v4'].detach().cpu().squeeze(0).mean(axis=0))\n",
        "  plt.axis('off')\n",
        "  plt.title('V4 layer (Average)')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzTiBeU25uMx"
      },
      "source": [
        "### Naive Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOX7uMjOzHQe"
      },
      "source": [
        "print(\"Naive Model\\n=============\")\n",
        "print(\"Before Training\")\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('naive_before_training'))\n",
        "plot_intermediate_layers(noisy_dog_image,net)\n",
        "plot_intermediate_layers(noisy_cat_image,net)\n",
        "\n",
        "print(\"After Training\")\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('naive_after_training'))\n",
        "plot_intermediate_layers(noisy_dog_image,net)\n",
        "plot_intermediate_layers(noisy_cat_image,net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9khfSJGC_6Fs"
      },
      "source": [
        "Over training, the naive model seems to have learnt more complex features as apparent from the intermediate layer outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yuyAalw50Mi"
      },
      "source": [
        "### Expert Experienced Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4qjdEfx5biR"
      },
      "source": [
        "print(\"Expert Model\\n=============\")\n",
        "print(\"Before Training\")\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('expert_before_training'))\n",
        "plot_intermediate_layers(noisy_dog_image,net)\n",
        "plot_intermediate_layers(noisy_cat_image,net)\n",
        "\n",
        "print(\"After Pretraining\")\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('expert_after_pretraining'))\n",
        "plot_intermediate_layers(noisy_dog_image,net)\n",
        "plot_intermediate_layers(noisy_cat_image,net)\n",
        "\n",
        "print(\"After Training\")\n",
        "net = AlexNet(num_classes=2,downscale=2)\n",
        "# Load and assign previous state\n",
        "net.load_state_dict(torch.load('expert_after_training'))\n",
        "plot_intermediate_layers(noisy_dog_image,net)\n",
        "plot_intermediate_layers(noisy_cat_image,net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uuv-Xv--AMii"
      },
      "source": [
        "Clearly, the network output is only somewhat similar between pretraining and training. The differences seem to suggest that the process of learning over blurred images did change the features the network focused which probably contibuted to improved response to blurry images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_TfsxjKk2KZ",
        "cellView": "form"
      },
      "source": [
        "#@markdown If you wish to generate cat and dog images with other degrees of blur or different type of noise \n",
        "#@markdown refer to this cell which contains the code for dataset cleaning and filtering the original dataset.\n",
        "#@markdown Feel free to uncomment and change the code to generate other variations of the dataset. \n",
        "\n",
        "#@markdown *Note: The dataset generation process can take a while to run.*\n",
        "\n",
        "\n",
        "# # Download the Data\n",
        "# if \"cats-and-dogs.zip\" not in os.listdir():\n",
        "#   !wget --no-check-certificate \\\n",
        "#     \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
        "#     -O \"cats-and-dogs.zip\"\n",
        "\n",
        "# local_zip = 'cats-and-dogs.zip'\n",
        "# zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "# zip_ref.extractall()\n",
        "# zip_ref.close()\n",
        "\n",
        "# cat_folder = \"PetImages/Cat/\"\n",
        "# dog_folder = \"PetImages/Dog/\"\n",
        "\n",
        "# def check_file(file):\n",
        "#   try:\n",
        "#     im = Image.open(file).convert('RGB')\n",
        "#     #im = im.filter(ImageFilter.BLUR)\n",
        "#     return True\n",
        "#   except:\n",
        "#     return False\n",
        "\n",
        "# cat_files = list(filter(check_file, [cat_folder+i for i in os.listdir(cat_folder)]))\n",
        "# dog_files = list(filter(check_file, [dog_folder+i for i in os.listdir(dog_folder)]))\n",
        "# np.random.shuffle(cat_files)\n",
        "# np.random.shuffle(dog_files)\n",
        "\n",
        "# # Train test split\n",
        "# test_ratio = 0.1\n",
        "# # assert len(cat_files) == len(dog_files)\n",
        "# N = min(len(cat_files),len(dog_files))\n",
        "\n",
        "# training_length = int((1-test_ratio)*N)\n",
        "# train_indices = np.arange(training_length)\n",
        "# test_indices = np.arange(training_length,N)\n",
        "\n",
        "# # Create data directory\n",
        "# try:\n",
        "#   os.mkdir(\"dataset\")\n",
        "#   os.mkdir(\"dataset/train/\")\n",
        "#   os.mkdir(\"dataset/test/\")\n",
        "#   os.mkdir(\"dataset/train/cat/\")\n",
        "#   os.mkdir(\"dataset/train/dog/\")\n",
        "#   os.mkdir(\"dataset/test/cat/\")\n",
        "#   os.mkdir(\"dataset/test/dog/\")\n",
        "#   os.mkdir(\"dataset_blur_5\")\n",
        "#   os.mkdir(\"dataset_blur_5/train/\")\n",
        "#   os.mkdir(\"dataset_blur_5/test/\")\n",
        "#   os.mkdir(\"dataset_blur_5/train/cat/\")\n",
        "#   os.mkdir(\"dataset_blur_5/train/dog/\")\n",
        "#   os.mkdir(\"dataset_blur_5/test/cat/\")\n",
        "#   os.mkdir(\"dataset_blur_5/test/dog/\")\n",
        "#   os.mkdir(\"dataset_blur_2\")\n",
        "#   os.mkdir(\"dataset_blur_2/train/\")\n",
        "#   os.mkdir(\"dataset_blur_2/test/\")\n",
        "#   os.mkdir(\"dataset_blur_2/train/cat/\")\n",
        "#   os.mkdir(\"dataset_blur_2/train/dog/\")\n",
        "#   os.mkdir(\"dataset_blur_2/test/cat/\")\n",
        "#   os.mkdir(\"dataset_blur_2/test/dog/\")\n",
        "# except:\n",
        "#   pass\n",
        "\n",
        "# for i in tqdm.tqdm(range(training_length)):\n",
        "#   target = f\"dataset/train/cat/{i+1}.jpg\"\n",
        "#   copyfile(cat_files[i],target)\n",
        "#   target = f\"dataset/train/dog/{i+1}.jpg\"\n",
        "#   copyfile(dog_files[i],target)\n",
        "#   target = f\"dataset_blur_2/train/cat/{i+1}.jpg\"\n",
        "#   im = Image.open(cat_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
        "#   im.save(target)\n",
        "#   target = f\"dataset_blur_2/train/dog/{i+1}.jpg\"\n",
        "#   im = Image.open(dog_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
        "#   im.save(target)\n",
        "#   target = f\"dataset_blur_5/train/cat/{i+1}.jpg\"\n",
        "#   im = Image.open(cat_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
        "#   im.save(target)\n",
        "#   target = f\"dataset_blur_5/train/dog/{i+1}.jpg\"\n",
        "#   im = Image.open(dog_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
        "#   im.save(target)\n",
        "\n",
        "# for i in tqdm.tqdm(range(training_length,N)):\n",
        "#   target = f\"dataset/test/cat/{int(i-training_length+1)}.jpg\"\n",
        "#   copyfile(cat_files[i],target)\n",
        "#   target = f\"dataset/test/dog/{int(i-training_length+1)}.jpg\"\n",
        "#   copyfile(dog_files[i],target)\n",
        "#   target = f\"dataset_blur_2/test/cat/{int(i-training_length+1)}.jpg\"\n",
        "#   im = Image.open(cat_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
        "#   im.save(target)\n",
        "#   target = f\"dataset_blur_2/test/dog/{int(i-training_length+1)}.jpg\"\n",
        "#   im = Image.open(dog_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 2))\n",
        "#   im.save(target)\n",
        "#   target = f\"dataset_blur_5/test/cat/{int(i-training_length+1)}.jpg\"\n",
        "#   im = Image.open(cat_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
        "#   im.save(target)\n",
        "#   target = f\"dataset_blur_5/test/dog/{int(i-training_length+1)}.jpg\"\n",
        "#   im = Image.open(dog_files[i]).convert('RGB')\n",
        "#   im = im.filter(ImageFilter.GaussianBlur(radius = 5))\n",
        "#   im.save(target)\n",
        "  \n",
        "# rmtree(\"PetImages\")\n",
        "\n",
        "# def zipdir(path, ziph):\n",
        "#     # ziph is zipfile handle\n",
        "#     for root, dirs, files in os.walk(path):\n",
        "#         for file in files:\n",
        "#             ziph.write(os.path.join(root, file), \n",
        "#                        os.path.relpath(os.path.join(root, file), \n",
        "#                                        os.path.join(path, '..')))\n",
        "            \n",
        "# zipf = zipfile.ZipFile('catvdog_clear.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "# zipdir('dataset/', zipf)\n",
        "# zipf.close()\n",
        "# zipf = zipfile.ZipFile('catvdog_blur_2.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "# zipdir('dataset_blur_2/', zipf)\n",
        "# zipf.close()\n",
        "# zipf = zipfile.ZipFile('catvdog_blur_5.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "# zipdir('dataset_blur_5/', zipf)\n",
        "# zipf.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}