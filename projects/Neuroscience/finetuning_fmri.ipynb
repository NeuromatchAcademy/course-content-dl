{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/Neuroscience/finetuning_fmri.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Moving beyond Labels: Finetuning CNNs on BOLD response\n",
    "\n",
    "**By Neuromatch Academy**\n",
    "\n",
    "__Content creators:__ Aakash Agrawal\n",
    "\n",
    "__Production editors:__ Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Finetuning CNN using regression loss\n",
    "\n",
    "- CNN are proven to be a better model of visual cortex, but the goal of visual cortex is not limited to image classification. \n",
    "\n",
    "- Typically, to model visual cortex responses using CNNs, we - \n",
    "\n",
    "  1. Extract features of intermediate layers\n",
    "  2. Reduce dimensionality of the data using techniques like PCA\n",
    "  3. Perform regression to predict neural data. \n",
    "\n",
    "- This approach fails to predict all the variance in the data.\n",
    "\n",
    "**A better approach is to train CNNs directly on the neural response.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "!pip install Pillow --quiet\n",
    "!pip install torch_intermediate_layer_getter --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "#  Imports\n",
    "import sys, os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torch_intermediate_layer_getter import IntermediateLayerGetter as MidGetter\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Downloading Kay dataset\n",
    "import requests\n",
    "\n",
    "fnames = [\"kay_labels.npy\", \"kay_labels_val.npy\", \"kay_images.npz\"]\n",
    "\n",
    "urls =['https://osf.io/r638s/download',\n",
    "       'https://osf.io/yqb3e/download',\n",
    "       'https://osf.io/ymnjv/download']\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fnames[i], 'wb') as fh:\n",
    "    fh.write(r.content)\n",
    "\n",
    "\n",
    "with np.load(fnames[2]) as dobj:\n",
    "  dat = dict(**dobj)\n",
    "labels = np.load('kay_labels.npy')\n",
    "val_labels = np.load('kay_labels_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Visualizing example images\n",
    "f, axs = plt.subplots(2, 4, figsize=(12, 6), sharex=True, sharey=True)\n",
    "for ax, im in zip(axs.flat, dat[\"stimuli\"]):\n",
    "  ax.imshow(im, cmap=\"gray\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Dataset Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "`dat` has the following fields:  \n",
    "- `stimuli`: stim x i x j array of grayscale stimulus images\n",
    "- `stimuli_test`: stim x i x j array of grayscale stimulus images in the test set  \n",
    "- `responses`: stim x voxel array of z-scored BOLD response amplitude\n",
    "- `responses_test`:  stim x voxel array of z-scored BOLD response amplitude in the test set  \n",
    "- `roi`: array of voxel labels\n",
    "- `roi_names`: array of names corresponding to voxel labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Converting stimulus to RGB and changing the scale to 0-255 (Specific to Kay dataset images)\n",
    "stimuli_tr = dat[\"stimuli\"]\n",
    "stimuli_ts = dat[\"stimuli_test\"]\n",
    "stimuli_tr_xformed = np.zeros((1750, 3, 128, 128))\n",
    "stimuli_ts_xformed = np.zeros((120, 3, 128, 128))\n",
    "for i in range(1750):\n",
    "  img = stimuli_tr[i, :, :]\n",
    "  img = ((img - np.min(img))*255/(np.max(img) - np.min(img))).astype(int)\n",
    "  stimuli_tr_xformed[i, :, :, :] = [img,img,img]\n",
    "\n",
    "for i in range(120):\n",
    "  img = stimuli_ts[i, :, :]\n",
    "  img = ((img - np.min(img))*255/(np.max(img) - np.min(img))).astype(int)\n",
    "  stimuli_ts_xformed[i, :, :, :] = [img, img, img]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Fine Tuning AlexNet on voxel activations\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Setting up training and test data for LOC region\n",
    "loc_id = np.where(dat['roi'] == 7)\n",
    "response_tr = np.squeeze(dat[\"responses\"][:, loc_id])\n",
    "response_ts = np.squeeze(dat[\"responses_test\"][:, loc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Custom dataloader for loading images in numpy array\n",
    "class MyDataset(Dataset):\n",
    "  def __init__(self, data, targets, transform=None):\n",
    "    self.data = data\n",
    "    self.targets = torch.LongTensor(targets)\n",
    "    self.transform = transform\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    x = self.data[index]\n",
    "    y = self.targets[index]\n",
    "\n",
    "    if self.transform:\n",
    "        x = Image.fromarray(self.data[index].astype(np.uint8).transpose(1, 2, 0))\n",
    "        x = self.transform(x)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "\n",
    "transform = {\n",
    "    'train': transforms.Compose([\n",
    "                                 transforms.RandomResizedCrop(224),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                      [0.229, 0.224, 0.225])\n",
    "                                 ]),\n",
    "    'val': transforms.Compose([\n",
    "                               transforms.Resize(256),\n",
    "                               transforms.CenterCrop(224),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                    [0.229, 0.224, 0.225])\n",
    "                               ]),\n",
    "             }\n",
    "\n",
    "dataset = {}\n",
    "dataset['train'] = MyDataset(list(stimuli_tr_xformed),\n",
    "                             list(response_tr), transform=transform['train'])\n",
    "dataset['val'] = MyDataset(list(stimuli_ts_xformed),\n",
    "                           list(response_ts), transform=transform['val'])\n",
    "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size=50) for x in ['train', 'val']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "net = models.alexnet(pretrained=True)\n",
    "num_ftrs = net.classifier[6].in_features\n",
    "net.classifier[6] = nn.Linear(num_ftrs, np.shape(response_ts)[1])\n",
    "\n",
    "net.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.1  # Change this\n",
    "num_epochs =  5  # Change this\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "best_model_wts = copy.deepcopy(net.state_dict())\n",
    "best_loss = 10.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\tprint(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
    "\tprint('-' * 20)\n",
    "\n",
    "\t# Each epoch has a training and validation phase\n",
    "\tfor phase in ['train', 'val']:\n",
    "\t\tif phase == 'train':\n",
    "\t\t\tnet.train()  # Set model to training mode\n",
    "\t\telse:\n",
    "\t\t\tnet.eval()   # Set model to evaluate mode\n",
    "\n",
    "\t\trunning_loss = 0.0\n",
    "\t\trunning_corrects = 0\n",
    "\n",
    "\t\t# Iterate over data.\n",
    "\t\tfor inputs, labels in dataloaders[phase]:\n",
    "\t\t\tinputs = inputs.to(device)\n",
    "\t\t\tlabels = labels.to(device)\n",
    "\n",
    "\t\t\t# zero the parameter gradients\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# forward\n",
    "\t\t\t# track history if only in train\n",
    "\t\t\twith torch.set_grad_enabled(phase == 'train'):\n",
    "\t\t\t\toutputs = net(inputs)\n",
    "\t\t\t\tloss = criterion(outputs.float(), labels.float())\n",
    "\n",
    "\t\t\t\t# backward + optimize only if in training phase\n",
    "\t\t\t\tif phase == 'train':\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# statistics\n",
    "\t\t\trunning_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "\t\tepoch_loss = running_loss / dataset_sizes[phase]\n",
    "\t\tprint(f\"{phase} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "\t\t# deep copy the model\n",
    "\t\tif phase == 'val' and epoch_loss < best_loss:\n",
    "\t\t\tbest_loss = epoch_loss\n",
    "\t\t\tbest_model_wts = copy.deepcopy(net.state_dict())\n",
    "\n",
    "\tprint()\n",
    "\n",
    "# load best model weights\n",
    "net.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "## Extract features of all the intermediate layers from ImageNet-trained and finetuned Alexnet\n",
    "return_layers = {\n",
    "    'features.2': 'conv1',\n",
    "    'features.5': 'conv2',\n",
    "    'features.7': 'conv3',\n",
    "    'features.9': 'conv4',\n",
    "    'features.12': 'conv5',\n",
    "    'classifier.1': 'fc1',\n",
    "    'classifier.4': 'fc2',\n",
    "    'classifier.6': 'fc3',\n",
    "    }\n",
    "\n",
    "# Loading AlexNet pretrained on Imagenet\n",
    "net_im = models.alexnet(pretrained=True)\n",
    "net_im.eval()\n",
    "net_im.to(device)\n",
    "\n",
    "\n",
    "# Setting up feature extraction step\n",
    "midfeat_ft = MidGetter(net, return_layers=return_layers, keep_output=True)\n",
    "midfeat_im = MidGetter(net_im, return_layers=return_layers, keep_output=True)\n",
    "\n",
    "# Loading validation data and forward pass through the network\n",
    "dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size=120) for x in ['val']}\n",
    "for inputs, labels in dataloaders['val']:\n",
    "  inputs = inputs.to(device)\n",
    "  mid_outputs_ft, _ = midfeat_ft(inputs)\n",
    "  mid_outputs_im, _ = midfeat_im(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Dissimilarity - Correlation\n",
    "# Loading V1 and LOC responses\n",
    "v1_id = np.where(dat['roi'] == 1)\n",
    "loc_id = np.where(dat['roi'] == 7)\n",
    "Rts_v1 = np.squeeze(dat[\"responses_test\"][:, v1_id])\n",
    "Rts_lo = np.squeeze(dat[\"responses_test\"][:, loc_id])\n",
    "\n",
    "# Observed dissimilarity  - Correlation\n",
    "fMRI_dist_metric_ft = \"euclidean\"  # ['correlation', 'euclidean']\n",
    "fMRI_dist_metric_im = \"correlation\"  # ['correlation', 'euclidean']\n",
    "\n",
    "Alexnet_ft_dist_metric = \"euclidean\"  # ['correlation', 'euclidean']\n",
    "Alexnet_im_dist_metric = \"correlation\"  # ['correlation', 'euclidean']\n",
    "\n",
    "dobs_v1_ft = pdist(Rts_v1, fMRI_dist_metric_ft)\n",
    "dobs_lo_ft = pdist(Rts_lo, fMRI_dist_metric_ft)\n",
    "dobs_v1_im = pdist(Rts_v1, fMRI_dist_metric_im)\n",
    "dobs_lo_im = pdist(Rts_lo, fMRI_dist_metric_im)\n",
    "\n",
    "# Comparing representation of V1 and LOC across different layers of Alexnet\n",
    "r, p = np.zeros((4, 8)), np.zeros((4, 8))\n",
    "for i,l in enumerate(mid_outputs_ft.keys()):\n",
    "  dnet_ft = pdist(torch.flatten(mid_outputs_ft[l], 1, -1).cpu().detach().numpy(),\n",
    "                  Alexnet_ft_dist_metric)\n",
    "  dnet_im = pdist(torch.flatten(mid_outputs_im[l], 1, -1).cpu().detach().numpy(),\n",
    "                  Alexnet_im_dist_metric)\n",
    "  r[0, i], p[0, i] = pearsonr(dnet_ft, dobs_v1_ft)\n",
    "  r[1, i], p[1, i] = pearsonr(dnet_im, dobs_v1_im)\n",
    "  r[2, i], p[2, i] = pearsonr(dnet_ft, dobs_lo_ft)\n",
    "  r[3, i], p[3, i] = pearsonr(dnet_im, dobs_lo_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Plotting correlation between observed and predicted dissimilarity values\n",
    "plt.bar(range(8), r[0, :], alpha=0.5)\n",
    "plt.bar(range(8), r[1, :], alpha=0.5)\n",
    "plt.legend(['Fine Tuned', 'Imagenet'])\n",
    "plt.ylabel('Correlation coefficient')\n",
    "plt.title('Match to V1')\n",
    "plt.xticks(range(8), mid_outputs_ft.keys())\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(8), r[2, :], alpha=0.5)\n",
    "plt.bar(range(8), r[3, :], alpha=0.5)\n",
    "plt.legend(['Fine Tuned', 'Imagenet'])\n",
    "plt.ylabel('Correlation coefficient')\n",
    "plt.title('Match to LOC')\n",
    "plt.xticks(range(8), mid_outputs_ft.keys())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "finetuning_fmri",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
