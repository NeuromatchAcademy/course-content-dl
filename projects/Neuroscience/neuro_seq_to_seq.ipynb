{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuroSeqToSeq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/fix_links/projects/Neuroscience/neuro_seq_to_seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2syTWGsew0gG"
      },
      "source": [
        "# Focus on what matters: inferring low-dimensional dynamics from neural recordings\n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Marius Pachitariu, Pedram Mouseli, Lucas Tavares, Jonny Coutinho, \n",
        "Blessing Itoro, Gaurang Mahajan, Rishika Mohanta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7mSFxx1SmsF"
      },
      "source": [
        "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
        "\n",
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGfolBnf0gb8"
      },
      "source": [
        "---\n",
        "# Objective: \n",
        "It is very difficult to interpret the activity of single neurons in the brain, because their firing patterns are noisy, and it is not clear how a single neuron can contribute to cognition and behavior. However, neurons in the brain participate in local, regional and brainwide dynamics. No neuron is isolated from these dynamics, and much of a single neuron's activity can be predicted from the dynamics. Furthermore, only populations of neurons as a whole can control cognition and behavior. Hence it is crucial to identify these dynamical patterns and relate them to stimuli or behaviors. \n",
        "\n",
        "In this notebook, we generate simulated data from a low-dimensional dynamical system and then use seq-to-seq methods to predict one subset of neurons from another. This allows us to identify the low-dimensional dynamics that are sufficient to explain the activity of neurons in the simulation. The methods described in this notebook can be applied to large-scale neural recordings of hundreds to tens of thousans of neurons, such as the ones from the NMA-CN course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEqdz1ZUMaj1"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTprPFLmT2RM"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raBVOEWgUK_B",
        "cellView": "form"
      },
      "source": [
        "# @title Figure settings\n",
        "from matplotlib import rcParams\n",
        "\n",
        "rcParams['figure.figsize'] = [20, 4]\n",
        "rcParams['font.size'] =15\n",
        "rcParams['axes.spines.top'] = False\n",
        "rcParams['axes.spines.right'] = False\n",
        "rcParams['figure.autolayout'] = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlmxFKlYT-mP"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBOU5c91UH9O"
      },
      "source": [
        "**Note:** If `cuda` is not enabled, go to `Runtime`--> `Change runtime type` and in `Hardware acceleration` choose `GPU`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_AruQWv4616"
      },
      "source": [
        "---\n",
        "# Simulate some data from a linear dynamical system"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXnBFw5ovdve"
      },
      "source": [
        "# set the seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# 100 trials is typical of neural data\n",
        "ntrials = 100\n",
        "\n",
        "# we simulate 200 neurons\n",
        "NN = 400 \n",
        "\n",
        "# we will pretend like every \"bin\" is 10ms, so the trial length is 2500ms\n",
        "NT = 250\n",
        "\n",
        "# let's use 10 latent components\n",
        "ncomp = 10\n",
        "\n",
        "# this is the recurrent dynamics matrix, which we made diagonal for simplicity\n",
        "# values have to be smaller than 1 for stability\n",
        "A0 =  np.diag(.8 + .2 * np.random.rand(ncomp,))\n",
        "\n",
        "# this is the projection matrix from components to neurons\n",
        "C0 = .025 * np.random.randn(ncomp, NN)\n",
        "\n",
        "# We generate the dynamics of the low-d system. We initialize the latent state. \n",
        "\n",
        "# start by initializing the latents\n",
        "y       = 2 * np.random.randn(ncomp)\n",
        "latents = np.zeros((NT, ntrials, ncomp))\n",
        "# we run the dynamics forward and add noise (or \"innovations\") at each timestep\n",
        "for t in range(NT):\n",
        "  y = y @ A0 +  np.random.randn(ntrials, ncomp) \n",
        "  latents[t] = y\n",
        "\n",
        "# we now project the latents to the neuron space and threshold to generate firing rates\n",
        "rates = np.maximum(0, latents @ C0)\n",
        "\n",
        "# now we draw poisson counts to simulate the number of spikes a neuron fires randomly\n",
        "x = np.random.poisson(rates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCFK-OKU7ZUr"
      },
      "source": [
        "---\n",
        "#  Define RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz5XNTZlDcCW"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, ncomp, NN1, NN2, bidi=True):\n",
        "    super(Net, self).__init__()\n",
        "\n",
        "    # play with some of the options in the RNN! \n",
        "    self.rnn = nn.RNN(NN1, ncomp, num_layers = 1, dropout = 0,\n",
        "                      bidirectional = bidi, nonlinearity = 'tanh')        \n",
        "    self.fc = nn.Linear(ncomp, NN2)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    y = self.rnn(x)[0]\n",
        "\n",
        "    if self.rnn.bidirectional:\n",
        "      # if the rnn is bidirectional, it concatenates the activations from the forward and backward pass\n",
        "      # we want to add them instead, so as to enforce the latents to match between the forward and backward pass\n",
        "      q = (y[:, :, :ncomp] + y[:, :, ncomp:])/2\n",
        "    else:\n",
        "      q = y\n",
        "\n",
        "    # the softplus function is just like a relu but it's smoothed out so we can't predict 0\n",
        "    # if we predict 0 and there was a spike, that's an instant Inf in the Poisson log-likelihood which leads to failure\n",
        "    z = F.softplus(self.fc(q), 10)\n",
        "    \n",
        "    return z, q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um9G1Glj8gpx"
      },
      "source": [
        "---\n",
        "# Plot and Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z8SAM-R8tW7"
      },
      "source": [
        "plt.figure(figsize = (10, 6))\n",
        "plt.imshow(x[:, 0, :].T, cmap='gray_r', vmax = 3, vmin=0, aspect='auto')\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Cell #')\n",
        "plt.colorbar(orientation='vertical', label='# Spikes in 0.01 s time bin')\n",
        "plt.title('Example trial')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUt7iHg3vd0j"
      },
      "source": [
        "# we separate the neuron data into two populations: the input and output\n",
        "x0 = torch.from_numpy(x[:, :, :200]).to(device).float()\n",
        "x1 = torch.from_numpy(x[:, :, 200:]).to(device).float()\n",
        "\n",
        "NN1 = x1.shape[-1]\n",
        "NN2 = x0.shape[-1]\n",
        "\n",
        "# we initialize the neural network\n",
        "net = Net(ncomp, NN1, NN2, bidi = True).to(device)\n",
        "\n",
        "# special thing:  we initialize the biases of the last layer in the neural network\n",
        "# we set them as the mean firing rates of the neurons. \n",
        "# this should make the initial predictions close to the mean, because the latents don't contribute much\n",
        "net.fc.bias.data[:] = x1.mean((0,1))\n",
        "\n",
        "# we set up the optimizer. Adjust the learning rate if the training is slow or if it explodes. \n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J7E4QotA8_4"
      },
      "source": [
        "---\n",
        "# Train the RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS4Cz5MMvUZm"
      },
      "source": [
        "# you can keep re-running this cell if you think the cost might decrease further\n",
        "\n",
        "# we define the Poisson log-likelihood loss\n",
        "def Poisson_loss(lam, spk):\n",
        "  return lam - spk * torch.log(lam)\n",
        "\n",
        "niter = 1000\n",
        "for k in range(niter): \n",
        "    # the network outputs the single-neuron prediction and the latents\n",
        "    z, y = net(x1)    \n",
        "\n",
        "    # our log-likelihood cost\n",
        "    cost = Poisson_loss(z, x0).mean()\n",
        "\n",
        "    # train the network as usual\n",
        "    cost.backward()    \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if k % 100 == 0:\n",
        "        print(f'iteration {k}, cost {cost.item():.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv3tOlnLAgU5"
      },
      "source": [
        "---\n",
        "# Compare true firing rates with the predicted.\n",
        "\n",
        "Note that for real data we only get the spikes, not the true firing rates! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "si86K0ipvxwl"
      },
      "source": [
        "rpred = z.detach().cpu().numpy()\n",
        "\n",
        "nn = 29\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(rates[:,nn, 0])\n",
        "plt.plot(rpred[:,nn, 0])\n",
        "plt.plot(-.5 + x[:, nn, 0]/4)\n",
        "\n",
        "plt.legend(['rates (true)', 'rates (predicted)', 'spikes'])\n",
        "plt.title(f'Neuron {nn}')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfZJrpojIPtk"
      },
      "source": [
        "## View firing rates for all neurons in one trial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4-kb1jqIPtl"
      },
      "source": [
        "plt.figure(figsize = (12, 8))\n",
        "plt.subplot(121)\n",
        "plt.imshow(rates[:, 12, :200].T, cmap='gray_r')\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Cell #')\n",
        "plt.title('True rates (trial 12)')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.imshow(rpred[:, 12, :].T, cmap='gray_r')\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Cell #')\n",
        "plt.title('Inferred rates (trial 12)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1yMhpgsKt0n"
      },
      "source": [
        "---\n",
        "# Visualize the latent dynamics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQxB4r6GKS4v"
      },
      "source": [
        "ycpu = y.detach().cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(20, 4))\n",
        "plt.subplot(121)\n",
        "plt.plot(ycpu[:, 0, :]);\n",
        "plt.title('All latents on trial 0')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(ycpu[:, :, 0]);\n",
        "plt.title('All trials for latent 0')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdjBF7KtKxSm"
      },
      "source": [
        "Not much to see for the latents. This is not surprising, since we generated them with random data, so they look just like a random walk process. \n",
        "\n",
        "Now apply this model on real data and see if you can interpret the latents. \n",
        "\n"
      ]
    }
  ]
}