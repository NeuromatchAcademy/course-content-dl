{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ReinforcementLearning/human_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Using RL to Model Cognitive Tasks\n",
    "\n",
    "**By Neurmatch Academy**\n",
    "\n",
    "__Content creators:__ Morteza Ansarinia, Yamil Vidal\n",
    "\n",
    "__Production editor:__ Spiros Chavlis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Objective\n",
    "\n",
    "- This project aims to use behavioral data to train an agent and then use the agent to investigate data produced by human subjects. Having a computational agent that mimics humans in such tests, we will be able to compare its mechanics with human data.\n",
    "\n",
    "- In another conception, we could fit an agent that learns many cognitive tasks that require abstract-level constructs such as executive functions. This is a multi-task control problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies \n",
    "!pip install dm-env --quiet\n",
    "!pip install dm-sonnet --quiet\n",
    "!pip install dm-acme==0.2.0 dm-acme[tf]==0.2.0 dm-acme[reverb]==0.2.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sonnet as snt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dm_env\n",
    "\n",
    "import acme\n",
    "from acme import specs\n",
    "from acme import wrappers\n",
    "from acme.tf import networks\n",
    "from acme.testing import fakes\n",
    "from acme import EnvironmentLoop\n",
    "from acme.agents.tf.dqn import DQN\n",
    "from acme.agents.tf.d4pg import D4PG\n",
    "from acme.agents.tf.ddpg import DDPG\n",
    "from acme.agents.tf.r2d2 import R2D2\n",
    "from acme.tf import utils as tf2_utils\n",
    "from acme.utils.loggers import TerminalLogger\n",
    "from acme.agents.tf.dmpo import DistributionalMPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "from IPython.display import clear_output\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title `InMemoryLogger` that keeps the data in memory\n",
    "\n",
    "class InMemoryLogger(acme.utils.loggers.Logger):\n",
    "  \"\"\"A simple logger that keeps all data in memory.\n",
    "\n",
    "  Reference:\n",
    "    https://github.com/deepmind/acme/blob/master/acme/utils/loggers/dataframe.py\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self._data = []\n",
    "\n",
    "  def write(self, data: acme.utils.loggers.LoggingData):\n",
    "    self._data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Background\n",
    "\n",
    "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).\n",
    "\n",
    "- Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).\n",
    "\n",
    "- Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.\n",
    "\n",
    "- One behavioral test example would be the N-back task.\n",
    "\n",
    "  - In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.\n",
    "\n",
    "  - The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a `match`.\n",
    "\n",
    "\n",
    "- Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Datasets\n",
    "\n",
    "- HCP WM task ([NMA-CN HCP notebooks](https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI))\n",
    "\n",
    "Any dataset that used cognitive tests would work.\n",
    "Question: limit to behavioral data vs fMRI?\n",
    "Question: Which stimuli and actions to use?\n",
    "classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)\n",
    "The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## N-back task\n",
    "\n",
    "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
    "\n",
    "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Cognitive Tests Environment\n",
    "\n",
    "First we develop an environment in that agents perform a cognitive test, here the N-back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Human dataset\n",
    "\n",
    "We need a dataset of human perfoming a N-back test, with the following features:\n",
    "\n",
    "- `participant_id`: following the BIDS format, it contains a unique identifier for each participant.\n",
    "- `trial_index`: same as `time_step`.\n",
    "- `stimulus`: same as `observation`.\n",
    "- `response`: same as `action`, recorded response by the human subject.\n",
    "- `expected_response`: correct response.\n",
    "- `is_correct`: same as `reward`, whether the human subject responded correctly.\n",
    "- `response_time`: won't be used here.\n",
    "\n",
    "Here we generate a mock dataset with those features, but remember to **replace this with real human data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
      "<string>:6: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>trial_index</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>response</th>\n",
       "      <th>response_time</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>non-match</td>\n",
       "      <td>0.503214</td>\n",
       "      <td>non-match</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>4</td>\n",
       "      <td>B</td>\n",
       "      <td>match</td>\n",
       "      <td>0.075696</td>\n",
       "      <td>non-match</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>match</td>\n",
       "      <td>0.742086</td>\n",
       "      <td>non-match</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>sub-9</td>\n",
       "      <td>28</td>\n",
       "      <td>D</td>\n",
       "      <td>match</td>\n",
       "      <td>0.319890</td>\n",
       "      <td>non-match</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>sub-9</td>\n",
       "      <td>29</td>\n",
       "      <td>E</td>\n",
       "      <td>match</td>\n",
       "      <td>2.348173</td>\n",
       "      <td>non-match</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>sub-9</td>\n",
       "      <td>30</td>\n",
       "      <td>B</td>\n",
       "      <td>non-match</td>\n",
       "      <td>0.092343</td>\n",
       "      <td>non-match</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>sub-9</td>\n",
       "      <td>31</td>\n",
       "      <td>B</td>\n",
       "      <td>match</td>\n",
       "      <td>1.013402</td>\n",
       "      <td>non-match</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>sub-9</td>\n",
       "      <td>32</td>\n",
       "      <td>C</td>\n",
       "      <td>match</td>\n",
       "      <td>1.631318</td>\n",
       "      <td>non-match</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participant_id  trial_index  ... expected_response is_correct\n",
       "0            sub-1            1  ...              None       True\n",
       "1            sub-1            2  ...              None       True\n",
       "2            sub-1            3  ...         non-match       True\n",
       "3            sub-1            4  ...         non-match      False\n",
       "4            sub-1            5  ...         non-match      False\n",
       "..             ...          ...  ...               ...        ...\n",
       "315          sub-9           28  ...         non-match      False\n",
       "316          sub-9           29  ...         non-match      False\n",
       "317          sub-9           30  ...         non-match       True\n",
       "318          sub-9           31  ...         non-match      False\n",
       "319          sub-9           32  ...         non-match      False\n",
       "\n",
       "[320 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_mock_nback_dataset(N=2,\n",
    "                                n_participants=10,\n",
    "                                n_trials=32,\n",
    "                                stimulus_choices=list('ABCDEF'),\n",
    "                                response_choices=['match', 'non-match']):\n",
    "  \"\"\"Generate a mock dataset for the N-back task.\"\"\"\n",
    "\n",
    "  n_rows = n_participants * n_trials\n",
    "\n",
    "  participant_ids = sorted([f'sub-{pid}' for pid in range(1,n_participants+1)] * n_trials)\n",
    "  trial_indices = list(range(1,n_trials+1)) * n_participants\n",
    "  stimulus_sequence = np.random.choice(stimulus_choices, n_rows)\n",
    "\n",
    "  responses = np.random.choice(response_choices, n_rows)\n",
    "  response_times = np.random.exponential(size=n_rows)\n",
    "\n",
    "  df = pd.DataFrame({\n",
    "      'participant_id': participant_ids,\n",
    "      'trial_index': trial_indices,\n",
    "      'stimulus': stimulus_sequence,\n",
    "      'response': responses,\n",
    "      'response_time': response_times\n",
    "  })\n",
    "\n",
    "  # mark matchig stimuli\n",
    "  _nback_stim = df['stimulus'].shift(N)\n",
    "  df['expected_response'] = (df['stimulus'] == _nback_stim).map({True: 'match', False: 'non-match'})\n",
    "\n",
    "  df['is_correct'] = (df['response'] == df['expected_response'])\n",
    "\n",
    "  # we don't care about burn-in trials (trial < N)\n",
    "  df.loc[df['trial_index'] <= N, 'is_correct'] = True\n",
    "  df.loc[df['trial_index'] <= N, ['response','response_time','expected_response']] = None\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "# ========\n",
    "# now generate the actual data with the provided function and plot some of its features\n",
    "mock_nback_data = generate_mock_nback_dataset()\n",
    "\n",
    "sns.displot(data=mock_nback_data, x='response_time')\n",
    "plt.suptitle('response time distribution of the mock N-back dataset', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "sns.displot(data=mock_nback_data, x='is_correct')\n",
    "plt.suptitle('Accuracy distribution of the mock N-back dataset', y=1.06)\n",
    "plt.show()\n",
    "\n",
    "mock_nback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Implementation scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Environment\n",
    "\n",
    "The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It is capable of performing two kinds of simulation:\n",
    "- rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
    "- receives human data (or mock data if you prefer), and returns what participants performed as the observation. This is more useful for preference-based RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class NBack(dm_env.Environment):\n",
    "\n",
    "  ACTIONS = ['match', 'non-match']\n",
    "\n",
    "  def __init__(self,\n",
    "               N=2,\n",
    "               episode_steps=32,\n",
    "               stimuli_choices=list('ABCDEF'),\n",
    "               human_data=None,\n",
    "               seed=1,\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      N\n",
    "      episode_steps\n",
    "      stimuli_choices\n",
    "      human_data\n",
    "      seed\n",
    "\n",
    "    \"\"\"\n",
    "    self.N = N\n",
    "    self.episode_steps = episode_steps\n",
    "    self.stimuli_choices = stimuli_choices\n",
    "    self.stimuli = np.empty(shape=episode_steps)  # will be filled in the `reset()`\n",
    "\n",
    "    self._reset_next_step = True\n",
    "\n",
    "    # whether mimic humans or reward the agent once it responds optimally.\n",
    "    if human_data is None:\n",
    "      self._imitate_human = False\n",
    "      self.human_data = None\n",
    "      self.human_subject_data = None\n",
    "    else:\n",
    "      self._imitate_human = True\n",
    "      self.human_data = human_data\n",
    "      self.human_subject_data = None\n",
    "\n",
    "    self._action_history = []\n",
    "\n",
    "  def reset(self):\n",
    "    self._reset_next_step = False\n",
    "    self._current_step = 0\n",
    "    self._action_history.clear()\n",
    "\n",
    "    # generate a random sequence instead of relying on human data\n",
    "    if self.human_data is None:\n",
    "      # self.stimuli = np.random.choice(self.stimuli_choices, self.episode_steps)\n",
    "      # FIXME This is a fix for acme & reverb issue with string observation. Agent should be able to handle strings\n",
    "      self.stimuli = np.random.choice(len(self.stimuli_choices), self.episode_steps).astype(np.float32)\n",
    "    else:\n",
    "      # randomly choose a subject from the human data and follow her trials and responses.\n",
    "      self.human_subject_data = self.human_data.query('participant_id == participant_id.sample().iloc[0]',\n",
    "                                                engine='python').sort_values('trial_index')\n",
    "      self.stimuli = self.human_subject_data['stimulus'].values\n",
    "      # FIXME should we always use one specific human subject or randomly select one in each episode?\n",
    "\n",
    "    return dm_env.restart(self._observation())\n",
    "\n",
    "\n",
    "  def _episode_return(self):\n",
    "    if self._imitate_human:\n",
    "      return np.mean(self.human_subject_data['response'] == self._action_history)\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "  def step(self, action: int):\n",
    "    if self._reset_next_step:\n",
    "      return self.reset()\n",
    "\n",
    "    if self._imitate_human:\n",
    "      # if it was the same action as the human subject, then reward the agent\n",
    "      human_action = self.human_subject_data['response'].iloc[self._current_step]\n",
    "      agent_action = NBack.ACTIONS[action]\n",
    "      step_reward = (agent_action == human_action)\n",
    "    else:\n",
    "      # assume the agent is rationale and doesn't want to reproduce human, reward once the response it correct\n",
    "      expected_action = 'match' if (self.stimuli[self._current_step] == self.stimuli[self._current_step - self.N]) else 'non-match'\n",
    "      agent_action = NBack.ACTIONS[action]\n",
    "      step_reward = 1. if (agent_action == expected_action) else -1.\n",
    "\n",
    "    self._action_history.append(agent_action)\n",
    "\n",
    "    self._current_step += 1\n",
    "\n",
    "    # Check for termination.\n",
    "    if self._current_step == self.stimuli.shape[0]:\n",
    "      self._reset_next_step = True\n",
    "      # we are using the mean of total time step rewards as the episode return\n",
    "      return dm_env.termination(reward=self._episode_return(),\n",
    "                                observation=self._observation())\n",
    "    else:\n",
    "      return dm_env.transition(reward=step_reward,\n",
    "                               observation=self._observation())\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return dm_env.specs.BoundedArray(\n",
    "        shape=self.stimuli.shape,\n",
    "        dtype=self.stimuli.dtype,\n",
    "        name='nback_stimuli', minimum=0, maximum=1)\n",
    "\n",
    "  def action_spec(self):\n",
    "    return dm_env.specs.DiscreteArray(\n",
    "        dtype=int,\n",
    "        num_values=len(NBack.ACTIONS),\n",
    "        name='action')\n",
    "\n",
    "  def _observation(self):\n",
    "\n",
    "    # agents observe only the current stimulus\n",
    "    obs = self.stimuli[self._current_step - 1]\n",
    "\n",
    "    # TODO uncomment to observe all the previrous stimuli instead of only the current stimulus\n",
    "    # obs = self.stimuli[:self.current_step]\n",
    "    # obs = ''.join(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  def plot_state(self):\n",
    "    \"\"\"Display current state of the environment.\n",
    "\n",
    "     Note: `M` mean `match`, and `.` is a `non-match`.\n",
    "    \"\"\"\n",
    "    from IPython.display import HTML\n",
    "    stimuli = self.stimuli[:self._current_step - 1]\n",
    "    actions = ['M' if a=='match' else '.' for a in self._action_history[:self._current_step - 1]]\n",
    "    return HTML(\n",
    "        f'<b>Environment ({self.N}-back):</b><br />'\n",
    "        f'<pre><b>Stimuli:</b> {\"\".join(map(str,map(int,stimuli)))}</pre>'\n",
    "        f'<pre><b>Actions:</b> {\"\".join(actions)}</pre>'\n",
    "    )\n",
    "\n",
    "  @staticmethod\n",
    "  def create_environment():\n",
    "    \"\"\"Utility function to create a N-back environment and its spec.\"\"\"\n",
    "\n",
    "    # Make sure the environment outputs single-precision floats.\n",
    "    environment = wrappers.SinglePrecisionWrapper(NBack())\n",
    "\n",
    "    # Grab the spec of the environment.\n",
    "    environment_spec = specs.make_environment_spec(environment)\n",
    "\n",
    "    return environment, environment_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Define a random agent\n",
    "\n",
    "For more information you can refer to NMA-DL W3D2 Basic Reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class RandomAgent(acme.Actor):\n",
    "\n",
    "  def __init__(self, environment_spec):\n",
    "    \"\"\"Gets the number of available actions from the environment spec.\"\"\"\n",
    "    self._num_actions = environment_spec.actions.num_values\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    \"\"\"Selects an action uniformly at random.\"\"\"\n",
    "    action = np.random.randint(self._num_actions)\n",
    "    return action\n",
    "\n",
    "  def observe_first(self, timestep):\n",
    "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def update(self):\n",
    "    \"\"\"Does not update as the RandomAgent does not learn from data.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Initialize the environment and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions:\n",
      " DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=1, num_values=2)\n",
      "observations:\n",
      " BoundedArray(shape=(32,), dtype=dtype('float32'), name='nback_stimuli', minimum=0.0, maximum=1.0)\n",
      "rewards:\n",
      " Array(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "environment, environment_spec = NBack.create_environment()\n",
    "agent = RandomAgent(environment_spec)\n",
    "\n",
    "print('actions:\\n', environment_spec.actions)\n",
    "print('observations:\\n', environment_spec.observations)\n",
    "print('rewards:\\n', environment_spec.rewards)\n",
    "\n",
    "# DEBUG\n",
    "# timestep = environment.step(NBack.ACTIONS[0])  # pytype: dm_env.TimeStep\n",
    "# timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Run the loop\n",
    "\n",
    "For more details, see NMA-DL W3D2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All episode returns: [-5.0, 1.0, -9.0, -1.0, -9.0, -5.0, -13.0, -1.0, -1.0, -7.0, -5.0, -7.0, -5.0, -7.0, 3.0, 3.0, -1.0, -5.0, 1.0, 3.0, -1.0, 9.0, -1.0, 3.0, -3.0, 1.0, 9.0, -1.0, -7.0, 5.0, 1.0, 13.0, 1.0, -7.0, 1.0, 3.0, -5.0, 3.0, -3.0, -7.0, -13.0, 1.0, 3.0, 1.0, 7.0, 1.0, 9.0, 7.0, 5.0, 7.0, 1.0, 9.0, 1.0, 5.0, -1.0, -3.0, -1.0, 3.0, 1.0, -1.0, -3.0, 9.0, 11.0, 7.0, -7.0, -3.0, 1.0, 13.0, 9.0, 11.0, -1.0, 3.0, 1.0, -9.0, 7.0, -3.0, -7.0, -5.0, 5.0, 7.0, 1.0, -5.0, -9.0, 9.0, -5.0, 3.0, -1.0, -7.0, 7.0, -11.0, -7.0, 7.0, -7.0, -1.0, 5.0, 7.0, -9.0, 1.0, -3.0, -1.0, 3.0, -3.0, 7.0, -3.0, 5.0, 5.0, 1.0, -7.0, -3.0, 9.0, 5.0, -1.0, 1.0, 3.0, 7.0, -1.0, -1.0, 13.0, -5.0, -7.0, 1.0, -11.0, -5.0, 9.0, -7.0, 1.0, 1.0, -7.0, -5.0, -1.0, 9.0, 3.0, -7.0, 7.0, 1.0, -1.0, -1.0, 9.0, -5.0, 1.0, -7.0, 3.0, 7.0, -5.0, 3.0, 5.0, -5.0, -3.0, 1.0, -13.0, -1.0, -1.0, 1.0, -3.0, -5.0, 1.0, 3.0, 7.0, 3.0, 7.0, 9.0, -3.0, 11.0, 9.0, -9.0, 11.0, -3.0, -1.0, -5.0, -5.0, -3.0, -1.0, 5.0, -3.0, 3.0, -1.0, 5.0, -7.0, -3.0, 5.0, 9.0, -5.0, 5.0, 3.0, -3.0, -3.0, 7.0, -11.0, 3.0, 3.0, 7.0, -1.0, 1.0, 1.0, -1.0, -1.0, 5.0, 1.0, -5.0, -1.0, 1.0, 1.0, 3.0, 3.0, 1.0, -3.0, -3.0, -3.0, -1.0, 3.0, -11.0, 9.0, 7.0, 7.0, -1.0, 1.0, 9.0, -3.0, -9.0, 1.0, 7.0, 3.0, 3.0, 1.0, 7.0, 3.0, -3.0, -1.0, 5.0, 1.0, 9.0, -9.0, -5.0, -3.0, -9.0, 11.0, -3.0, -1.0, 9.0, -11.0, 5.0, 1.0, -1.0, -9.0, -3.0, 5.0, -3.0, 7.0, -3.0, -3.0, -5.0, 3.0, 3.0, 7.0, -1.0, -7.0, -1.0, 3.0, 1.0, 3.0, -3.0, 11.0, 1.0, -9.0, 7.0, -7.0, 1.0, 3.0, 5.0, 1.0, 1.0, -3.0, -1.0, -3.0, -11.0, 7.0, 3.0, 1.0, -3.0, 3.0, 7.0, 5.0, 1.0, -9.0, -3.0, -1.0, 3.0, -7.0, 5.0, -11.0, 3.0, -3.0, -5.0, -9.0, 9.0, 1.0, -3.0, -3.0, 1.0, 3.0, 3.0, 9.0, -3.0, 5.0, 11.0, -5.0, 1.0, -7.0, -5.0, 1.0, 5.0, 5.0, -1.0, -3.0, -3.0, -1.0, 17.0, -11.0, 7.0, -1.0, 7.0, -1.0, -3.0, -3.0, 3.0, 7.0, -1.0, -9.0, -3.0, -5.0, 5.0, -1.0, 3.0, -5.0, -1.0, 3.0, -3.0, 1.0, 1.0, 9.0, -1.0, -1.0, 1.0, -11.0, -3.0, 3.0, 5.0, 7.0, -7.0, -7.0, 1.0, -3.0, 7.0, 5.0, -11.0, -3.0, -3.0, 1.0, -1.0, -3.0, 1.0, 3.0, -3.0, 5.0, -3.0, -1.0, 7.0, -1.0, 1.0, -7.0, 1.0, 5.0, -3.0, -3.0, 1.0, -1.0, -7.0, 1.0, -3.0, 5.0, 1.0, -7.0, 7.0, 3.0, 11.0, -3.0, -11.0, 5.0, 5.0, -1.0, 1.0, -1.0, 1.0, 3.0, -7.0, 3.0, 5.0, -1.0, 5.0, -11.0, -5.0, -7.0, -1.0, 3.0, -7.0, -3.0, 3.0, -9.0, -1.0, 1.0, 7.0, 1.0, 7.0, -3.0, 1.0, -9.0, -11.0, -13.0, -5.0, 3.0, -9.0, 7.0, 5.0, 3.0, 5.0, -5.0, 1.0, -9.0, -9.0, -9.0, 11.0, 1.0, 5.0, -3.0, 3.0, 3.0, 5.0, -3.0, 13.0, -1.0, 1.0, -5.0, -1.0, 5.0, -9.0, -9.0, 5.0, -1.0, 11.0, -3.0, -1.0, -3.0, 1.0, 5.0, 5.0, -7.0, 7.0, -9.0, 1.0, 7.0, 5.0, 3.0, 1.0, 3.0, 7.0, -3.0, 1.0, 3.0, -1.0, -9.0, -1.0, 9.0, 5.0, -3.0, -5.0, -7.0, -9.0, 3.0, 5.0, -1.0, 9.0, -9.0, -11.0, -3.0, -3.0, 1.0, 5.0, 1.0, -5.0, -3.0, 15.0, -1.0, 1.0, -3.0, 1.0, 7.0, 7.0, -9.0, -1.0, -3.0, -3.0, -1.0, 1.0, 13.0, 9.0, -5.0, 5.0, -5.0, -1.0, -3.0, 1.0, 5.0, 11.0, 1.0, 1.0, -3.0, -3.0, -1.0, -9.0, -5.0, 1.0, -9.0, -3.0, -11.0, -1.0, -5.0, -3.0, 5.0, -1.0, -11.0, 3.0, 5.0, -3.0, -3.0, 11.0, 3.0, -7.0, 9.0, -1.0, -5.0, 1.0, 5.0, -5.0, 7.0, 1.0, 1.0, 11.0, -7.0, 11.0, 5.0, 3.0, 1.0, -3.0, 7.0, -1.0, -7.0, 3.0, 1.0, 3.0, -9.0, 3.0, 15.0, 3.0, 3.0, -21.0, -5.0, 1.0, -9.0, 15.0, -9.0, -1.0, -1.0, 3.0, -3.0, -9.0, 3.0, 1.0, 1.0, -9.0, -3.0, -1.0, -7.0, -1.0, 5.0, 1.0, 7.0, 3.0, 5.0, -5.0, 3.0, -5.0, -3.0, 5.0, -5.0, 3.0, 3.0, 5.0, 3.0, 3.0, 1.0, -9.0, 9.0, 7.0, -7.0, -5.0, -7.0, -13.0, 7.0, -9.0, -1.0, 7.0, 1.0, 3.0, -5.0, 3.0, -5.0, 1.0, -1.0, -5.0, 1.0, 3.0, -1.0, 7.0, 7.0, 1.0, 3.0, -5.0, -3.0, -3.0, -11.0, -1.0, 11.0, -3.0, 7.0, -11.0, 5.0, -1.0, -7.0, 5.0, 11.0, 11.0, -9.0, 1.0, 1.0, -1.0, 1.0, 1.0, -3.0, 1.0, -5.0, -5.0, 1.0, -3.0, -3.0, -1.0, 3.0, -1.0, 3.0, -3.0, 5.0, -3.0, -3.0, 7.0, 1.0, 5.0, -11.0, 1.0, -3.0, 5.0, 9.0, -5.0, -1.0, 3.0, -7.0, 1.0, -9.0, -7.0, 1.0, 3.0, 1.0, -1.0, -5.0, -1.0, -9.0, -1.0, -1.0, -1.0, -5.0, -5.0, -3.0, -5.0, -5.0, 9.0, 1.0, -7.0, -5.0, -5.0, 5.0, -3.0, 13.0, -1.0, -7.0, -5.0, 3.0, 1.0, 11.0, 1.0, -11.0, -5.0, -3.0, -1.0, 3.0, -9.0, 3.0, -3.0, -7.0, -3.0, 11.0, -1.0, 7.0, -3.0, 1.0, -9.0, -9.0, 7.0, -9.0, 1.0, 1.0, 1.0, -5.0, -7.0, -5.0, -3.0, -3.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 3.0, -7.0, 3.0, 9.0, 9.0, 3.0, 7.0, 3.0, 1.0, 11.0, 1.0, 3.0, 5.0, 1.0, -7.0, 1.0, 5.0, 3.0, 15.0, -13.0, -5.0, -5.0, -3.0, -3.0, -3.0, -7.0, 9.0, -7.0, 13.0, -5.0, 7.0, -11.0, -5.0, 1.0, -5.0, -1.0, 5.0, 7.0, 1.0, 7.0, 1.0, -7.0, -7.0, -5.0, 5.0, 5.0, 5.0, -1.0, -1.0, -9.0, 3.0, -3.0, -5.0, 3.0, -3.0, 1.0, -5.0, -3.0, -13.0, 7.0, 3.0, -3.0, -1.0, 9.0, -7.0, 5.0, -3.0, -1.0, 5.0, -9.0, 1.0, 3.0, 3.0, 5.0, -11.0, -9.0, -3.0, -5.0, -1.0, -1.0, 5.0, -7.0, 5.0, 7.0, -3.0, -11.0, 3.0, -7.0, -1.0, 11.0, -3.0, 7.0, -5.0, -7.0, 3.0, 5.0, -5.0, 3.0, 3.0, -1.0, -1.0, -7.0, 9.0, 7.0, 1.0, 3.0, -3.0, -1.0, 1.0, 5.0, -3.0, 5.0, -3.0, -7.0, 1.0, -3.0, -1.0, 3.0, 1.0, 1.0, 5.0, -7.0, -1.0, 7.0, -3.0, -1.0, 5.0, 7.0, 3.0, -1.0, -3.0, 5.0, -9.0, 5.0, -5.0, -1.0, 5.0, 1.0, -3.0, -1.0, -1.0, 5.0, -1.0, 3.0, -1.0, -9.0, 1.0, 5.0, 1.0, 3.0, -11.0, 7.0, 7.0, 1.0, -7.0, -1.0, -1.0, -1.0, 3.0, 5.0, -1.0, 7.0, 5.0, 5.0, -5.0, 15.0, -7.0, 5.0, -1.0, 5.0, 3.0, -7.0, -3.0, 9.0, -3.0, -1.0, 1.0, -9.0, 5.0, -3.0, 1.0, -5.0, -5.0, 7.0, 5.0, 3.0, 1.0, 3.0, -9.0, -3.0, 7.0, -7.0, -1.0, 1.0, 1.0, 1.0, -1.0, -13.0, 3.0, 7.0, 3.0, -7.0, -3.0, 1.0, 7.0, 1.0, -7.0, 7.0, -3.0, 7.0, 3.0, -1.0, -5.0, 3.0, 7.0, -3.0, 9.0, -9.0, -5.0, 7.0, 7.0, -3.0, 5.0, 1.0, 3.0, 9.0, 1.0, 3.0, -5.0, -1.0, 5.0, 5.0, -5.0, 3.0, -5.0, -5.0, -3.0, 5.0, 1.0, 1.0, 1.0, -1.0, 1.0, -11.0, 7.0, 1.0, -1.0, 1.0, 1.0, 11.0, 1.0, 3.0, 3.0, 15.0, 3.0, 1.0, -5.0, -5.0, 5.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "# fitting parameters\n",
    "n_episodes = 1_000\n",
    "n_total_steps = 0\n",
    "log_loss = False\n",
    "n_steps = n_episodes * 32\n",
    "all_returns = []\n",
    "\n",
    "\n",
    "# main loop\n",
    "for episode in range(n_episodes):\n",
    "  episode_steps = 0\n",
    "  episode_return = 0\n",
    "  episode_loss = 0\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  timestep = environment.reset()\n",
    "\n",
    "  # Make the first observation.\n",
    "  agent.observe_first(timestep)\n",
    "\n",
    "  # Run an episode\n",
    "  while not timestep.last():\n",
    "\n",
    "    # DEBUG\n",
    "    # print(timestep)\n",
    "\n",
    "    # Generate an action from the agent's policy and step the environment.\n",
    "    action = agent.select_action(timestep.observation)\n",
    "    timestep = environment.step(action)\n",
    "\n",
    "    # Have the agent observe the timestep and let the agent update itself.\n",
    "    agent.observe(action, next_timestep=timestep)\n",
    "    agent.update()\n",
    "\n",
    "    # Book-keeping.\n",
    "    episode_steps += 1\n",
    "    n_total_steps += 1\n",
    "    episode_return += timestep.reward\n",
    "\n",
    "    if log_loss:\n",
    "      episode_loss += agent.last_loss\n",
    "\n",
    "    if n_steps is not None and n_total_steps >= n_steps:\n",
    "      break\n",
    "\n",
    "  # Collect the results and combine with counts.\n",
    "  steps_per_second = episode_steps / (time.time() - start_time)\n",
    "  result = {\n",
    "      'episode': episode,\n",
    "      'episode_length': episode_steps,\n",
    "      'episode_return': episode_return,\n",
    "  }\n",
    "  if log_loss:\n",
    "    result['loss_avg'] = episode_loss/episode_steps\n",
    "\n",
    "  all_returns.append(episode_return)\n",
    "\n",
    "  from IPython.display import display\n",
    "  display(environment.plot_state())\n",
    "  # Log the given results.\n",
    "  print(result)\n",
    "\n",
    "  if n_steps is not None and n_total_steps >= n_steps:\n",
    "    break\n",
    "\n",
    "clear_output()\n",
    "print('\\n', 'All episode returns:', all_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note:** You can simplify the environment loop using [DeepMind Acme](https://github.com/deepmind/acme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def make_networks_d4pg(action_spec,\n",
    "                       policy_layer_sizes=(256, 256, 256),\n",
    "                       critic_layer_sizes=(512, 512, 256),\n",
    "                       vmin=-150.,\n",
    "                       vmax=150.,\n",
    "                       num_atoms=51,\n",
    "                      ):\n",
    "  \"\"\"Networks for D4PG agent.\"\"\"\n",
    "  action_size = np.prod(action_spec.shape, dtype=int)\n",
    "\n",
    "  policy_network = snt.Sequential([\n",
    "      tf2_utils.batch_concat,\n",
    "      networks.LayerNormMLP(layer_sizes=policy_layer_sizes + (action_size,)),\n",
    "      networks.TanhToSpec(spec=action_spec)\n",
    "      ])\n",
    "  critic_network = snt.Sequential([\n",
    "      networks.CriticMultiplexer(\n",
    "          action_network=networks.ClipToSpec(action_spec),\n",
    "          critic_network=networks.LayerNormMLP(\n",
    "              layer_sizes=critic_layer_sizes,\n",
    "              activate_final=True),\n",
    "      ),\n",
    "      networks.DiscreteValuedHead(vmin=vmin,\n",
    "                                  vmax=vmax,\n",
    "                                  num_atoms=num_atoms)\n",
    "      ])\n",
    "\n",
    "  return policy_network, critic_network\n",
    "\n",
    "\n",
    "def make_networks_dqn(action_spec):\n",
    "  network = snt.Sequential([\n",
    "      snt.Flatten(),\n",
    "      snt.nets.MLP([50, 50, action_spec.num_values]),\n",
    "  ])\n",
    "  return network\n",
    "\n",
    "\n",
    "policy_optimizer = snt.optimizers.Adam(1e-4)\n",
    "critic_optimizer = snt.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# init the N-back environment\n",
    "env, env_spec = NBack.create_environment()\n",
    "\n",
    "# DEBUG fake testing environment.\n",
    "# Uncomment this to debug your agent without using the N-back environment.\n",
    "# env = fakes.DiscreteEnvironment(\n",
    "#     num_actions=2,\n",
    "#     num_observations=1000,\n",
    "#     obs_dtype=np.float32,\n",
    "#     episode_length=32)\n",
    "# env_spec = specs.make_environment_spec(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# DQN agent\n",
    "# agent = DQN(\n",
    "#     environment_spec=env_spec,\n",
    "#     network=make_networks_dqn(env_spec.actions))\n",
    "\n",
    "# D4PG agent\n",
    "# policy_network, critic_network = make_networks_d4pg(env_spec.actions)\n",
    "# agent = D4PG(environment_spec=env_spec,\n",
    "#              policy_network=policy_network,\n",
    "#              critic_network=critic_network,\n",
    "#              observation_network=tf2_utils.batch_concat, # Identity Op.\n",
    "#              policy_optimizer=policy_optimizer,\n",
    "#              critic_optimizer=critic_optimizer,\n",
    "#              logger=InMemoryLogger())\n",
    "\n",
    "# random agent\n",
    "agent = RandomAgent(env_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we run the environment loop with the initiated agent and print the training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_length</th>\n",
       "      <th>episode_return</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>episodes</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12216.048785</td>\n",
       "      <td>996</td>\n",
       "      <td>31872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12512.140207</td>\n",
       "      <td>997</td>\n",
       "      <td>31904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11690.421392</td>\n",
       "      <td>998</td>\n",
       "      <td>31936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>32</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>12981.693394</td>\n",
       "      <td>999</td>\n",
       "      <td>31968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13170.221568</td>\n",
       "      <td>1000</td>\n",
       "      <td>32000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_length episode_return  steps_per_second  episodes  steps\n",
       "995              32            3.0      12216.048785       996  31872\n",
       "996              32            1.0      12512.140207       997  31904\n",
       "997              32            3.0      11690.421392       998  31936\n",
       "998              32           -5.0      12981.693394       999  31968\n",
       "999              32            3.0      13170.221568      1000  32000"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loop\n",
    "loop = EnvironmentLoop(env, agent, logger=InMemoryLogger())\n",
    "loop.run(n_episodes)\n",
    "\n",
    "# print logs\n",
    "logs = pd.DataFrame(loop._logger._data)\n",
    "logs.tail()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "human_rl",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
