{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_alignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J3wRwkevo-Td",
        "s6iB4QvQihW-",
        "yWtbZfaGwoMI"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ComputerVision/image_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-lGmmumP4Ps"
      },
      "source": [
        "# Image Alignment \n",
        "\n",
        "**By Neuromatch Academy**\n",
        "\n",
        "__Content creators:__ Kaleb Vinehout\n",
        "\n",
        "__Production editor:__ Spiros Chavlis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm7IZcjQaplT"
      },
      "source": [
        "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
        "\n",
        "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qneC4knzasps"
      },
      "source": [
        "---\n",
        "# Objective\n",
        "\n",
        "This notebook will give you starting points to perform Spatial Transformers. These can be used for registraion of images. This is useful when comparing multiple datasets together. Check out https://arxiv.org/abs/1506.02025 for more details.These can also be used to plug into any CNN architecture to deal with dataset rotations and scale invariance in a given dataset\n",
        "\n",
        "* Spatial transformers contain three main parts.The first is a localizaion net the second is grid generator and the last is a sampler. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRbhFKtE4EPw"
      },
      "source": [
        "---\n",
        "# Intro to Image Alignment\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCruSKr5a0nz"
      },
      "source": [
        "## Image Alignment Applications\n",
        "* To answer many biological questions, it is necessary to align sets of images together\n",
        "* Use Spatial Transfomers as a preprocessing step for any CNN achitecutre. This could be done before facial recognition in order to crop and align images before spatial recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0XYTvYfa2nr"
      },
      "source": [
        "## Acknowledgments:\n",
        "This Notebook was developed by Kaleb Vinehout. It borrows from material by Ghassen Hamrouni, Asror Wali, and Erwin Russel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIt7jEa1a5xj"
      },
      "source": [
        "---\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB9RkKWOxoWr",
        "cellView": "form"
      },
      "source": [
        "# @title Install dependencies\n",
        "!pip install scikit-image --quiet\n",
        "!pip install Pillow --quiet\n",
        "!pip install n2v --quiet\n",
        "!pip install csbdeep --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryUYf8VVxoWs"
      },
      "source": [
        "# Imports\n",
        "import glob\n",
        "import time\n",
        "import sklearn.decomposition\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from PIL import Image\n",
        "from skimage.util import random_noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "o8zVNQRZbBNP"
      },
      "source": [
        "# @title Figure settings\n",
        "%matplotlib inline\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Supu5f98eNSm"
      },
      "source": [
        "# @title Set device (GPU or CPU). Execute `set_device()`\n",
        "# especially if torch modules used.\n",
        "\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"GPU is not enabled in this notebook. \\n\"\n",
        "          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook. \\n\"\n",
        "          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n",
        "          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n",
        "\n",
        "  return device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG__DJJueeEU"
      },
      "source": [
        "device = set_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njuu_wm4418Q"
      },
      "source": [
        "---\n",
        "# Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PeApWp_DMVr"
      },
      "source": [
        "## Loader for classic MNIST as an example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "f1wwmsW7jEpf"
      },
      "source": [
        "# @title Download MNIST dataset\n",
        "import tarfile, requests, os\n",
        "\n",
        "fname = 'MNIST.tar.gz'\n",
        "name = 'MNIST'\n",
        "url = 'https://osf.io/y2fj6/download'\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  print('\\nDownloading MNIST dataset...')\n",
        "  r = requests.get(url, allow_redirects=True)\n",
        "  with open(fname, 'wb') as fh:\n",
        "    fh.write(r.content)\n",
        "  print('\\nDownloading MNIST completed.')\n",
        "\n",
        "if not os.path.exists(name):\n",
        "  with tarfile.open(fname) as tar:\n",
        "    tar.extractall()\n",
        "    os.remove(fname)\n",
        "else:\n",
        "  print('MNIST dataset is dowloaded.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPwAeOW3QJva"
      },
      "source": [
        "# Training dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(root='.', train=True, download=False,\n",
        "                   transform=transforms.Compose([\n",
        "                                                 transforms.ToTensor(),\n",
        "                                                 transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                                 ])),\n",
        "                                           batch_size=64,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "# Test dataset\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(root='.', train=False, download=False,\n",
        "                   transform=transforms.Compose([\n",
        "                                                 transforms.ToTensor(),\n",
        "                                                 transforms.Normalize((0.1307,), (0.3081,))\n",
        "                                                 ])),\n",
        "                                          batch_size=64,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nrUo_1mLVuX"
      },
      "source": [
        "Define functions to convert between Tensor and numpy image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuGiSXdxE1Tk"
      },
      "source": [
        "def convert_image_np(inp):\n",
        "  \"\"\"Convert a Tensor to numpy image.\"\"\"\n",
        "  inp = inp.numpy().transpose((1, 2, 0))\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std = np.array([0.229, 0.224, 0.225])\n",
        "  inp = std * inp + mean\n",
        "  inp = np.clip(inp, 0, 1)\n",
        "  return inp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CL68OcNLdYk"
      },
      "source": [
        "def convert2tensor(self, args):\n",
        "  data = np.asarray([e[0] for e in self.binary_train_dataset])\n",
        "  target = np.asarray([e[1] for e in self.binary_train_dataset])\n",
        "\n",
        "  tensor_data = torch.from_numpy(data)\n",
        "  tensor_data = tensor_data.float()\n",
        "  tensor_target = torch.from_numpy(target)\n",
        "\n",
        "  train = data_utils.TensorDataset(tensor_data, tensor_target)\n",
        "  train_loader = data_utils.DataLoader(train, batch_size=args.batch_size, shuffle = True)\n",
        "  return train_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eybxxajO7Jon"
      },
      "source": [
        "Plot the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYwfYfY86lr-"
      },
      "source": [
        "## Display Images\n",
        "# Get a batch of training data\n",
        "data = next(iter(test_loader))[0].to(device)\n",
        "input_tensor = data.cpu()\n",
        "in_grid = convert_image_np(torchvision.utils.make_grid(input_tensor))\n",
        "\n",
        "# Plot the images\n",
        "plt.figure()\n",
        "plt.imshow(in_grid)\n",
        "plt.show()\n",
        "\n",
        "# plot ONE image\n",
        "plt.figure()\n",
        "plt.imshow(torchvision.utils.make_grid(input_tensor).numpy().transpose((1, 2, 0)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxf_VXl3IJLY"
      },
      "source": [
        "---\n",
        "# Spatial Transformer on images\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU62VmRLCESL"
      },
      "source": [
        "## Spatial Transformer Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjDKv-D2yngg"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    # Spatial transformer localization-network\n",
        "    self.localization = nn.Sequential(\n",
        "        nn.Conv2d(1, 8, kernel_size=7),\n",
        "        nn.MaxPool2d(2, stride=2),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(8, 10, kernel_size=5),\n",
        "        nn.MaxPool2d(2, stride=2),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    # Regressor for the 3 * 2 affine matrix\n",
        "    self.fc_loc = nn.Sequential(\n",
        "        nn.Linear(10 * 3 * 3, 32),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(32, 3 * 2)\n",
        "    )\n",
        "\n",
        "    # Initialize the weights/bias with identity transformation\n",
        "    self.fc_loc[2].weight.data.zero_()\n",
        "    self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "  # Spatial transformer network forward function\n",
        "  def stn(self, x):\n",
        "    xs = self.localization(x)\n",
        "    xs = xs.view(-1, 10 * 3 * 3)\n",
        "    theta = self.fc_loc(xs)\n",
        "    theta = theta.view(-1, 2, 3)\n",
        "\n",
        "    grid = F.affine_grid(theta, x.size())\n",
        "    x = F.grid_sample(x, grid)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def forward(self, x):\n",
        "    # transform the input\n",
        "    x = self.stn(x)\n",
        "\n",
        "    # Perform the usual forward pass\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "    x = x.view(-1, 320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training=self.training)\n",
        "    x = self.fc2(x)\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dirqPGzxI9-5"
      },
      "source": [
        "## Train and Test functions for the STN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8h2sjQ0drVb"
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TthxVWEBKZVL"
      },
      "source": [
        "### Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0ghWBVzKTcj"
      },
      "source": [
        "def train(train_loader, optimizer, epoch, device):\n",
        "\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % 500 == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15Ol7p5PKbtS"
      },
      "source": [
        "### Test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_c2aRdNKcZ_"
      },
      "source": [
        "def test(test_loader, device):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "\n",
        "      # sum up batch loss\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      # get the index of the max log-probability\n",
        "      pred = output.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
        "          .format(test_loss, correct, len(test_loader.dataset),\n",
        "                  100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRESvRB0OnJM"
      },
      "source": [
        "### Run Train and test the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d1y20VNRES1"
      },
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  train(train_loader, optimizer, epoch, device)\n",
        "  test(test_loader, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmQJFDAKKoS2"
      },
      "source": [
        "### Visualize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vSqCyZ9Kqbu"
      },
      "source": [
        "def visualize_stn():\n",
        "  with torch.no_grad():\n",
        "    # Get a batch of training data\n",
        "    data = next(iter(test_loader))[0].to(device)\n",
        "\n",
        "    input_tensor = data.cpu()\n",
        "    transformed_input_tensor = model.stn(data).cpu()\n",
        "\n",
        "    in_grid = convert_image_np(\n",
        "        torchvision.utils.make_grid(input_tensor))\n",
        "\n",
        "    out_grid = convert_image_np(\n",
        "        torchvision.utils.make_grid(transformed_input_tensor))\n",
        "\n",
        "    # Plot the results side-by-side\n",
        "    f, axarr = plt.subplots(1, 2)\n",
        "    axarr[0].imshow(in_grid)\n",
        "    axarr[0].set_title('Dataset Images')\n",
        "\n",
        "    axarr[1].imshow(out_grid)\n",
        "    axarr[1].set_title('Transformed Images')\n",
        "  return in_grid, out_grid\n",
        "\n",
        "\n",
        "# Visualize the STN transformation on some input batch\n",
        "[in_grid, out_grid] = visualize_stn()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3wRwkevo-Td"
      },
      "source": [
        "---\n",
        "# Use Sørensen–Dice coefficient (DSC) to calculate the simularity of images --> modify for your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwaJF3vRlcab"
      },
      "source": [
        "## Function to compare similarity of images \n",
        "\n",
        "Compare the similarity of two images with the the Sørensen–Dice coefficient. See details [here](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_mktg95la7S"
      },
      "source": [
        "def calc_dice(im1, im2):\n",
        "  \"\"\"\n",
        "  This calculates the DICE between two images. The maximum DICE is 1, the minimum is Zero.\n",
        "  Args:\n",
        "  -\tim1, im2: one of the imges to calcualte DICE coeffeicnet. Note image1.shape has to equal image2.shape\n",
        "  Returns:\n",
        "  -\tdice: the dice coeffeicent\n",
        "  \"\"\"\n",
        "  im1 = np.asarray(im1).astype(np.bool)\n",
        "  im2 = np.asarray(im2).astype(np.bool)\n",
        "\n",
        "  if im1.shape != im2.shape:\n",
        "    raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
        "\n",
        "  # Compute Dice coefficient\n",
        "  intersection = np.logical_and(im1, im2)\n",
        "\n",
        "  dice = 2. * intersection.sum() / (im1.sum() + im2.sum())\n",
        "\n",
        "  return dice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYTCVKEVmSHo"
      },
      "source": [
        "## Run Dice on individual images set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk4oZxr9mTTS"
      },
      "source": [
        "# what do you need to do to calculate dice for your images?\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST(root='.', train=True, download=False,\n",
        "                   transform=transforms.Compose([transforms.ToTensor()])),\n",
        "                   batch_size=64,\n",
        "                   shuffle=True,\n",
        "                   num_workers=2)\n",
        "\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "idx1, idx2 = 1, 25\n",
        "img1, img2 = images[1], images[20]\n",
        "\n",
        "plt.figure()\n",
        "plt.subplot(121)\n",
        "plt.imshow(img1.squeeze())\n",
        "plt.axis('off')\n",
        "plt.title(f'Label: {labels[1]}')\n",
        "plt.subplot(122)\n",
        "plt.imshow(img2.squeeze())\n",
        "plt.axis('off')\n",
        "plt.title(f'Label: {labels[20]}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "dice = calc_dice(img1, img2)\n",
        "print(\"The Dice is {}\".format(dice))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww6KOtymiX51"
      },
      "source": [
        "---\n",
        "# Optional preprocessing steps --> modify for your data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcAO5hc1FwSC"
      },
      "source": [
        "## Add salt/pepper noise to the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuX8lY-IWrXA"
      },
      "source": [
        "### Add noise class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhDDYuZsF31R"
      },
      "source": [
        "# are there other types of noise you can add? What effect do different types of noise have? EX: gasusain?\n",
        "\n",
        "def salt_pepper_noise(trainloader):\n",
        "  for data in trainloader:\n",
        "    img, _ = data[0], data[1]\n",
        "    s_and_p = torch.tensor(random_noise(img, mode='s&p',\n",
        "                                        salt_vs_pepper=0.5,\n",
        "                                        clip=True))\n",
        "  return s_and_p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI7WXuWMWuVr"
      },
      "source": [
        "add noise to both train and test datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFsE9S_ydnIQ"
      },
      "source": [
        "plot clean and noisy data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6iB4QvQihW-"
      },
      "source": [
        "## Remove noise with noise2void"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYsj-xiJwufZ"
      },
      "source": [
        "Check out: https://aswali.github.io/WNet/ and this paper:https://arxiv.org/abs/1711.08506"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkO7v-O8l3td",
        "cellView": "form"
      },
      "source": [
        "# @title Import `noise2void` dependances\n",
        "from n2v.models import N2VConfig, N2V\n",
        "from csbdeep.utils import plot_history\n",
        "from n2v.utils.n2v_utils import manipulate_val_data\n",
        "from n2v.internals.N2V_DataGenerator import N2V_DataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHVOE4k9imZw",
        "cellView": "form"
      },
      "source": [
        "# @title Make denoising function\n",
        "def noise2void(data, model_name, patch_size=64):\n",
        "  \"\"\"\n",
        "  Removes noise in 3d image using the noise 2 void method. Based on https://arxiv.org/abs/1811.10980 w/ this implementation: https://github.com/juglab/n2v\n",
        "      Args:\n",
        "      -\tdata: Numpy array  2d to be deionised\n",
        "      -   model: name of model to load, if provided this model is used to denoise instead of model made from data otherwise this is name given to model (ex:#model_name = 'n2v_3D_blk')\n",
        "      -patch_size: this is the size of patches in X and Y, default is 64\n",
        "      Returns:\n",
        "      -\tdata_denoise: Numpy array noise removed\n",
        "  \"\"\"\n",
        "  # We create our DataGenerator-object.\n",
        "  datagen = N2V_DataGenerator()\n",
        "  # In the 'dims' parameter we specify the order of dimensions in the image files we are reading.\n",
        "  if data.ndim == 2:\n",
        "    print('2D image found to denosie')\n",
        "    dataZYX = data\n",
        "    data_exp = np.expand_dims(dataZYX, axis=(0, 1, 4))  # expand dimensions One at the front is used to hold a potential stack of images such as a movie, One at the end could hold color channels such as RGB. #expand dimensions One at the front is used to hold a potential stack of images such as a movie, One at the end could hold color channels such as RGB.\n",
        "    patch_shape = (patch_size, patch_size)\n",
        "    model_axis = 'YX'\n",
        "  print('arrary with extra dimensions is size of {}'.format(data_exp.shape))\n",
        "  print('patches are {}'.format(patch_shape))\n",
        "  # the base directory in which our model will live\n",
        "  basedir = 'models'\n",
        "  path = basedir + '/' + model_name\n",
        "  if not os.path.exists(path):\n",
        "    print(path)\n",
        "    # create model\n",
        "    patches = datagen.generate_patches_from_list(data_exp, shape=patch_shape)\n",
        "    print('patches shape {}'.format(patches.shape))\n",
        "    # Patches are created so they do not overlap.\n",
        "    # (Note: this is not the case if you specify a number of patches. See the docstring for details!)\n",
        "    # Non-overlapping patches enable us to split them into a training and validation set.\n",
        "    # modify split so set as a %\n",
        "    perc_95 = int(patches.shape[0] * 0.95)\n",
        "    X = patches[:perc_95]  # this is 600/640\n",
        "    X_val = patches[perc_95:]  # this is 40/640\n",
        "\n",
        "    # train model\n",
        "    # You can increase \"train_steps_per_epoch\" to get even better results at the price of longer computation.\n",
        "    fast = 128  # default\n",
        "    slow = 50  # to get better results?  --> apply same model to Z plane\n",
        "    speed = fast\n",
        "    config = N2VConfig(X, unet_kern_size=3, train_steps_per_epoch=int(X.shape[0] / speed), train_epochs=20,\n",
        "                        train_loss='mse', batch_norm=True, train_batch_size=4, n2v_perc_pix=0.198,\n",
        "                        n2v_patch_shape=patch_shape, n2v_manipulator='uniform_withCP', n2v_neighborhood_radius=5)\n",
        "\n",
        "    # Let's look at the parameters stored in the config-object.\n",
        "    vars(config)\n",
        "    # We are now creating our network model.\n",
        "    model = N2V(config=config, name=model_name, basedir=basedir)\n",
        "    history = model.train(X, X_val)\n",
        "    print(sorted(list(history.history.keys())))\n",
        "    model.export_TF(name='Noise2Void - data',\n",
        "                    description='This is the 3D Noise2Void for  data.',\n",
        "                    authors=[\"Kaleb Vinehout\"],\n",
        "                    test_img=X_val[0, ..., 0], axes=model_axis,\n",
        "                    patch_shape=patch_shape)\n",
        "  # run prediction model on rest of data in 3D image\n",
        "  # A previously trained model is loaded by creating a new N2V-object without providing a 'config'.\n",
        "  model = N2V(config=None, name=model_name, basedir=basedir)\n",
        "  # Here we process the data.\n",
        "  # The 'n_tiles' parameter can be used if images are too big for the GPU memory.\n",
        "  # If we do not provide the 'n_tiles' parameter the system will automatically try to find an appropriate tiling.\n",
        "  data_denoise = model.predict(dataZYX, axes=model_axis)  # , n_tiles=(2, 4, 4))\n",
        "\n",
        "  return data_denoise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n2t2YU4krPH"
      },
      "source": [
        "# Runun the denoise on our data\n",
        "data_denoise = noise2void(data, model_name='test_model', patch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0YVGt3qi4nh"
      },
      "source": [
        "Make figures of raw data and denoised data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTS7f6E0jLWX"
      },
      "source": [
        "# Make figures of raw data and denoised data\n",
        "plt.figure()\n",
        "plt.imshow(data, cmap='magma', vmin=np.percentile(data, 0.1), vmax=np.percentile(data, 99.9))\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(data_denoise, cmap='magma', vmin=np.percentile(data_denoise, 0.1), vmax=np.percentile(data_denoise, 99.9))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWtbZfaGwoMI"
      },
      "source": [
        "## Segment image with W-net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVY2DiEbwrXP",
        "cellView": "form"
      },
      "source": [
        "# @title Wnet class\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, in_filters, out_filters, seperable=True):\n",
        "    super(Block, self).__init__()\n",
        "    \n",
        "    if seperable:\n",
        "      self.spatial1=nn.Conv2d(in_filters, in_filters, kernel_size=3, groups=in_filters, padding=1)\n",
        "      self.depth1=nn.Conv2d(in_filters, out_filters, kernel_size=1)\n",
        "      self.conv1=lambda x: self.depth1(self.spatial1(x))\n",
        "      self.spatial2=nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1, groups=out_filters)\n",
        "      self.depth2=nn.Conv2d(out_filters, out_filters, kernel_size=1)\n",
        "      self.conv2=lambda x: self.depth2(self.spatial2(x))\n",
        "\n",
        "    else:\n",
        "      self.conv1=nn.Conv2d(in_filters, out_filters, kernel_size=3, padding=1)\n",
        "      self.conv2=nn.Conv2d(out_filters, out_filters, kernel_size=3, padding=1)\n",
        "    \n",
        "    \n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.dropout1 = nn.Dropout(0.65) \n",
        "    self.batchnorm1=nn.BatchNorm2d(out_filters)\n",
        "\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.dropout2 = nn.Dropout(0.65) \n",
        "    self.batchnorm2=nn.BatchNorm2d(out_filters)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.batchnorm1(self.conv1(x)).clamp(0)\n",
        "    x = self.relu1(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.batchnorm2(self.conv2(x)).clamp(0)\n",
        "    x = self.relu2(x)\n",
        "    x = self.dropout2(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "class UEnc(nn.Module):\n",
        "  def __init__(self, squeeze, ch_mul=64, in_chans=3):\n",
        "    super(UEnc, self).__init__()\n",
        "\n",
        "    self.enc1=Block(in_chans, ch_mul, seperable=False)\n",
        "    self.enc2=Block(ch_mul, 2*ch_mul)\n",
        "    self.enc3=Block(2*ch_mul, 4*ch_mul)\n",
        "    self.enc4=Block(4*ch_mul, 8*ch_mul)\n",
        "\n",
        "    self.middle=Block(8*ch_mul, 16*ch_mul)\n",
        "\n",
        "    self.up1=nn.ConvTranspose2d(16*ch_mul, 8*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec1=Block(16*ch_mul, 8*ch_mul)\n",
        "    self.up2=nn.ConvTranspose2d(8*ch_mul, 4*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec2=Block(8*ch_mul, 4*ch_mul)\n",
        "    self.up3=nn.ConvTranspose2d(4*ch_mul, 2*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec3=Block(4*ch_mul, 2*ch_mul)\n",
        "    self.up4=nn.ConvTranspose2d(2*ch_mul, ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec4=Block(2*ch_mul, ch_mul, seperable=False)\n",
        "\n",
        "    self.final=nn.Conv2d(ch_mul, squeeze, kernel_size=(1, 1))\n",
        "    self.softmax = nn.Softmax2d()\n",
        "      \n",
        "  def forward(self, x):\n",
        "\n",
        "    enc1=self.enc1(x)\n",
        "\n",
        "    enc2=self.enc2(F.max_pool2d(enc1, (2, 2)))\n",
        "\n",
        "    enc3=self.enc3(F.max_pool2d(enc2, (2,2)))\n",
        "\n",
        "    enc4=self.enc4(F.max_pool2d(enc3, (2,2)))\n",
        "\n",
        "    middle=self.middle(F.max_pool2d(enc4, (2,2)))\n",
        "\n",
        "    up1=torch.cat([enc4, self.up1(middle)], 1)\n",
        "    dec1=self.dec1(up1)\n",
        "\n",
        "    up2=torch.cat([enc3, self.up2(dec1)], 1)\n",
        "    dec2=self.dec2(up2)\n",
        "\n",
        "    up3=torch.cat([enc2, self.up3(dec2)], 1)\n",
        "    dec3=self.dec3(up3)\n",
        "\n",
        "    up4=torch.cat([enc1, self.up4(dec3)], 1)\n",
        "    dec4=self.dec4(up4)\n",
        "\n",
        "    final=self.final(dec4)\n",
        "\n",
        "    return final\n",
        "\n",
        "\n",
        "class UDec(nn.Module):\n",
        "  def __init__(self, squeeze, ch_mul=64, in_chans=3):\n",
        "    super(UDec, self).__init__()\n",
        "\n",
        "    self.enc1=Block(squeeze, ch_mul, seperable=False)\n",
        "    self.enc2=Block(ch_mul, 2*ch_mul)\n",
        "    self.enc3=Block(2*ch_mul, 4*ch_mul)\n",
        "    self.enc4=Block(4*ch_mul, 8*ch_mul)\n",
        "\n",
        "    self.middle=Block(8*ch_mul, 16*ch_mul)\n",
        "\n",
        "    self.up1=nn.ConvTranspose2d(16*ch_mul, 8*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec1=Block(16*ch_mul, 8*ch_mul)\n",
        "    self.up2=nn.ConvTranspose2d(8*ch_mul, 4*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec2=Block(8*ch_mul, 4*ch_mul)\n",
        "    self.up3=nn.ConvTranspose2d(4*ch_mul, 2*ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec3=Block(4*ch_mul, 2*ch_mul)\n",
        "    self.up4=nn.ConvTranspose2d(2*ch_mul, ch_mul, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "    self.dec4=Block(2*ch_mul, ch_mul, seperable=False)\n",
        "    \n",
        "    self.final=nn.Conv2d(ch_mul, in_chans, kernel_size=(1, 1))\n",
        "      \n",
        "  def forward(self, x):\n",
        "    enc1 = self.enc1(x)\n",
        "\n",
        "    enc2 = self.enc2(F.max_pool2d(enc1, (2, 2)))\n",
        "\n",
        "    enc3 = self.enc3(F.max_pool2d(enc2, (2,2)))\n",
        "\n",
        "    enc4 = self.enc4(F.max_pool2d(enc3, (2,2)))\n",
        "\n",
        "    middle = self.middle(F.max_pool2d(enc4, (2,2)))\n",
        "\n",
        "    up1 = torch.cat([enc4, self.up1(middle)], 1)\n",
        "    dec1 = self.dec1(up1)\n",
        "\n",
        "    up2 = torch.cat([enc3, self.up2(dec1)], 1)\n",
        "    dec2 = self.dec2(up2)\n",
        "\n",
        "    up3 = torch.cat([enc2, self.up3(dec2)], 1)\n",
        "    dec3 =self.dec3(up3)\n",
        "\n",
        "    up4 = torch.cat([enc1, self.up4(dec3)], 1)\n",
        "    dec4 = self.dec4(up4)\n",
        "\n",
        "    final=self.final(dec4)\n",
        "\n",
        "    return final\n",
        "\n",
        "\n",
        "class WNet(nn.Module):\n",
        "  def __init__(self, squeeze, ch_mul=64, in_chans=3, out_chans=1000):\n",
        "    super(WNet, self).__init__()\n",
        "    if out_chans==1000:\n",
        "        out_chans=in_chans\n",
        "    self.UEnc=UEnc(squeeze, ch_mul, in_chans)\n",
        "    self.UDec=UDec(squeeze, ch_mul, out_chans)\n",
        "\n",
        "  def forward(self, x, returns='both'):\n",
        "    enc = self.UEnc(x)\n",
        "\n",
        "    if returns=='enc':\n",
        "      return enc\n",
        "\n",
        "    dec = self.UDec(F.softmax(enc, 1))\n",
        "\n",
        "    if returns=='dec':\n",
        "      return dec\n",
        "\n",
        "    if returns=='both':\n",
        "      return enc, dec\n",
        "    else:\n",
        "      raise ValueError('Invalid returns, returns must be in [enc dec both]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rHdKWEvgz5"
      },
      "source": [
        "## Wnet train/test/loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwk7MFs9vi7o"
      },
      "source": [
        "softmax = nn.Softmax2d()\n",
        "criterionIdt = torch.nn.MSELoss()\n",
        "\n",
        "def train_op(model, optimizer, input, k, img_size, psi=0.5):\n",
        "  enc = model(input, returns='enc')\n",
        "  d = enc.clone().detach()\n",
        "  n_cut_loss=soft_n_cut_loss(input,  softmax(enc),  img_size)\n",
        "  n_cut_loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  dec = model(input, returns='dec')\n",
        "  rec_loss=reconstruction_loss(input, dec)\n",
        "  rec_loss.backward()\n",
        "  optimizer.step()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  return (model, n_cut_loss, rec_loss)\n",
        "\n",
        "\n",
        "def reconstruction_loss(x, x_prime):\n",
        "  rec_loss = criterionIdt(x_prime, x)\n",
        "  return rec_loss\n",
        "\n",
        "\n",
        "def test():\n",
        "  wnet=WNet.WNet(4)\n",
        "  synthetic_data=torch.rand((1, 3, 128, 128))\n",
        "  optimizer=torch.optim.SGD(wnet.parameters(), 0.001) #.cuda()\n",
        "  train_op(wnet, optimizer, synthetic_data)\n",
        "\n",
        "\n",
        "def show_image(image):\n",
        "  img = image.numpy().transpose((1, 2, 0))\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJMVW3BxwhcU"
      },
      "source": [
        "epochsall = 100\n",
        "\n",
        "# Create empty lists for average N_cut losses and reconstruction losses\n",
        "n_cut_losses_avg = []\n",
        "rec_losses_avg = []\n",
        "\n",
        "# Squeeze k\n",
        "k = 4\n",
        "img_size =  # define image size from test_loader\n",
        "wnet = WNet(k).to(device)\n",
        "\n",
        "learning_rate = 0.003\n",
        "optimizer = torch.optim.SGD(wnet.parameters(), lr=learning_rate)\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(img_size),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "# Train 1 image set batch size=1 and set shuffle to False\n",
        "# Here we can train new model for each image, or batch it\n",
        "dataloader = train_loader  # or test_loader\n",
        "\n",
        "# Run for every epoch\n",
        "for epoch in range(epochsall):\n",
        "\n",
        "  # At 1000 epochs divide SGD learning rate by 10\n",
        "  if (epoch > 0 and epoch % 1000 == 0):\n",
        "    learning_rate = learning_rate/10\n",
        "    optimizer = torch.optim.SGD(wnet.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Print out every epoch:\n",
        "  print(f\"Epoch = {epoch}\")\n",
        "\n",
        "  # Create empty lists for N_cut losses and reconstruction losses\n",
        "  n_cut_losses = []\n",
        "  rec_losses = []\n",
        "  start_time = time.time()\n",
        "\n",
        "  for (idx, batch) in enumerate(dataloader):\n",
        "    # Train 1 image idx > 1\n",
        "    if(idx > 1): break\n",
        "\n",
        "    # Train Wnet with CUDA if available\n",
        "    batch[0] = batch[0].to(device)\n",
        "    \n",
        "    wnet, n_cut_loss, rec_loss = train_op(wnet, optimizer, batch[0], k, img_size)\n",
        "\n",
        "    n_cut_losses.append(n_cut_loss.detach())\n",
        "    rec_losses.append(rec_loss.detach())\n",
        "\n",
        "  n_cut_losses_avg.append(torch.mean(torch.FloatTensor(n_cut_losses)))\n",
        "  rec_losses_avg.append(torch.mean(torch.FloatTensor(rec_losses)))\n",
        "  print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "images, labels = next(iter(dataloader))\n",
        "\n",
        "# Run wnet with cuda if enabled\n",
        "images = images.to(device)\n",
        "\n",
        "enc, dec = wnet(images)\n",
        "\n",
        "torch.save(wnet.state_dict(), \"model_\" + args.name)\n",
        "wnet_model = wnet\n",
        "np.save(\"n_cut_losses_\" + args.name, n_cut_losses_avg)\n",
        "np.save(\"rec_losses_\" + args.name, rec_losses_avg)\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5ySKrMxDlG"
      },
      "source": [
        "## Apply Wnet to set of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gaZzFVFxHTF"
      },
      "source": [
        "model = WNet.WNet(4).to(device)  # squezze layers\n",
        "\n",
        "model.load_state_dict(torch.load(wnet_model))\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((64, 64)),\n",
        "                            transforms.ToTensor()])\n",
        "\n",
        "image = Image.open(args.image).convert('RGB')\n",
        "x = transform(image)[None, :, :, :]\n",
        "\n",
        "enc, dec = model(x)\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "show_image(x[0])\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "show_image(enc[0, :1, :, :].detach())\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "show_image(dec[0, :, :, :].detach())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
